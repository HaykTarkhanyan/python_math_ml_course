\documentclass[aspectratio=169]{beamer}

\usetheme{default}
\usecolortheme{dove}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}{%
  \hfill{\large\insertframenumber\,/\,\inserttotalframenumber}\hspace{0.8em}\vspace{0.5em}%
}

\definecolor{popblue}{RGB}{52, 101, 164}
\definecolor{sampred}{RGB}{204, 0, 0}
\definecolor{paramgreen}{RGB}{0, 140, 70}
\definecolor{warnred}{RGB}{180, 40, 40}
\definecolor{orange1}{RGB}{220, 120, 0}
\definecolor{violet1}{RGB}{120, 50, 160}
\definecolor{lightbg}{RGB}{245, 245, 250}

\setbeamercolor{frametitle}{fg=popblue}
\setbeamercolor{title}{fg=popblue}

\usepackage{pgfplots}
\usepackage{tikz}
\usetikzlibrary{shapes, arrows.meta, positioning, calc, decorations.pathreplacing, patterns}
\pgfplotsset{compat=1.18}
\usepackage{amsmath, amssymb}
\usepackage{fontenc}

\title{Lecture 4: Point Estimation}
\subtitle{Method of Moments $\cdot$ Maximum Likelihood $\cdot$ Why MLE Works}
\date{}

\begin{document}

% ============================================================
\begin{frame}
\titlepage
\end{frame}

% ============================================================
\begin{frame}
\frametitle{The Estimation Problem}
\begin{center}
\begin{tikzpicture}
  \node[draw=popblue, fill=popblue!5, rounded corners=10pt, text width=11cm, align=center, inner sep=15pt] {
    A factory produces lightbulbs. You test 50 and find a mean lifetime of 1{,}200 hours.\\[10pt]
    {\Large What can you say about the \textbf{true} mean lifetime?}\\[10pt]
    In Lecture~3 we learned how to \textbf{judge} estimators (bias, variance, MSE, efficiency).\\
    Today: how to \textbf{construct} them systematically.
  };
\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}
\frametitle{From Data to Parameters}
\begin{center}
\begin{tikzpicture}[
  arrow/.style={-{Stealth[length=8pt]}, line width=1.5pt}
]
  % Data
  \node[draw=sampred, fill=sampred!8, rounded corners=8pt, minimum width=4cm, minimum height=1.5cm, font=\large\bfseries, align=center] (data) at (-4, 0) {Data\\{\normalsize\normalfont $X_1, \ldots, X_n$}};

  % Black box
  \node[draw=popblue, fill=popblue!15, rounded corners=8pt, minimum width=3cm, minimum height=1.5cm, font=\large\bfseries, align=center] (method) at (0, 0) {Method\\{\normalsize\normalfont ???}};

  % Estimate
  \node[draw=paramgreen, fill=paramgreen!8, rounded corners=8pt, minimum width=4cm, minimum height=1.5cm, font=\large\bfseries, align=center] (est) at (4, 0) {Estimate\\{\normalsize\normalfont $\hat{\theta}$}};

  \draw[arrow, sampred] (data) -- (method);
  \draw[arrow, paramgreen] (method) -- (est);

  % Two methods below
  \node[draw, rounded corners=4pt, fill=popblue!8, minimum width=4cm, font=\normalsize, align=center] at (-2.5, -2.5) {\textbf{Method of Moments}\\(match sample moments)};
  \node[draw, rounded corners=4pt, fill=orange1!8, minimum width=4cm, font=\normalsize, align=center] at (2.5, -2.5) {\textbf{Maximum Likelihood}\\(maximize data probability)};

  \draw[-{Stealth}, dashed] (-2.5, -1.7) -- (0, -0.9);
  \draw[-{Stealth}, dashed] (2.5, -1.7) -- (0, -0.9);
\end{tikzpicture}
\end{center}
\end{frame}

% ============================================================
\section{Method of Moments}

\begin{frame}
\frametitle{Method of Moments (MoM)}

\textbf{Idea:} Set population moments equal to sample moments, then solve for the parameters.

\vspace{0.4cm}
\begin{center}
\begin{tikzpicture}[
  mombox/.style={draw, rounded corners=5pt, minimum height=1cm, text width=4.5cm, align=center, font=\normalsize, inner sep=6pt}
]
  \node[mombox, fill=paramgreen!10] at (-4, 0) {
    $\mathbb{E}[X] = g_1(\theta)$\\[4pt]
    $\mathbb{E}[X^2] = g_2(\theta)$\\[2pt]
    $\vdots$
  };

  \node[font=\Huge] at (0, 0) {$\longrightarrow$};
  \node[font=\small] at (0, -0.6) {replace with};

  \node[mombox, fill=sampred!10] at (4, 0) {
    $\bar{X} = g_1(\hat{\theta})$\\[4pt]
    $\frac{1}{n}\sum X_i^2 = g_2(\hat{\theta})$\\[2pt]
    $\vdots$
  };
\end{tikzpicture}
\end{center}

\vspace{0.5cm}
\begin{columns}
\begin{column}{0.48\textwidth}
  \textcolor{paramgreen}{\textbf{Pros:}}
  \begin{itemize}\small
    \item Simple, quick to compute
    \item No distributional assumption needed for computation
  \end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
  \textcolor{warnred}{\textbf{Cons:}}
  \begin{itemize}\small
    \item Can give impossible values (e.g., $\hat{\sigma}^2 < 0$)
    \item Generally less efficient than MLE
    \item Awkward with many parameters
  \end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{MoM Example: Normal Distribution}
\begin{center}
\begin{tikzpicture}
  \node[draw=popblue, fill=popblue!5, rounded corners=8pt, text width=11cm, align=left, inner sep=12pt] {
    \textbf{Model:} $X_1, \ldots, X_n \sim \mathcal{N}(\mu, \sigma^2)$. Two unknowns, need two equations.\\[8pt]
    \textbf{1st moment:} $\mathbb{E}[X] = \mu \quad\Rightarrow\quad \hat{\mu}_{\text{MoM}} = \bar{X}$\\[8pt]
    \textbf{2nd moment:} $\mathbb{E}[X^2] = \mu^2 + \sigma^2 \quad\Rightarrow\quad \hat{\sigma}^2_{\text{MoM}} = \frac{1}{n}\sum X_i^2 - \bar{X}^2 = \frac{1}{n}\sum(X_i - \bar{X})^2$\\[10pt]
    \small (Note: this divides by $n$, not $n{-}1$ --- \textbf{biased!} Recall Bessel's correction from Lecture~3.)
  };
\end{tikzpicture}
\end{center}
\end{frame}

% ============================================================
\section{Likelihood}

\begin{frame}
\frametitle{The Likelihood Function}

\begin{center}
\begin{tikzpicture}
  % Big idea box
  \node[draw=orange1, fill=orange1!5, rounded corners=10pt, text width=12cm, align=center, inner sep=12pt] {
    {\large\bfseries Given the data I observed, how \textit{plausible} is each parameter value?}\\[10pt]
    $$L(\theta) = P(\text{data} \mid \theta) = \prod_{i=1}^n f(X_i \mid \theta)$$\\[6pt]
    \small Same formula as the joint density, but now\\
    \textbf{data is fixed, $\theta$ varies} (not the other way around!)
  };
\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Likelihood: Coin Flip Example}
\begin{center}
\begin{tikzpicture}
  \node[font=\normalsize] at (0, 4) {Data: 7 heads in 10 flips. \quad $L(p) = \binom{10}{7} p^7 (1-p)^3$};

  \begin{axis}[
    width=11cm, height=5.5cm,
    xlabel={$p$ (probability of heads)},
    ylabel={$L(p)$},
    xmin=0, xmax=1,
    ymin=0, ymax=0.32,
    axis lines=left,
    grid=major, grid style={gray!15},
    every axis plot/.append style={line width=2pt, samples=100}
  ]
    \addplot[popblue, domain=0:1] {120 * x^7 * (1-x)^3};

    % MLE marker
    \draw[very thick, sampred, dashed] (axis cs:0.7, 0) -- (axis cs:0.7, 0.267);
    \fill[sampred] (axis cs:0.7, 0.267) circle (4pt);
    \node[font=\small\bfseries, sampred, right] at (axis cs:0.72, 0.27) {MLE: $\hat{p} = 0.7$};

    % Other values
    \draw[thin, dashed, gray] (axis cs:0.5, 0) -- (axis cs:0.5, 0.117);
    \node[font=\footnotesize, gray, left] at (axis cs:0.48, 0.13) {$p=0.5$: less plausible};
  \end{axis}
\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Log-Likelihood: Why We Prefer It}
\begin{center}
\begin{tikzpicture}[
  llbox/.style={draw, rounded corners=5pt, minimum height=2.5cm, text width=4.8cm, align=center, inner sep=8pt}
]
  \node[llbox, fill=sampred!8] at (-4, 0) {
    \textbf{Likelihood}\\[6pt]
    $L(\theta) = \prod_{i=1}^n f(X_i \mid \theta)$\\[8pt]
    \small Product of $n$ small numbers\\
    $\Rightarrow$ numerical underflow
  };

  \node[font=\Huge] at (0, 0) {$\log$};
  \draw[-{Stealth}, very thick] (-1.2, 0) -- (1.2, 0);

  \node[llbox, fill=paramgreen!8] at (4, 0) {
    \textbf{Log-Likelihood}\\[6pt]
    $\ell(\theta) = \sum_{i=1}^n \log f(X_i \mid \theta)$\\[8pt]
    \small Sum: numerically stable\\
    Same maximizer as $L$
  };
\end{tikzpicture}
\end{center}

\vspace{0.5cm}
\textbf{Score function} (from Lecture~3): $s(\theta) = \frac{\partial \ell(\theta)}{\partial \theta}$ \quad --- the gradient of the log-likelihood.\\[4pt]
\small At the MLE: $s(\hat{\theta}) = 0$ \quad (first-order condition). The \textbf{curvature} of $\ell$ at this point $\to$ Fisher information.
\end{frame}

% ============================================================
\section{Maximum Likelihood Estimation}

\begin{frame}
\frametitle{Maximum Likelihood Estimation}

\begin{center}
\begin{tikzpicture}
  \node[draw=popblue, fill=popblue!8, rounded corners=10pt, text width=11cm, align=center, inner sep=15pt, font=\large] {
    $\hat{\theta}_{\text{MLE}} = \arg\max_\theta \; \ell(\theta) = \arg\max_\theta \sum_{i=1}^n \log f(X_i \mid \theta)$
  };
  \node[font=\normalsize] at (0, -1.5) {``Choose the parameter value that makes the observed data \textbf{most probable.}''};
\end{tikzpicture}
\end{center}

\vspace{0.2cm}
\begin{center}
\begin{tikzpicture}[scale=0.93, every node/.style={transform shape},
  stp/.style={draw=popblue!60, fill=popblue!6, rounded corners=5pt, minimum height=0.9cm, align=center, inner sep=5pt, font=\small},
  arr/.style={-{Stealth[length=6pt]}, thick, popblue!70}
]
  \node[stp] (s1) at (0, 0) {Write $\ell(\theta)$};
  \node[stp] (s2) at (3.1, 0) {Differentiate};
  \node[stp] (s3) at (6.0, 0) {Set $\frac{d\ell}{d\theta} = 0$};
  \node[stp] (s4) at (8.9, 0) {Solve for $\hat\theta$};
  \node[stp, fill=paramgreen!8, draw=paramgreen!60] (s5) at (11.6, 0) {Check $\ell'' < 0$};
  \draw[arr] (s1) -- (s2);
  \draw[arr] (s2) -- (s3);
  \draw[arr] (s3) -- (s4);
  \draw[arr] (s4) -- (s5);
\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}
\frametitle{MLE: Bernoulli (Coin Fairness)}
\begin{center}
\begin{tikzpicture}
  \node[draw=popblue, fill=popblue!3, rounded corners=8pt, text width=12cm, align=left, inner sep=12pt] {
    \textbf{Model:} $X_i \sim \text{Bernoulli}(p)$, \quad observe $k$ successes in $n$ trials.\\[8pt]
    $\ell(p) = k\log p + (n-k)\log(1-p)$\\[8pt]
    $\frac{\partial\ell}{\partial p} = \frac{k}{p} - \frac{n-k}{1-p} = 0$\\[8pt]
    $$\boxed{\hat{p}_{\text{MLE}} = \frac{k}{n} = \bar{X}}$$\\[4pt]
    \small The sample proportion --- exactly what you'd guess intuitively.
  };
\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}
\frametitle{MLE: Normal (Measurement Error)}
\begin{center}
\begin{tikzpicture}
  \node[draw=paramgreen, fill=paramgreen!3, rounded corners=8pt, text width=12cm, align=left, inner sep=12pt] {
    \textbf{Model:} $X_i \sim \mathcal{N}(\mu, \sigma^2)$\\[6pt]
    $\ell(\mu, \sigma^2) = -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log\sigma^2 - \frac{1}{2\sigma^2}\sum_{i=1}^n(X_i - \mu)^2$\\[8pt]
    Setting $\frac{\partial\ell}{\partial\mu} = 0$: \quad $\boxed{\hat{\mu}_{\text{MLE}} = \bar{X}}$\\[8pt]
    Setting $\frac{\partial\ell}{\partial\sigma^2} = 0$: \quad $\boxed{\hat{\sigma}^2_{\text{MLE}} = \frac{1}{n}\sum_{i=1}^n(X_i - \bar{X})^2}$\\[8pt]
    \small Note: divides by $n$, not $n{-}1$ --- the MLE for $\sigma^2$ is \textbf{biased} (Lecture~3: Bessel's correction).\\
    But recall: the biased $\hat\sigma^2_n$ has \textbf{lower MSE} than the unbiased $S^2$!
  };
\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}
\frametitle{MLE: Poisson (Rare Events)}

\vspace{-0.1cm}
\begin{columns}[T]
\begin{column}{0.54\textwidth}
\begin{tikzpicture}
  \node[draw=orange1, fill=orange1!3, rounded corners=8pt, text width=6.5cm, align=left, inner sep=8pt, font=\small] {
    \textbf{Model:} $X_i \sim \text{Pois}(\lambda)$\\[3pt]
    {\scriptsize (goals/match, earthquakes/yr, typos/pg)}\\[6pt]
    $\ell(\lambda) = (\!\sum X_i)\log\lambda - n\lambda + \text{c}$\\[6pt]
    $\frac{\partial\ell}{\partial\lambda} = \frac{\sum X_i}{\lambda} - n = 0$\\[6pt]
    $$\boxed{\hat{\lambda}_{\text{MLE}} = \bar{X}}$$\\[-2pt]
    \small Sample mean estimates the \textit{rate}.
  };
\end{tikzpicture}
\end{column}
\begin{column}{0.44\textwidth}
\vspace{0.1cm}
\begin{center}
\begin{tikzpicture}
  \begin{axis}[
    width=5.5cm, height=5.5cm,
    xlabel={$\lambda$},
    ylabel={$\ell(\lambda) - \ell(\hat\lambda)$},
    xmin=0.5, xmax=6,
    ymin=-15, ymax=1,
    axis lines=left,
    grid=major, grid style={gray!15},
    every axis plot/.append style={line width=2pt, samples=80},
  ]
    % l(lambda) - l(3) = 60(ln(lambda)-ln(3)) - 20(lambda-3), n=20, sum=60
    \addplot[popblue, domain=0.5:6] {60*(ln(x)-ln(3)) - 20*(x-3)};

    % MLE marker
    \draw[very thick, sampred, dashed] (axis cs:3, -15) -- (axis cs:3, 0);
    \fill[sampred] (axis cs:3, 0) circle (3pt);
    \node[font=\small\bfseries, sampred, above] at (axis cs:3, 0.3) {$\hat\lambda = 3$};
  \end{axis}
\end{tikzpicture}
\end{center}
\footnotesize\centering Example: $n=20$, $\sum X_i = 60$
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{MLE: Exponential (Waiting Times)}
\begin{center}
\begin{tikzpicture}
  \node[draw=sampred, fill=sampred!3, rounded corners=8pt, text width=12cm, align=left, inner sep=12pt] {
    \textbf{Model:} $X_i \sim \text{Exp}(\lambda)$ \quad (time between arrivals, device lifetimes)\\[8pt]
    $f(x \mid \lambda) = \lambda e^{-\lambda x}$ \quad for $x \ge 0$\\[6pt]
    $\ell(\lambda) = n\log\lambda - \lambda\sum_{i=1}^n X_i$\\[8pt]
    $\frac{\partial\ell}{\partial\lambda} = \frac{n}{\lambda} - \sum X_i = 0$\\[8pt]
    $$\boxed{\hat{\lambda}_{\text{MLE}} = \frac{1}{\bar{X}}}$$\\[4pt]
    \small The reciprocal of the sample mean --- intuitive since $\mathbb{E}[X] = 1/\lambda$.
  };
\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}
\frametitle{MLE: Summary of Examples}
\begin{center}
\renewcommand{\arraystretch}{1.8}
\begin{tabular}{lccc}
  \textbf{Distribution} & \textbf{Parameter} & \textbf{MLE} & \textbf{Real-world use} \\
  \hline
  Bernoulli$(p)$ & $p$ & $\bar{X}$ & Coin fairness, conversion rates \\
  Normal$(\mu, \sigma^2)$ & $\mu, \sigma^2$ & $\bar{X}, \;\frac{1}{n}\sum(X_i-\bar{X})^2$ & Measurement error \\
  Poisson$(\lambda)$ & $\lambda$ & $\bar{X}$ & Count data, rare events \\
  Exponential$(\lambda)$ & $\lambda$ & $1/\bar{X}$ & Waiting times, lifetimes \\
  \hline
\end{tabular}
\end{center}

\vspace{0.3cm}
\small All connect to distributions from Module~19. Next: we'll see that these MLEs are \textbf{efficient} (hit the Cram\'er--Rao bound from Lecture~3).
\end{frame}

% ============================================================
\section{MLE Properties}

\begin{frame}
\frametitle{Invariance Property}
\begin{center}
\begin{tikzpicture}
  \node[draw=popblue, fill=popblue!8, rounded corners=10pt, text width=11cm, align=center, inner sep=12pt] {
    If $\hat{\theta}_{\text{MLE}}$ is the MLE of $\theta$, then for any function $g$:\\[8pt]
    {\Large $\widehat{g(\theta)}_{\text{MLE}} = g(\hat{\theta}_{\text{MLE}})$}
  };
\end{tikzpicture}
\end{center}

\vspace{0.2cm}
\begin{columns}[T]
\begin{column}{0.50\textwidth}
\textbf{Example:}
\begin{itemize}\setlength{\itemsep}{2pt}
  \item MLE of $\lambda$ for Exp is $\hat{\lambda} = 1/\bar{X}$
  \item Want MLE of mean $\mu = 1/\lambda$?
  \item Apply $g(\lambda) = 1/\lambda$:\;\;$\hat{\mu} = \bar{X}$ \;\textcolor{paramgreen}{$\checkmark$}
\end{itemize}

\vspace{0.15cm}
\small This doesn't hold for MoM or other estimators in general.
\end{column}
\begin{column}{0.48\textwidth}
\begin{center}
\begin{tikzpicture}[
  box/.style={draw, rounded corners=5pt, minimum width=2cm, minimum height=0.8cm, align=center, inner sep=4pt, font=\normalsize},
  arr/.style={-{Stealth[length=6pt]}, thick}
]
  \node[box, fill=popblue!8] (theta) at (0, 1.5) {$\theta$};
  \node[box, fill=orange1!8] (gtheta) at (3.5, 1.5) {$g(\theta)$};
  \node[box, fill=popblue!8] (thetahat) at (0, 0) {$\hat\theta_{\text{MLE}}$};
  \node[box, fill=orange1!8] (gthetahat) at (3.5, 0) {$g(\hat\theta_{\text{MLE}})$};

  \draw[arr, gray] (theta) -- (gtheta) node[midway, above, font=\small] {$g$};
  \draw[arr, popblue] (theta) -- (thetahat) node[midway, left, font=\small] {MLE};
  \draw[arr, orange1] (gtheta) -- (gthetahat) node[midway, right, font=\small] {MLE};
  \draw[arr, gray] (thetahat) -- (gthetahat) node[midway, below, font=\small] {$g$};

  \node[font=\large\bfseries, paramgreen] at (1.75, 0.75) {$=$};
\end{tikzpicture}
\end{center}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Identifiability}
\begin{center}
\begin{tikzpicture}
  \node[draw=warnred, fill=warnred!5, rounded corners=10pt, text width=11cm, align=center, inner sep=12pt] {
    \textbf{Can we even hope to recover $\theta$?}\\[8pt]
    A model is \textbf{identifiable} if different parameter values give different distributions:\\[6pt]
    $\theta_1 \ne \theta_2 \;\Rightarrow\; f(\cdot \mid \theta_1) \ne f(\cdot \mid \theta_2)$
  };
\end{tikzpicture}
\end{center}

\vspace{0.4cm}
\textbf{When it fails:}
\begin{itemize}
  \item \textbf{Mixture models:} swapping component labels gives the same distribution
  \item \textbf{Overparameterized models:} more parameters than the data can distinguish
  \item \textbf{Symmetric likelihoods:} multiple maxima, MLE is not unique
\end{itemize}

\vspace{0.3cm}
\small If the model isn't identifiable, no amount of data will help.
\end{frame}

\begin{frame}
\frametitle{Visualizing Non-Identifiability}
\begin{center}
\begin{tikzpicture}
  \begin{axis}[
    width=12cm, height=5.5cm,
    xlabel={$x$},
    ylabel={Density},
    xmin=-6, xmax=8,
    ymin=0, ymax=0.25,
    axis lines=left,
    grid=major, grid style={gray!15},
    every axis plot/.append style={line width=1.5pt, samples=100},
    legend style={at={(0.98,0.95)}, anchor=north east, font=\small}
  ]
    % Same mixture, two parameterizations
    \addplot[popblue, domain=-6:8] {0.5 * exp(-(x+1)^2/2)/sqrt(2*pi) + 0.5 * exp(-(x-3)^2/2)/sqrt(2*pi)};
    \addlegendentry{$\frac{1}{2}\mathcal{N}(-1,1) + \frac{1}{2}\mathcal{N}(3,1)$}

    \addplot[sampred, dashed, domain=-6:8] {0.5 * exp(-(x-3)^2/2)/sqrt(2*pi) + 0.5 * exp(-(x+1)^2/2)/sqrt(2*pi)};
    \addlegendentry{$\frac{1}{2}\mathcal{N}(3,1) + \frac{1}{2}\mathcal{N}(-1,1)$}
  \end{axis}

  \node[font=\normalsize\bfseries, text=warnred] at (6.5, -0.5) {Same distribution, different ``parameters''!};
\end{tikzpicture}
\end{center}
\end{frame}

% ============================================================
\section{MLE Meets Lecture 3}

\begin{frame}
\frametitle{MLE and Sufficient Statistics}

\small
In Lecture~3 we learned: a \textbf{sufficient statistic} $T(\mathbf{X})$ captures everything about $\theta$.

\vspace{0.15cm}
\begin{center}
\begin{tikzpicture}
  \node[draw=popblue, fill=popblue!5, rounded corners=8pt, text width=12cm, align=center, inner sep=8pt] {
    \textbf{Key fact:} The MLE depends on the data \textbf{only through} the sufficient statistic.\\[4pt]
    If $T(\mathbf{X})$ is sufficient for $\theta$, then the MLE $\hat\theta$ is a function of $T$.
  };
\end{tikzpicture}
\end{center}

\vspace{0.1cm}
\textbf{Check our examples:}
\vspace{0.05cm}

\renewcommand{\arraystretch}{1.4}
\begin{center}
\begin{tabular}{lccc}
  \textbf{Model} & \textbf{Suff.\ stat} $T$ & \textbf{MLE} & \textbf{Function of $T$?} \\
  \hline
  $\text{Bern}(p)$ & $\sum X_i$ & $\bar{X} = T/n$ & \textcolor{paramgreen}{\checkmark} \\
  $N(\mu, \sigma_0^2)$ & $\sum X_i$ & $\bar{X} = T/n$ & \textcolor{paramgreen}{\checkmark} \\
  $\text{Pois}(\lambda)$ & $\sum X_i$ & $\bar{X} = T/n$ & \textcolor{paramgreen}{\checkmark} \\
  $\text{Exp}(\lambda)$ & $\sum X_i$ & $1/\bar{X} = n/T$ & \textcolor{paramgreen}{\checkmark} \\
  \hline
\end{tabular}
\end{center}

\vspace{0.1cm}
\begin{center}
\small No coincidence --- MLE \textbf{always} uses sufficient statistics. No information is wasted.
\end{center}
\end{frame}

\begin{frame}
\frametitle{MLE in Exponential Families}

\small
Recall the \textbf{exponential family} form from Lecture~3:
$\;f(x \mid \theta) = h(x)\exp\!\big(\eta(\theta)\,T(x) - A(\theta)\big)$

\vspace{0.2cm}
For $n$ i.i.d.\ observations, the log-likelihood is:
$$\ell(\theta) = \eta(\theta)\sum_{i=1}^n T(X_i) - nA(\theta) + \text{const}$$

Setting the derivative to zero:
$$\eta'(\theta)\sum_{i=1}^n T(X_i) = n A'(\theta)$$

\vspace{0.1cm}
\begin{center}
\begin{tikzpicture}
  \node[draw=paramgreen, fill=paramgreen!5, rounded corners=8pt, text width=12cm, align=center, inner sep=10pt] {
    \textbf{For natural exponential families} ($\eta = \theta$):\\[6pt]
    $A'(\hat\theta_{\text{MLE}}) = \frac{1}{n}\sum_{i=1}^n T(X_i)$ \qquad (match population mean to sample mean)\\[6pt]
    \small The MLE is \textbf{always} a function of the sufficient statistic $\sum T(X_i)$,\\
    and it \textbf{equals the MoM estimator} in natural form!
  };
\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Why MLE Works: The Big Theoretical Guarantees}

\small
Under regularity conditions (Lecture~3), MLE has remarkable properties:

\vspace{0.15cm}
\begin{center}
\begin{tikzpicture}[
  gbox/.style={draw=popblue!60, fill=popblue!6, rounded corners=5pt, minimum width=12.5cm, align=left, inner sep=6pt, font=\small}
]
  \node[gbox] at (0, 2.4) {\textbf{1.\;Consistent:} $\hat\theta_{\text{MLE}} \xrightarrow{P} \theta_0$ as $n \to \infty$ \;\;(gets the right answer eventually)};
  \node[gbox] at (0, 1.2) {\textbf{2.\;Asymptotically Normal:} $\sqrt{n}(\hat\theta_{\text{MLE}} - \theta_0) \xrightarrow{d} N\!\left(0,\; \frac{1}{I(\theta_0)}\right)$};
  \node[gbox] at (0, 0) {\textbf{3.\;Asymptotically Efficient:} achieves the \textbf{Cram\'er--Rao bound} as $n \to \infty$};
  \node[gbox] at (0, -1.2) {\textbf{4.\;Invariant:} MLE of $g(\theta)$ is $g(\hat\theta_{\text{MLE}})$ for any function $g$};
\end{tikzpicture}
\end{center}

\vspace{0.15cm}
\begin{center}
\fcolorbox{violet1}{violet1!5}{\parbox{11cm}{\centering\small
  \textbf{Translation:} With enough data, MLE is approximately unbiased,\\
  approximately normal, and \textbf{no other estimator can do better}.\\[3pt]
  This is why MLE is the default method in statistics and machine learning.
}}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Asymptotic Normality: Seeing It}

\small
As $n$ grows, the sampling distribution of the MLE converges to a Normal centered at the truth:

\vspace{0.1cm}
\begin{center}
\begin{tikzpicture}
  \begin{axis}[
    width=12cm, height=5.5cm,
    xlabel={$\hat\theta_{\text{MLE}}$},
    ylabel={Density},
    xmin=-4, xmax=4,
    ymin=0, ymax=1.1,
    axis lines=left,
    grid=major, grid style={gray!15},
    every axis plot/.append style={line width=1.8pt, smooth, samples=80},
    legend style={at={(0.98,0.95)}, anchor=north east, font=\small, fill=white, fill opacity=0.9},
  ]
    % n=10: wide
    \addplot[orange1!70, domain=-4:4] {1/sqrt(2*pi*1)*exp(-x^2/(2*1))};
    \addlegendentry{$n = 10$}

    % n=50: medium
    \addplot[popblue, domain=-4:4] {1/sqrt(2*pi*0.2)*exp(-x^2/(2*0.2))};
    \addlegendentry{$n = 50$}

    % n=200: tight
    \addplot[paramgreen, domain=-4:4] {1/sqrt(2*pi*0.05)*exp(-x^2/(2*0.05))};
    \addlegendentry{$n = 200$}

    % True value
    \draw[dashed, thick, sampred] (axis cs:0, 0) -- (axis cs:0, 1.1);
    \node[font=\small\bfseries, sampred, above left] at (axis cs:0, 1.05) {$\theta_0$};
  \end{axis}
\end{tikzpicture}
\end{center}

\vspace{-0.1cm}
\begin{center}
\small Variance shrinks as $\frac{1}{n\,I(\theta_0)}$\,: more data $\Rightarrow$ tighter bell $\Rightarrow$ more precise estimate.\\
With $n = 200$ observations, MLE is practically pinpointed at the truth.
\end{center}
\end{frame}

\begin{frame}
\frametitle{MLE Achieves the Cram\'er--Rao Bound}

\small
From Lecture~3, the \textbf{CR bound}: $\text{Var}(\hat\theta) \geq \frac{1}{n\,I(\theta)}$ for unbiased estimators.

\vspace{0.2cm}
\textbf{Does MLE hit this bound?}

\vspace{0.2cm}
\begin{center}
\renewcommand{\arraystretch}{1.6}
\begin{tabular}{lcccc}
  \textbf{Model} & \textbf{MLE} & $\text{Var}(\hat\theta_{\text{MLE}})$ & \textbf{CR bound} & \textbf{Efficient?} \\
  \hline
  $\text{Bern}(p)$ & $\bar{X}$ & $\frac{p(1-p)}{n}$ & $\frac{p(1-p)}{n}$ & \textcolor{paramgreen}{\textbf{Yes}} \\[3pt]
  $N(\mu, \sigma_0^2)$ & $\bar{X}$ & $\frac{\sigma_0^2}{n}$ & $\frac{\sigma_0^2}{n}$ & \textcolor{paramgreen}{\textbf{Yes}} \\[3pt]
  $\text{Pois}(\lambda)$ & $\bar{X}$ & $\frac{\lambda}{n}$ & $\frac{\lambda}{n}$ & \textcolor{paramgreen}{\textbf{Yes}} \\
  \hline
\end{tabular}
\end{center}

\vspace{0.2cm}
\begin{center}
\fcolorbox{paramgreen}{paramgreen!5}{\parbox{11cm}{\centering\small
  For \textbf{exponential families}, the MLE of the natural parameter is efficient (hits the CR bound exactly). For other models, MLE is \textbf{asymptotically} efficient --- it approaches the bound as $n \to \infty$.
}}
\end{center}
\end{frame}

% ============================================================
\section{Practical}

\begin{frame}
\frametitle{Practical: Implement MLE}
\begin{center}
\begin{tikzpicture}
  \node[draw=popblue, fill=popblue!3, rounded corners=10pt, text width=12cm, align=left, inner sep=12pt] {
    \begin{enumerate}\setlength{\itemsep}{4pt}
      \item Implement MLE for a Gaussian \textbf{from scratch}:
        \begin{itemize}\setlength{\itemsep}{1pt}
          \item Write the log-likelihood function
          \item Optimize numerically (\texttt{scipy.optimize}) and compare with closed form
        \end{itemize}
      \item Compare $\hat\sigma^2_{\text{MLE}}$ (divides by $n$) with $S^2$ (divides by $n{-}1$).\\
        Verify the bias from Lecture~3 empirically with simulation.
      \item Fit a Poisson to real count data. Check: is the MLE efficient?\\
        Compute the CR bound and compare with the observed variance.
      \item Plot the log-likelihood surface --- observe the peak at the MLE\\
        and relate its \textbf{curvature} to Fisher information.
    \end{enumerate}
  };
\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Homework}
\begin{enumerate}
  \item Derive the MLE for $\text{Geometric}(p)$: $\;f(x \mid p) = (1-p)^{x-1}p$, $\;x = 1, 2, \ldots$\\
    Is this MLE unbiased? Is it efficient (check against the CR bound)?

  \vspace{0.15cm}
  \item For $X_1, \ldots, X_n \sim N(\mu, \sigma^2)$, show that $\hat\sigma^2_{\text{MLE}} = \frac{1}{n}\sum(X_i - \bar{X})^2$\\
    equals the MoM estimator. Why is this not a coincidence? (Hint: exponential family.)

  \vspace{0.15cm}
  \item Show that the MLE for $\text{Uniform}(0, \theta)$ is $\hat\theta = X_{(n)} = \max(X_1, \ldots, X_n)$.\\
    Is this unbiased? Is it consistent? (Note: this is \textbf{not} an exponential family!)

  \vspace{0.15cm}
  \item Simulate $n = 50$ samples from $\text{Poisson}(\lambda = 3)$ and compute the MLE.\\
    Repeat 10{,}000 times. Verify: (a) $\hat\lambda$ is approximately unbiased, (b)~$\text{Var}(\hat\lambda) \approx \lambda/n$.
\end{enumerate}
\end{frame}

\begin{frame}
\begin{center}
  {\Huge\bfseries\textcolor{popblue}{Questions?}}

  \vspace{1cm}

  {\large Next lecture: MAP estimation, priors, and the Bayesian perspective}
\end{center}
\end{frame}

\end{document}
