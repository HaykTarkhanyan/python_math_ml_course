\documentclass[aspectratio=169]{beamer}

\usetheme{default}
\usecolortheme{dove}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}{%
  \hfill{\large\insertframenumber\,/\,\inserttotalframenumber}\hspace{0.8em}\vspace{0.5em}%
}

\definecolor{popblue}{RGB}{52, 101, 164}
\definecolor{sampred}{RGB}{204, 0, 0}
\definecolor{paramgreen}{RGB}{0, 140, 70}
\definecolor{warnred}{RGB}{180, 40, 40}
\definecolor{orange1}{RGB}{220, 120, 0}
\definecolor{violet1}{RGB}{120, 50, 160}
\definecolor{lightbg}{RGB}{245, 245, 250}

\setbeamercolor{frametitle}{fg=popblue}
\setbeamercolor{title}{fg=popblue}

\usepackage{pgfplots}
\usepackage{tikz}
\usetikzlibrary{shapes, arrows.meta, positioning, calc, decorations.pathreplacing, patterns}
\pgfplotsset{compat=1.18}
\usepackage{amsmath, amssymb}
\usepackage{fontenc}

\title{Lecture 3: Properties of Estimators}
\subtitle{Bias $\cdot$ Variance $\cdot$ MSE $\cdot$ Consistency $\cdot$ Sufficiency $\cdot$ Cram\'er--Rao}
\date{}

\begin{document}

% ============================================================
\begin{frame}
\titlepage
\end{frame}

% ============================================================
\begin{frame}
\frametitle{We use estimators every day. Are they any good?}
\begin{center}
\begin{tikzpicture}
  \node[draw=popblue, fill=popblue!5, rounded corners=10pt, text width=11cm, align=center, inner sep=15pt] {
    We already use estimators (Lecture~1, plug-in principle):\\[4pt]
    $\bar{X}$ for $\mu$, \quad $S^2$ for $\sigma^2$, \quad $\hat{p} = \frac{\text{count}}{n}$ for $p$\\[12pt]
    {\Large But how do we \textbf{judge} an estimator?}\\[10pt]
    Is it close to the truth? How much does it jump around? Can we do better?
  };
\end{tikzpicture}
\end{center}
\end{frame}

% ============================================================
\section{Bias}

\begin{frame}
\frametitle{Bias: Is the Estimator Centered on the Truth?}

$$\text{Bias}(\hat\theta) \;=\; \mathbb{E}[\hat\theta] - \theta$$

\vspace{0.2cm}
\begin{center}
\begin{tikzpicture}
  % Unbiased
  \begin{scope}[xshift=-5cm]
    \node[font=\bfseries, paramgreen] at (2, 3.2) {Unbiased};
    \draw[thick, ->] (0, 0) -- (4.5, 0);
    \draw[thick, ->] (0, -0.2) -- (0, 2.8);
    \draw[very thick, paramgreen, smooth, domain=0:4, samples=50] plot (\x, {2.5*exp(-(\x-2)*(\x-2)/0.4)});
    \draw[dashed, thick, popblue] (2, 0) -- (2, 2.7);
    \node[below, font=\small, popblue] at (2, -0.1) {$\theta$};
    \node[font=\small] at (2, -0.7) {$\mathbb{E}[\hat\theta] = \theta$};
  \end{scope}

  % Biased
  \begin{scope}[xshift=2cm]
    \node[font=\bfseries, sampred] at (2, 3.2) {Biased};
    \draw[thick, ->] (0, 0) -- (4.5, 0);
    \draw[thick, ->] (0, -0.2) -- (0, 2.8);
    \draw[very thick, sampred, smooth, domain=0:4, samples=50] plot (\x, {2.5*exp(-(\x-2.8)*(\x-2.8)/0.4)});
    \draw[dashed, thick, popblue] (2, 0) -- (2, 2.7);
    \node[below, font=\small, popblue] at (2, -0.1) {$\theta$};
    \draw[{Stealth}-{Stealth}, thick, orange1] (2, 1.5) -- (2.8, 1.5);
    \node[above, font=\small\bfseries, orange1] at (2.4, 1.5) {bias};
    \node[font=\small] at (2.4, -0.7) {$\mathbb{E}[\hat\theta] \neq \theta$};
  \end{scope}
\end{tikzpicture}
\end{center}

\vspace{0.1cm}
\begin{center}
\small If $\text{Bias}(\hat\theta) = 0$ for all $\theta$, the estimator is \textbf{unbiased}.
\end{center}
\end{frame}

\begin{frame}
\frametitle{Worked Example: Is $\bar{X}$ Unbiased for $\mu$?}

Let $X_1, \ldots, X_n$ be i.i.d.\ with $\mathbb{E}[X_i] = \mu$. \;Is $\hat\mu = \bar{X} = \frac{1}{n}\sum_{i=1}^n X_i$ unbiased?

\vspace{0.3cm}
\textbf{Step 1:} Compute $\mathbb{E}[\hat\mu]$:
$$\mathbb{E}[\bar{X}] = \mathbb{E}\!\left[\frac{1}{n}\sum_{i=1}^n X_i\right]
= \frac{1}{n}\sum_{i=1}^n \mathbb{E}[X_i]
= \frac{1}{n}\cdot n\mu = \mu$$

\pause
\textbf{Step 2:} Check bias:
$$\text{Bias}(\bar{X}) = \mathbb{E}[\bar{X}] - \mu = \mu - \mu = 0 \quad\textcolor{paramgreen}{\checkmark\;\text{Unbiased!}}$$

\vspace{0.2cm}
\begin{center}
\fcolorbox{popblue}{popblue!5}{\parbox{11cm}{\centering\small
  \textbf{Recipe for any estimator:}\\
  (1)~Compute $\mathbb{E}[\hat\theta]$ \;\;$\to$\;\; (2)~Subtract the true $\theta$ \;\;$\to$\;\; (3)~If the result is $0$, it's unbiased.
}}
\end{center}
\end{frame}

% --- Item 1: More detailed bias derivation ---
\begin{frame}
\frametitle{Worked Example: Why Dividing by $n$ Is Biased}

\small
We want to estimate $\sigma^2 = \text{Var}(X_i)$. Natural guess: $\hat\sigma^2_n = \frac{1}{n}\sum_{i=1}^n (X_i - \bar{X})^2$.

\vspace{0.1cm}
\textbf{Trick:} rewrite each $(X_i - \bar{X})$ by adding and subtracting the true mean $\mu$:
$$X_i - \bar{X} = \underbrace{(X_i - \mu)}_{\text{deviation from truth}} - \underbrace{(\bar{X} - \mu)}_{\text{estimation error}}$$

Squaring and summing gives the \textbf{key identity}:
$$\sum_{i=1}^n (X_i - \bar{X})^2 = \sum_{i=1}^n(X_i - \mu)^2 \;-\; n(\bar{X} - \mu)^2$$

\pause
\vspace{-0.1cm}
\textbf{Take expectations} (using $\mathbb{E}[(X_i - \mu)^2] = \sigma^2$ and $\text{Var}(\bar{X}) = \sigma^2/n$):

\vspace{-0.3cm}
\begin{align*}
  \mathbb{E}\!\left[\textstyle\sum(X_i - \mu)^2\right] &= n\sigma^2 &\text{($n$ terms, each $\sigma^2$)}\\[-3pt]
  \mathbb{E}\!\left[n(\bar{X} - \mu)^2\right] &= n \cdot \text{Var}(\bar{X}) = n \cdot \frac{\sigma^2}{n} = \textcolor{orange1}{\sigma^2} &\text{(one ``lost'' degree of freedom)}
\end{align*}

\vspace{-0.4cm}
$$\Rightarrow\quad\mathbb{E}\!\left[\textstyle\sum(X_i - \bar{X})^2\right] = n\sigma^2 - \textcolor{orange1}{\sigma^2} = \textcolor{sampred}{(n-1)\sigma^2}$$
\end{frame}

\begin{frame}
\frametitle{Bessel's Correction: The Fix}

\small
From the previous slide: $\mathbb{E}\!\left[\sum(X_i - \bar{X})^2\right] = (n{-}1)\sigma^2$, so:

\vspace{0.2cm}
\begin{center}
\fcolorbox{sampred}{sampred!5}{\parbox{11cm}{\centering
  $\mathbb{E}[\hat\sigma^2_n] = \mathbb{E}\!\left[\frac{1}{n}\sum(X_i - \bar{X})^2\right] = \frac{(n-1)\sigma^2}{n} \neq \sigma^2$ \quad\textcolor{sampred}{\textbf{Biased!}}\\[4pt]
  \small It \textbf{underestimates} by $\sigma^2/n$. \;Why? We used $\bar{X}$ instead of $\mu$, ``using up'' one degree of freedom.
}}
\end{center}

\vspace{0.3cm}
\begin{center}
\fcolorbox{paramgreen}{paramgreen!5}{\parbox{11cm}{\centering
  \textbf{Bessel's correction:} Divide by $n{-}1$ instead of $n$:\\[4pt]
  $S^2 = \frac{1}{n-1}\sum_{i=1}^n(X_i - \bar{X})^2$ \qquad $\mathbb{E}[S^2] = \sigma^2$ \;\;\textcolor{paramgreen}{$\checkmark$ Unbiased!}
}}
\end{center}

\vspace{0.2cm}
\begin{center}
\small\textbf{Intuition:} We estimated $\mu$ from the same data, so the residuals $(X_i - \bar{X})$\\
are ``too small'' on average. Dividing by $n{-}1$ corrects for this.
\end{center}
\end{frame}

\begin{frame}
\frametitle{Bias: Summary}

\renewcommand{\arraystretch}{1.8}
\begin{center}
\begin{tabular}{lcc}
  \textbf{Estimator} & \textbf{Bias} & \textbf{Unbiased?} \\
  \hline
  $\bar{X} = \frac{1}{n}\sum X_i$ \;\text{for}\; $\mu$ &
  $0$ &
  \textcolor{paramgreen}{\textbf{Yes}} \\
  $\hat\sigma^2_n = \frac{1}{n}\sum(X_i-\bar{X})^2$ \;\text{for}\; $\sigma^2$ &
  $-\frac{\sigma^2}{n}$ &
  \textcolor{sampred}{\textbf{No}} \\
  $S^2 = \frac{1}{n-1}\sum(X_i-\bar{X})^2$ \;\text{for}\; $\sigma^2$ &
  $0$ &
  \textcolor{paramgreen}{\textbf{Yes}} \\
  $\hat{p} = \frac{\sum X_i}{n}$ \;\text{for}\; $p$ (Bernoulli) &
  $0$ &
  \textcolor{paramgreen}{\textbf{Yes}} \\
  \hline
\end{tabular}
\end{center}

\vspace{0.3cm}
\begin{center}
\fcolorbox{orange1}{orange1!5}{\parbox{10cm}{\centering\small
  Dividing by $n$ instead of $n{-}1$ \textbf{underestimates} the true variance.\\
  Bessel's correction ($n{-}1$) fixes this. Recall Lecture~2!
}}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Unbiasedness Alone Isn't Enough}

Consider estimating $\mu = \mathbb{E}[X_i]$ from $X_1, \ldots, X_n$.

\vspace{0.2cm}
\textbf{Surprising fact:} $\tilde\mu = X_1$ is also \textbf{unbiased}!
$$\mathbb{E}[X_1] = \mu \quad\Rightarrow\quad \text{Bias}(X_1) = 0 \quad\textcolor{paramgreen}{\checkmark}$$

\pause
But it's a terrible estimator --- it ignores $X_2, \ldots, X_n$ entirely!

\vspace{0.2cm}
\begin{center}
\fcolorbox{orange1}{orange1!5}{\parbox{11cm}{\centering\small
  \textbf{Three statisticians go deer hunting.}\\[3pt]
  The first one shoots \textbf{1 meter to the left} of the deer.\\
  The second one shoots \textbf{1 meter to the right}.\\
  The third one shouts: \textit{``We got it!''}\\[4pt]
  On average they hit the target --- \textbf{unbiased!} But not very useful\ldots
}}
\end{center}

\vspace{0.15cm}
\begin{center}
\small\textbf{Lesson:} Unbiasedness only says $\mathbb{E}[\hat\theta] = \theta$. It says nothing about\\
how much $\hat\theta$ \textbf{varies}. We need more: \textbf{variance} and \textbf{MSE}.
\end{center}
\end{frame}

% ============================================================
\section{Variance and MSE}

\begin{frame}
\frametitle{The Dartboard Analogy}
\vspace{-0.2cm}
\begin{center}
\begin{tikzpicture}[scale=0.92]
  % --- Top-left: High bias, low variance ---
  \begin{scope}[xshift=-3.5cm, yshift=1.2cm]
    \draw[thick, gray!40] (0,0) circle (1.1);
    \draw[thick, gray!40] (0,0) circle (0.7);
    \draw[thick, gray!40] (0,0) circle (0.3);
    \fill[sampred] (0,0) circle (2.5pt);
    \foreach \x/\y in {0.8/0.7, 0.9/0.6, 0.75/0.8, 0.85/0.65, 0.95/0.75, 0.8/0.6} {
      \fill[popblue] (\x, \y) circle (2.5pt);
    }
    \node[font=\footnotesize\bfseries] at (0, -1.4) {High bias, low var};
    \node[font=\scriptsize, gray] at (0, -1.75) {Precise but inaccurate};
  \end{scope}

  % --- Top-right: Low bias, high variance ---
  \begin{scope}[xshift=3.5cm, yshift=1.2cm]
    \draw[thick, gray!40] (0,0) circle (1.1);
    \draw[thick, gray!40] (0,0) circle (0.7);
    \draw[thick, gray!40] (0,0) circle (0.3);
    \fill[sampred] (0,0) circle (2.5pt);
    \foreach \x/\y in {-0.7/0.5, 0.4/-0.8, -0.3/1.0, 0.8/0.3, -0.5/-0.6, 0.2/0.7} {
      \fill[popblue] (\x, \y) circle (2.5pt);
    }
    \node[font=\footnotesize\bfseries] at (0, -1.4) {Low bias, high var};
    \node[font=\scriptsize, gray] at (0, -1.75) {Accurate but imprecise};
  \end{scope}

  % --- Bottom-left: High bias, high variance ---
  \begin{scope}[xshift=-3.5cm, yshift=-2.2cm]
    \draw[thick, gray!40] (0,0) circle (1.1);
    \draw[thick, gray!40] (0,0) circle (0.7);
    \draw[thick, gray!40] (0,0) circle (0.3);
    \fill[sampred] (0,0) circle (2.5pt);
    \foreach \x/\y in {0.4/1.0, 1.1/0.3, 0.2/0.8, 0.9/0.9, 0.6/0.2, 0.3/1.2} {
      \fill[popblue] (\x, \y) circle (2.5pt);
    }
    \node[font=\footnotesize\bfseries] at (0, -1.4) {High bias, high var};
    \node[font=\scriptsize, gray] at (0, -1.75) {Worst of both worlds};
  \end{scope}

  % --- Bottom-right: Low bias, low variance ---
  \begin{scope}[xshift=3.5cm, yshift=-2.2cm]
    \draw[thick, gray!40] (0,0) circle (1.1);
    \draw[thick, gray!40] (0,0) circle (0.7);
    \draw[thick, gray!40] (0,0) circle (0.3);
    \fill[sampred] (0,0) circle (2.5pt);
    \foreach \x/\y in {0.08/0.12, -0.08/-0.04, 0.04/0.08, -0.04/-0.12, 0.12/-0.08, -0.08/0.04} {
      \fill[popblue] (\x, \y) circle (2.5pt);
    }
    \node[font=\footnotesize\bfseries, paramgreen] at (0, -1.4) {Low bias, low var};
    \node[font=\scriptsize, paramgreen] at (0, -1.75) {The goal!};
  \end{scope}
\end{tikzpicture}
\end{center}
\vspace{-0.2cm}
\begin{center}
\small \textcolor{sampred}{Bullseye} = true $\theta$. \;\textcolor{popblue}{Blue dots} = estimates from repeated samples.
\end{center}
\end{frame}

% --- Item 2: Explain why sigma^2/n ---
\begin{frame}
\frametitle{Variance of an Estimator}

The \textbf{variance} measures how much $\hat\theta$ wobbles across samples:
$\text{Var}(\hat\theta) = \mathbb{E}\!\left[(\hat\theta - \mathbb{E}[\hat\theta])^2\right]$

\vspace{0.05cm}
\textbf{Why is $\text{Var}(\bar{X}) = \sigma^2/n$ and not $\sigma^2/n^2$?}

\vspace{-0.2cm}
\begin{align*}
  \text{Var}(\bar{X})
  &= \text{Var}\!\left(\frac{1}{n}\sum_{i=1}^n X_i\right)
  = \frac{1}{n^2}\,\text{Var}\!\left(\sum_{i=1}^n X_i\right) &\text{($\frac{1}{n}$ comes out as $\frac{1}{n^2}$)}\\[2pt]
  &= \frac{1}{n^2}\sum_{i=1}^n \text{Var}(X_i) &\text{(independent $\Rightarrow$ variances \textbf{add})}\\[2pt]
  &= \frac{1}{n^2}\cdot n\sigma^2 = \boxed{\frac{\sigma^2}{n}} &\text{($n$ terms cancel one $n$)}
\end{align*}

\vspace{-0.3cm}
\begin{center}
\begin{tikzpicture}[
  ebox/.style={draw=popblue!60, fill=popblue!6, rounded corners=5pt, minimum width=7cm, align=center, font=\small, inner sep=4pt}
]
  \node[ebox] at (0, 0) {$\text{SE}(\bar{X}) = \dfrac{\sigma}{\sqrt{n}}$ \;\;(\textbf{standard error} = $\sqrt{\text{Var}}$)};
\end{tikzpicture}
\end{center}
\end{frame}

% --- Item 3: MSE split into 2 slides ---
\begin{frame}
\frametitle{Mean Squared Error: The Total Error}

\small
Bias tells us about the \textbf{aim}, variance about the \textbf{spread}. Can we combine them?

\vspace{0.2cm}
\begin{center}
\begin{tikzpicture}
  \node[draw=popblue, fill=popblue!5, rounded corners=8pt, text width=11cm, align=center, inner sep=10pt] {
    \textbf{Mean Squared Error:} $\;\text{MSE}(\hat\theta) = \mathbb{E}[(\hat\theta - \theta)^2]$\\[6pt]
    The average squared distance from the estimate to the truth.
  };
\end{tikzpicture}
\end{center}

\vspace{0.2cm}
\textbf{The trick:} add and subtract $\mathbb{E}[\hat\theta]$ to decompose the error:
$$\hat\theta - \theta
  = \underbrace{(\hat\theta - \mathbb{E}[\hat\theta])}_{\text{random fluctuation}} + \underbrace{(\mathbb{E}[\hat\theta] - \theta)}_{\text{bias (a constant!)}}$$

\vspace{0.2cm}
\begin{center}
\fcolorbox{orange1}{orange1!5}{\parbox{11cm}{\centering\small
  This splits the total error into two pieces: the \textbf{random part} (how much $\hat\theta$ moves around its own mean)
  and the \textbf{systematic part} (how far that mean is from the truth).
}}
\end{center}
\end{frame}

\begin{frame}
\frametitle{MSE = Bias$^2$ + Variance: The Proof}

\small
Now square $\hat\theta - \theta = (\hat\theta - \mathbb{E}[\hat\theta]) + (\mathbb{E}[\hat\theta] - \theta)$ and take expectations:

\vspace{-0.1cm}
\begin{align*}
  \mathbb{E}[(\hat\theta - \theta)^2]
  &= \mathbb{E}\!\left[(\hat\theta - \mathbb{E}[\hat\theta])^2\right]
   + 2\underbrace{(\mathbb{E}[\hat\theta] - \theta)}_{\text{constant}} \cdot\underbrace{\mathbb{E}[\hat\theta - \mathbb{E}[\hat\theta]]}_{= \,0\text{ (always!)}}
   + (\mathbb{E}[\hat\theta] - \theta)^2
\end{align*}

\vspace{-0.3cm}
The cross term vanishes because $\hat\theta - \mathbb{E}[\hat\theta]$ has mean zero \textbf{by definition}.

\vspace{0.1cm}
$$\boxed{\text{MSE}(\hat\theta) = \text{Var}(\hat\theta) + \text{Bias}^2(\hat\theta)}$$

\vspace{0.1cm}
\begin{center}
\begin{tikzpicture}
  \node[draw=popblue, fill=popblue!8, rounded corners=6pt, minimum width=2.5cm, minimum height=0.7cm, font=\small] (mse) at (0, 0) {\textbf{MSE}};
  \node[draw=sampred, fill=sampred!8, rounded corners=6pt, minimum width=2.5cm, minimum height=0.7cm, font=\small] (bias) at (-3, -1.2) {\textbf{Bias}$^2$};
  \node[draw=orange1, fill=orange1!8, rounded corners=6pt, minimum width=2.5cm, minimum height=0.7cm, font=\small] (var) at (3, -1.2) {\textbf{Variance}};
  \draw[-{Stealth}, very thick] (mse) -- (bias) node[midway, left, font=\footnotesize] {systematic};
  \draw[-{Stealth}, very thick] (mse) -- (var) node[midway, right, font=\footnotesize] {random};
\end{tikzpicture}
\end{center}

\vspace{-0.05cm}
\begin{center}
\small Unbiased means $\text{MSE} = \text{Var}$, but a biased estimator can still win if its variance is low enough.
\end{center}
\end{frame}

\begin{frame}
\frametitle{When Biased Beats Unbiased}

\textbf{Example:} Estimating $\sigma^2$ from $X_1,\ldots,X_n \sim N(\mu, \sigma^2)$.

\vspace{0.2cm}
\begin{center}
\renewcommand{\arraystretch}{1.6}
\begin{tabular}{lccc}
  \textbf{Estimator} & \textbf{Bias} & \textbf{Variance} & \textbf{MSE} \\
  \hline
  $S^2 = \frac{1}{n-1}\sum(X_i - \bar{X})^2$ & $0$ & $\frac{2\sigma^4}{n-1}$ & $\frac{2\sigma^4}{n-1}$ \\[6pt]
  $\hat\sigma^2_n = \frac{1}{n}\sum(X_i - \bar{X})^2$ & $-\frac{\sigma^2}{n}$ & $\frac{2(n-1)\sigma^4}{n^2}$ & $\frac{(2n-1)\sigma^4}{n^2}$ \\[6pt]
  \hline
\end{tabular}
\end{center}

\vspace{0.2cm}
\begin{center}
\begin{tikzpicture}
  \node[draw=paramgreen, fill=paramgreen!5, rounded corners=6pt, text width=11cm, align=center, inner sep=8pt, font=\small] {
    Compare: $\frac{2n-1}{n^2}$ vs $\frac{2}{n-1}$ $\;\Rightarrow\;$ $\hat\sigma^2_n$ has \textbf{lower MSE} for all $n \geq 2$!\\[4pt]
    The biased estimator beats the unbiased $S^2$ because its variance reduction outweighs the small bias.
  };
\end{tikzpicture}
\end{center}
\end{frame}

% Item 4: MSE Comparison Visualized slide REMOVED

% ============================================================
% --- Item 5 + Item 6: Bias-Variance Tradeoff (2 slides, Armenian colors, ML connection) ---
\section{Bias-Variance Tradeoff}

\begin{frame}
\begin{center}
\vspace{1cm}
{\LARGE\bfseries\textcolor{popblue}{The Bias-Variance Tradeoff}}\\[15pt]
{\large You can't minimize bias and variance at the same time.}\\[8pt]
{\large How do we find the \textbf{sweet spot}?}
\end{center}
\end{frame}

\begin{frame}
\frametitle{The Bias-Variance Tradeoff}
\begin{center}
\begin{tikzpicture}
  \begin{axis}[
    width=12cm, height=6.2cm,
    xlabel={Estimator complexity},
    ylabel={Error},
    xmin=0, xmax=10,
    ymin=0, ymax=5,
    xtick=\empty,
    ytick=\empty,
    axis lines=left,
    every axis plot/.append style={line width=2pt, smooth, samples=50},
    legend style={at={(0.98,0.95)}, anchor=north east, font=\small, fill=white, fill opacity=0.9},
  ]
    % Armenian flag order: red, blue, orange
    % Bias^2 (decreasing) - RED
    \addplot[sampred, domain=0.5:9.5] {3.5*exp(-0.3*x) + 0.2};
    \addlegendentry{Bias$^2$}

    % MSE (U-shaped) - BLUE
    \addplot[popblue, domain=0.5:9.5, line width=2.5pt] {3.5*exp(-0.3*x) + 0.1*x^1.3 + 0.3};
    \addlegendentry{MSE (total error)}

    % Variance (increasing) - ORANGE
    \addplot[orange1, domain=0.5:9.5] {0.1*x^1.3 + 0.1};
    \addlegendentry{Variance}

    % Optimal point
    \draw[dashed, thick, paramgreen] (axis cs:3.8, 0) -- (axis cs:3.8, 4.5);
    \node[font=\small\bfseries, paramgreen] at (axis cs:3.8, 4.8) {sweet spot};

    % Annotations
    \node[font=\footnotesize] at (axis cs:1.5, 4.5) {too simple};
    \node[font=\footnotesize] at (axis cs:8, 4.5) {too complex};
  \end{axis}
\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Bias-Variance in Machine Learning}

\small
This tradeoff is \textbf{everywhere} in ML --- it's the same principle in different disguises:

\vspace{0.1cm}
\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{lll}
  \textbf{Setting} & \textbf{Too simple (high bias)} & \textbf{Too complex (high var)} \\
  \hline
  Polynomial regression & Degree 1 (line) & Degree 20 (wiggly) \\
  KNN & Large $k$ (oversmoothed) & $k=1$ (memorizes noise) \\
  Decision tree & Shallow tree (underfits) & Deep tree (overfits) \\
  Neural network & Too few neurons & Too many neurons \\
  Regularization & Strong penalty ($\lambda$ large) & No penalty ($\lambda = 0$) \\
  \hline
\end{tabular}
\end{center}

\vspace{0.1cm}
\begin{center}
\begin{tikzpicture}
  \node[draw=violet1, fill=violet1!5, rounded corners=6pt, text width=12cm, align=center, inner sep=6pt, font=\small] {
    \textbf{Key insight:} In all these cases, the total error (MSE, test loss) is minimized at an
    intermediate complexity. This is why we need \textbf{cross-validation}, \textbf{regularization},
    and \textbf{held-out test sets} --- to find the sweet spot empirically.
  };
\end{tikzpicture}
\end{center}
\end{frame}

% ============================================================
% --- Items 6, 7, 8: Consistency (transition, expanded, extra slide) ---
\section{Consistency}

\begin{frame}
\begin{center}
\vspace{1cm}
{\LARGE\bfseries\textcolor{popblue}{Consistency}}\\[15pt]
{\large Does our estimator converge to the truth}\\[4pt]
{\large as we collect more and more data?}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Consistency: Getting It Right Eventually}

An estimator $\hat\theta_n$ is \textbf{consistent} if it converges to the truth as $n \to \infty$:
$$\hat\theta_n \xrightarrow{P} \theta \qquad\text{i.e.,}\quad \Pr\!\left(|\hat\theta_n - \theta| > \varepsilon\right) \to 0 \;\;\text{for all } \varepsilon > 0$$

\vspace{0.3cm}
\begin{center}
\begin{tikzpicture}
  \begin{axis}[
    width=12cm, height=4.5cm,
    xlabel={Sample size $n$},
    ylabel={$\hat\theta_n$},
    xmin=0, xmax=100,
    ymin=-1, ymax=3,
    grid=major, grid style={gray!15},
    legend style={at={(0.98,0.95)}, anchor=north east, font=\small},
  ]
    % True value
    \addplot[dashed, thick, sampred, domain=0:100] {1};
    \addlegendentry{True $\theta = 1$}

    % Faded sample paths
    \addplot[orange1!35, thin, mark=none, forget plot] coordinates {
      (1, -0.3) (3, 1.9) (5, 0.4) (8, 1.4) (12, 0.6) (16, 1.3)
      (20, 0.8) (25, 1.2) (30, 0.9) (35, 1.1) (40, 0.95)
      (45, 1.05) (50, 0.97) (55, 1.03) (60, 0.98) (65, 1.02)
      (70, 0.99) (75, 1.01) (80, 0.995) (85, 1.005) (90, 0.998) (95, 1.002) (100, 1.001)
    };
    \addplot[paramgreen!35, thin, mark=none, forget plot] coordinates {
      (1, 0.1) (3, 2.2) (5, 0.7) (8, 1.7) (12, 0.4) (16, 1.5)
      (20, 0.7) (25, 1.25) (30, 0.85) (35, 1.15) (40, 0.88)
      (45, 1.1) (50, 0.93) (55, 1.06) (60, 0.96) (65, 1.03)
      (70, 0.97) (75, 1.02) (80, 0.985) (85, 1.01) (90, 0.993) (95, 1.004) (100, 0.998)
    };
    \addplot[violet1!35, thin, mark=none, forget plot] coordinates {
      (1, 2.8) (3, -0.5) (5, 1.5) (8, 0.3) (12, 1.8) (16, 0.55)
      (20, 1.35) (25, 0.75) (30, 1.15) (35, 0.88) (40, 1.08)
      (45, 0.93) (50, 1.05) (55, 0.95) (60, 1.03) (65, 0.97)
      (70, 1.02) (75, 0.98) (80, 1.01) (85, 0.99) (90, 1.005) (95, 0.997) (100, 1.002)
    };

    % Main sample path
    \addplot[popblue, thick, mark=none] coordinates {
      (1, 2.5) (3, 0.2) (5, 1.8) (8, 0.5) (12, 1.6) (16, 0.7)
      (20, 1.3) (25, 0.8) (30, 1.2) (35, 0.85) (40, 1.15)
      (45, 0.9) (50, 1.08) (55, 0.92) (60, 1.05) (65, 0.95)
      (70, 1.03) (75, 0.97) (80, 1.02) (85, 0.98) (90, 1.01) (95, 0.99) (100, 1.005)
    };
    \addlegendentry{$\hat\theta_n$ (sample paths)}
  \end{axis}
\end{tikzpicture}
\end{center}
\end{frame}

% --- Item 8: Extra consistency slide ---
\begin{frame}
\frametitle{Consistent vs Inconsistent: A Contrast}

\small
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\begin{center}
\textcolor{paramgreen}{\textbf{Consistent:}} $\hat\mu = \bar{X}_n$
\end{center}
\begin{itemize}\setlength{\itemsep}{2pt}
  \item $\mathbb{E}[\bar{X}_n] = \mu$ \;(unbiased)
  \item $\text{Var}(\bar{X}_n) = \sigma^2/n \to 0$
  \item Uses \textbf{all} $n$ observations
  \item More data $\Rightarrow$ more precise
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\begin{center}
\textcolor{sampred}{\textbf{Not consistent:}} $\tilde\mu = X_1$
\end{center}
\begin{itemize}\setlength{\itemsep}{2pt}
  \item $\mathbb{E}[X_1] = \mu$ \;(also unbiased!)
  \item $\text{Var}(X_1) = \sigma^2$ \;(constant!)
  \item Uses \textbf{only} the first observation
  \item Ignores all other data forever
\end{itemize}
\end{column}
\end{columns}

\vspace{0.3cm}
\begin{center}
\begin{tikzpicture}
  \node[draw=orange1, fill=orange1!5, rounded corners=6pt, text width=12cm, align=center, inner sep=8pt, font=\small] {
    \textbf{Unbiased $\neq$ consistent.} \;$X_1$ is unbiased but NOT consistent.\\[3pt]
    \textbf{Consistent $\neq$ unbiased.} \;$\hat\sigma^2_n = \frac{1}{n}\sum(X_i - \bar{X})^2$ is biased but IS consistent\\
    (because its bias $\to 0$ and its variance $\to 0$).
  };
\end{tikzpicture}
\end{center}
\end{frame}

% --- Item 7: Chebyshev explanation for consistency ---
\begin{frame}
\frametitle{Sufficient Conditions for Consistency}

\small
\textbf{Chebyshev's inequality} gives us a concrete tool:
\vspace{-0.2cm}
$$\Pr\!\left(|\hat\theta_n - \theta| \geq \varepsilon\right) \;\leq\; \frac{\mathbb{E}[(\hat\theta_n - \theta)^2]}{\varepsilon^2} = \frac{\text{MSE}(\hat\theta_n)}{\varepsilon^2} = \frac{\text{Bias}^2 + \text{Var}}{\varepsilon^2}$$

\vspace{-0.25cm}
\begin{center}
\begin{tikzpicture}[
  sbox/.style={draw=popblue!60, fill=popblue!6, rounded corners=4pt, minimum width=10cm, align=center, font=\small, inner sep=4pt}
]
  \node[sbox] at (0, 1.1) {$\text{Bias}(\hat\theta_n) \to 0$ as $n \to \infty$};
  \node[sbox] at (0, 0.1) {$\text{Var}(\hat\theta_n) \to 0$ as $n \to \infty$};
  \node[font=\normalsize] at (0, -0.55) {$\Downarrow$};
  \node[draw=paramgreen, fill=paramgreen!6, rounded corners=4pt, minimum width=10cm, align=center, font=\small, inner sep=4pt]
    at (0, -1.15) {MSE $\to 0$ \;$\Rightarrow$\; $\Pr(|\hat\theta_n - \theta| \geq \varepsilon) \to 0$ \;$\Rightarrow$\; \textbf{consistent!}};
\end{tikzpicture}
\end{center}

\vspace{-0.1cm}
\small\textbf{Example:} $\bar{X}_n$ is consistent for $\mu$: $\text{Bias} = 0$, $\text{Var} = \sigma^2/n \to 0$, so $\Pr(|\bar{X}_n - \mu| \geq \varepsilon) \leq \sigma^2/(n\varepsilon^2) \to 0$.

\vspace{0.05cm}
\centering\small This is precisely the \textbf{(Weak) Law of Large Numbers}: $\bar{X}_n \xrightarrow{P} \mu$.
\end{frame}

% \begin{frame}
% \frametitle{Worked Example: Is $\hat\sigma^2_n$ Consistent?}

% \small
% We know $\hat\sigma^2_n = \frac{1}{n}\sum(X_i - \bar{X})^2$ is \textbf{biased} for $\sigma^2$. But is it \textbf{consistent}?

% \vspace{0.15cm}
% \textbf{Step 1:} Check bias as $n \to \infty$:
% $$\text{Bias}(\hat\sigma^2_n) = -\frac{\sigma^2}{n} \;\xrightarrow{n\to\infty}\; 0 \quad\textcolor{paramgreen}{\checkmark}$$

% \pause
% \textbf{Step 2:} Check variance as $n \to \infty$ (for Normal data):
% $$\text{Var}(\hat\sigma^2_n) = \frac{2(n-1)\sigma^4}{n^2} \;\xrightarrow{n\to\infty}\; 0 \quad\textcolor{paramgreen}{\checkmark}$$

% \pause
% \textbf{Step 3:} Apply the sufficient condition:
% $$\text{MSE}(\hat\sigma^2_n) = \underbrace{\text{Bias}^2}_{\to\,0} + \underbrace{\text{Var}}_{\to\,0} \;\to\; 0$$

% \vspace{0.1cm}
% \begin{center}
% \fcolorbox{paramgreen}{paramgreen!5}{\parbox{11cm}{\centering\small
%   By Chebyshev: $\Pr(|\hat\sigma^2_n - \sigma^2| \geq \varepsilon) \leq \frac{\text{MSE}}{\varepsilon^2} \to 0$.\\[3pt]
%   So $\hat\sigma^2_n$ is \textbf{consistent} for $\sigma^2$ even though it's biased!
% }}
% \end{center}
% \end{frame}

% ============================================================
% --- Item 6: Transition to sufficiency ---
\section{Sufficiency}

\begin{frame}
\begin{center}
\vspace{1cm}
{\LARGE\bfseries\textcolor{popblue}{Sufficiency}}\\[15pt]
{\large We have $n$ data points. Do we really need \textbf{all} of them?}\\[4pt]
{\large Can we \textbf{compress} without losing information?}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Sufficiency: Can We Compress the Data?}

\small
\textbf{Example:} $X_1, \ldots, X_n \sim \text{Bern}(p)$. To estimate $p$:
\begin{itemize}\setlength{\itemsep}{2pt}
  \item We only need $T = \sum X_i$ (total number of successes)
  \item The specific order (HHTHT vs THHTH) tells us nothing more about $p$
\end{itemize}

\vspace{0.2cm}
\begin{center}
\begin{tikzpicture}
  \node[draw=popblue, fill=popblue!5, rounded corners=8pt, text width=12cm, align=center, inner sep=10pt] {
    \textbf{Definition:} A statistic $T(\mathbf{X})$ is \textbf{sufficient} for $\theta$ if\\[4pt]
    the conditional distribution of $\mathbf{X} \mid T(\mathbf{X})$ does not depend on $\theta$.
  };
\end{tikzpicture}
\end{center}

\vspace{0.2cm}
\begin{center}
\small\textbf{Intuition:} Once you know $T$, the remaining randomness in the data is just noise ---\\
it carries \textbf{no information} about $\theta$. \;$T$ is a ``lossless summary.''
\end{center}
\end{frame}

\begin{frame}
\frametitle{Sufficiency as Data Compression}

\begin{center}
\begin{tikzpicture}[scale=0.95,
  box/.style={draw, rounded corners=6pt, minimum height=1.2cm, align=center, inner sep=8pt, font=\small},
  arrow/.style={-{Stealth[length=7pt]}, line width=1.5pt}
]
  \node[box, fill=sampred!8, text width=3.5cm] (raw) at (-5, 0) {
    \textbf{Raw data}\\[2pt]
    $X_1, X_2, \ldots, X_n$\\
    {\scriptsize ($n$ numbers)}
  };
  \node[box, fill=popblue!10, text width=3.5cm] (suff) at (0, 0) {
    \textbf{Sufficient stat}\\[2pt]
    $T(\mathbf{X})$\\
    {\scriptsize ($k \ll n$ numbers)}
  };
  \node[box, fill=paramgreen!10, text width=3.5cm] (est) at (5, 0) {
    \textbf{Estimator}\\[2pt]
    $\hat\theta = g(T)$\\
    {\scriptsize (1 number)}
  };
  \draw[arrow, sampred!70] (raw) -- (suff) node[midway, above, font=\footnotesize\bfseries] {compress};
  \draw[arrow, popblue!70] (suff) -- (est) node[midway, above, font=\footnotesize\bfseries] {estimate};
  \node[font=\footnotesize, orange1] at (-2.5, -1.0) {\textbf{lossless} (no info about $\theta$ lost)};
  \draw[-{Stealth}, orange1, thick] (-2.5, -0.7) -- (-2.5, -0.2);
  \node[font=\small\bfseries] at (-5, -2.2) {Example};
  \node[font=\small] at (-5, -2.9) {$0,1,1,0,1,1,1,0,1,0$};
  \node[font=\small\bfseries, popblue] at (0, -2.2) {Bernoulli};
  \node[font=\small] at (0, -2.9) {$T = \sum X_i = 6$};
  \node[font=\small] at (5, -2.9) {$\hat{p} = 6/10 = 0.6$};
  \draw[-{Stealth}, gray, thin] (-3.3, -2.9) -- (-1.6, -2.9);
  \draw[-{Stealth}, gray, thin] (1.6, -2.9) -- (3.5, -2.9);
\end{tikzpicture}
\end{center}

\vspace{0.1cm}
\begin{center}
\small The order $(0,1,1,0,1,\ldots)$ doesn't matter for estimating $p$ ---
only the \textbf{total count} matters.
\end{center}
\end{frame}

\begin{frame}
\frametitle{How to Check: Fisher--Neyman Factorization}

\small
\textbf{Theorem:} $T(\mathbf{X})$ is sufficient for $\theta$ if and only if:
$$f(\mathbf{x} \mid \theta) = g\!\left(T(\mathbf{x}),\; \theta\right) \cdot h(\mathbf{x})$$
where $g$ depends on the data \textbf{only through $T$}, and $h$ does not depend on $\theta$.

\vspace{0.15cm}
\textbf{Bernoulli worked example:} $X_1, \ldots, X_n \sim \text{Bern}(p)$, let $T = \sum X_i$.
$$f(\mathbf{x} \mid p) = \prod_{i=1}^n p^{x_i}(1-p)^{1-x_i} = \underbrace{p^{\,\sum x_i}(1-p)^{n - \sum x_i}}_{g(T,\; p)} \cdot \underbrace{1\vphantom{p^{\sum}}}_{h(\mathbf{x})}$$

\vspace{0.1cm}
\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{lll}
  \textbf{Model} & \textbf{Sufficient statistic} & \textbf{Intuition} \\
  \hline
  $\text{Bern}(p)$ & $T = \sum X_i$ & 1 number for 1 parameter \\
  $N(\mu, \sigma^2_0)$ \;($\sigma^2_0$ known) & $T = \bar{X}$ & 1 number for 1 parameter \\
  $N(\mu, \sigma^2)$ \;(both unknown) & $T = (\bar{X},\; S^2)$ & 2 numbers for 2 parameters \\
  \hline
\end{tabular}
\end{center}
\end{frame}

% --- Item 9: Split minimal sufficiency into 2 slides ---
\begin{frame}
\frametitle{Minimal Sufficiency}

\small
The full data $\mathbf{X}$ is always trivially sufficient. But can we compress \textbf{further}?

\vspace{0.15cm}
\begin{center}
\begin{tikzpicture}
  \node[draw=popblue, fill=popblue!5, rounded corners=6pt, text width=12cm, align=center, inner sep=8pt] {
    A sufficient statistic is \textbf{minimal} if it is a function of every other sufficient statistic.\\[3pt]
    It achieves the \textbf{maximum compression} without losing information about $\theta$.
  };
\end{tikzpicture}
\end{center}

\vspace{0.15cm}
\textbf{Example:} For $X_1, \ldots, X_n \sim N(\mu, \sigma^2_0)$ with $\sigma^2_0$ known:

\vspace{0.1cm}
\begin{center}
\begin{tikzpicture}[
  sbox/.style={draw, rounded corners=4pt, minimum width=3.5cm, minimum height=0.9cm, align=center, inner sep=5pt, font=\small}
]
  \node[sbox, fill=sampred!8] (full) at (-4.5, 0) {$\mathbf{X} = (X_1, \ldots, X_n)$\\{\scriptsize sufficient, no compression}};
  \node[sbox, fill=orange1!8] (pair) at (0, 0) {$(\bar{X}, S^2)$\\{\scriptsize sufficient, partial}};
  \node[sbox, fill=paramgreen!8] (min) at (4.5, 0) {$\bar{X}$ alone\\{\scriptsize sufficient \textbf{and minimal}}};
  \draw[-{Stealth}, thick] (full) -- (pair) node[midway, above, font=\scriptsize] {compress};
  \draw[-{Stealth}, thick] (pair) -- (min) node[midway, above, font=\scriptsize] {compress more};
\end{tikzpicture}
\end{center}

\vspace{0.15cm}
\begin{center}
\small Since only $\mu$ is unknown, $S^2$ carries no extra information --- $\bar{X}$ alone is enough.
\end{center}
\end{frame}

\begin{frame}
\frametitle{The Rao--Blackwell Theorem}

\small
Why does sufficiency matter for estimation? Because it lets us \textbf{improve} any estimator:

\vspace{0.15cm}
\begin{center}
\begin{tikzpicture}
  \node[draw=paramgreen, fill=paramgreen!5, rounded corners=8pt, text width=12cm, align=center, inner sep=10pt] {
    \textbf{Rao--Blackwell Theorem:} Given \textit{any} unbiased estimator $\tilde\theta$\\
    and a sufficient statistic $T$, define $\hat\theta = \mathbb{E}[\tilde\theta \mid T]$. Then:\\[6pt]
    (1) $\hat\theta$ is still \textbf{unbiased}: $\mathbb{E}[\hat\theta] = \mathbb{E}[\tilde\theta] = \theta$\\[3pt]
    (2) $\hat\theta$ has \textbf{lower or equal variance}: $\text{Var}(\hat\theta) \leq \text{Var}(\tilde\theta)$\\[6pt]
    Conditioning on a sufficient statistic \textbf{never hurts, often helps}.
  };
\end{tikzpicture}
\end{center}

\pause
\vspace{0.15cm}
\textbf{Worked example:} $X_1,\ldots,X_n \sim \text{Bern}(p)$, \;sufficient stat $T = \sum X_i$.

\vspace{0.1cm}
\begin{center}
$\underbrace{\tilde{p} = X_1}_{\text{naive: unbiased, Var} = p(1-p)}$
\quad$\xrightarrow{\;\;\mathbb{E}[\,\cdot\mid T\,]\;\;}$\quad
$\underbrace{\hat{p} = \mathbb{E}[X_1 \mid T] = T/n = \bar{X}}_{\text{improved: unbiased, Var} = p(1-p)/n}$
\quad\textcolor{paramgreen}{$\boldsymbol{\times n}$ \textbf{better!}}
\end{center}
\end{frame}

\begin{frame}
\frametitle{What Does $\mathbb{E}[\tilde\theta \mid T]$ Actually Mean?}

\small
\textbf{Concrete example:} $X_1, X_2, X_3 \sim \text{Bern}(p)$, \;$T = X_1 + X_2 + X_3$, \;$\tilde{p} = X_1$.

\vspace{0.15cm}
Suppose someone tells you $T = 2$ (two successes). Which data vectors give $T = 2$?

\vspace{0.1cm}
\begin{center}
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{cccc}
  $(X_1, X_2, X_3)$ & $T$ & $\tilde{p} = X_1$ & Equally likely? \\
  \hline
  $(1, 1, 0)$ & $2$ & $1$ & Yes \\
  $(1, 0, 1)$ & $2$ & $1$ & Yes \\
  $(0, 1, 1)$ & $2$ & $0$ & Yes \\
  \hline
\end{tabular}
\end{center}

\pause
\vspace{0.05cm}
$$\mathbb{E}[X_1 \mid T = 2] = \frac{1 + 1 + 0}{3} = \frac{2}{3} = \frac{T}{n} \quad\textcolor{paramgreen}{\checkmark}$$

\vspace{0.05cm}
\begin{center}
\fcolorbox{popblue}{popblue!5}{\parbox{11cm}{\centering\small
  \textbf{``Condition on $T$''} means: average $\tilde\theta$ over all data configurations\\
  that produce the same value of $T$. The noise (which specific $X_i$'s\\
  are 1 vs 0) gets averaged away. Only the useful part ($T$) survives.
}}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Why Does Rao--Blackwell Work?}

\small
The key is the \textbf{law of total variance} (a.k.a.\ Eve's law):
\vspace{-0.2cm}
$$\text{Var}(\tilde\theta) = \underbrace{\mathbb{E}\!\left[\text{Var}(\tilde\theta \mid T)\right]}_{\text{``useless'' noise}\;\geq\, 0} + \text{Var}\!\left(\underbrace{\mathbb{E}[\tilde\theta \mid T]}_{\hat\theta}\right)$$

\pause
\vspace{-0.2cm}
Since the first term $\geq 0$, we immediately get:
$\;\boxed{\text{Var}(\tilde\theta) \;\geq\; \text{Var}(\hat\theta)}$

\vspace{0.05cm}
\begin{center}
\begin{tikzpicture}[
  box/.style={draw, rounded corners=5pt, minimum width=5cm, minimum height=1.2cm, align=center, inner sep=5pt, font=\small}
]
  \node[box, fill=sampred!8] at (-4.2, 0) {
    $\tilde\theta$ \textbf{(before)}\\[2pt]
    Uses data in a noisy way
  };
  \node[font=\Large\bfseries, popblue] at (-1.2, 0) {$\xrightarrow{\;\;\mathbb{E}[\,\cdot\mid T\,]\;\;}$};
  \node[box, fill=paramgreen!8] at (2.2, 0) {
    $\hat\theta = \mathbb{E}[\tilde\theta \mid T]$ \textbf{(after)}\\[2pt]
    Averages out the noise
  };
\end{tikzpicture}
\end{center}

\vspace{-0.05cm}
\begin{center}
\fcolorbox{popblue}{popblue!5}{\parbox{11cm}{\centering\small
  \textbf{Intuition:} $T$ captures all the useful information about $\theta$.
  Conditioning on $T$ removes the ``useless'' randomness (the part that
  doesn't tell us about $\theta$). What's left is a cleaner estimator.
}}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Finding Minimal Sufficient Statistics}

\small
\textbf{Theorem (Likelihood Ratio Criterion):} $T(\mathbf{X})$ is minimal sufficient iff for all $\mathbf{x}, \mathbf{y}$:

$$T(\mathbf{x}) = T(\mathbf{y}) \quad\Longleftrightarrow\quad \frac{f(\mathbf{x} \mid \theta)}{f(\mathbf{y} \mid \theta)} \;\text{does not depend on}\; \theta$$

\vspace{0.15cm}
\textbf{Bernoulli example:} $X_1, \ldots, X_n \sim \text{Bern}(p)$.

$$\frac{f(\mathbf{x} \mid p)}{f(\mathbf{y} \mid p)}
= \frac{p^{\sum x_i}(1-p)^{n-\sum x_i}}{p^{\sum y_i}(1-p)^{n-\sum y_i}}
= \left(\frac{p}{1-p}\right)^{\!\sum x_i - \sum y_i}$$

Free of $p$ $\;\Longleftrightarrow\;$ $\sum x_i = \sum y_i$.
\;\;So $T = \sum X_i$ is \textbf{minimal sufficient} for $p$. \;\textcolor{paramgreen}{$\checkmark$}

\vspace{0.2cm}
\begin{center}
\fcolorbox{orange1}{orange1!5}{\parbox{11cm}{\centering\small
  \textbf{Recipe:} Write the likelihood ratio $f(\mathbf{x}\mid\theta)/f(\mathbf{y}\mid\theta)$.\\
  Find which function of the data must match for the ratio to lose its $\theta$-dependence.\\
  That function is the minimal sufficient statistic.
}}
\end{center}
\end{frame}

% ============================================================
\section{Exponential Family}

\begin{frame}
\frametitle{The Exponential Family: A Unifying Framework}

\small
All our examples --- Bernoulli, Normal, Poisson, Exponential --- share one structure:

$$\boxed{f(x \mid \theta) = h(x)\;\exp\!\Big(\eta(\theta)\,T(x) - A(\theta)\Big)}$$

\vspace{-0.1cm}
\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{lccc}
  \textbf{Distribution} & \textbf{Natural param}~$\eta(\theta)$ & $T(x)$ & \textbf{Suff.\ stat} ($n$ obs) \\
  \hline
  $\text{Bern}(p)$ & $\log\frac{p}{1-p}$ & $x$ & $\sum X_i$ \\[1pt]
  $N(\mu,\sigma_0^2)$ \;($\sigma_0^2$ known) & $\mu/\sigma_0^2$ & $x$ & $\sum X_i$ \\[1pt]
  $\text{Pois}(\lambda)$ & $\log\lambda$ & $x$ & $\sum X_i$ \\[1pt]
  $\text{Exp}(\lambda)$ & $-\lambda$ & $x$ & $\sum X_i$ \\
  \hline
\end{tabular}
\end{center}

\vspace{0.1cm}
\begin{center}
\fcolorbox{popblue}{popblue!5}{\parbox{11cm}{\centering\small
  \textbf{Pattern:} For single-parameter families, $T(x) = x$. The sufficient statistic\\
  for $n$ observations is always $\sum T(X_i)$ --- straight from the factorization theorem!
}}
\end{center}
\end{frame}

% --- Item 10: Fix exponential families slide ---
\begin{frame}
\frametitle{Why Exponential Families Are Special}

\small
Nearly every nice property we've discussed is \textbf{automatic} in exponential families:

\vspace{0.05cm}
\begin{center}
\begin{tikzpicture}[
  pbox/.style={draw=popblue!60, fill=popblue!6, rounded corners=4pt, minimum width=12.5cm, align=left, inner sep=4pt, font=\small}
]
  \node[pbox] at (0, 1.2) {\textbf{Sufficiency:} $T(\mathbf{X}) = \sum T(X_i)$ is sufficient \textbf{and minimal}};
  \node[pbox] at (0, 0.35) {\textbf{Completeness:} the natural sufficient statistic is \textbf{complete} (see below)};
  \node[pbox] at (0, -0.5) {\textbf{Regularity:} all conditions for the Cram\'er--Rao bound (coming soon) are satisfied};
  \node[pbox] at (0, -1.35) {\textbf{Optimal estimators exist:} we'll see this when we reach the CR bound};
\end{tikzpicture}
\end{center}

\vspace{0.05cm}
\begin{center}
\fcolorbox{paramgreen}{paramgreen!5}{\parbox{12cm}{\centering\small
  \textbf{Completeness} means: if $\mathbb{E}_\theta[g(T)] = 0$ for all $\theta$, then $g(T) = 0$ a.s.
  $\to$ \textbf{no non-trivial unbiased estimator of zero} based on $T$.\\[3pt]
  \textbf{Lehmann--Scheff\'e:} An unbiased estimator based on a \textbf{complete} sufficient
  statistic is the \textbf{unique best} unbiased estimator (UMVUE).
  For exp.\ families, $\sum T(X_i)$ is always complete $\Rightarrow$ UMVUE exists!
}}
\end{center}
\end{frame}

% ============================================================
% --- Items 11--16: Fisher Information and Cram\'er--Rao ---
\section{Fisher Information and Cram\'er--Rao}

\begin{frame}
\frametitle{Can We Do Better? The Fundamental Question}

\begin{center}
\begin{tikzpicture}
  \node[draw=popblue, fill=popblue!5, rounded corners=10pt, text width=12cm, align=center, inner sep=12pt] {
    We know $\text{Var}(\bar{X}) = \sigma^2/n$ for estimating the mean.\\[8pt]
    {\Large Can \textbf{any} unbiased estimator have \textbf{lower} variance?}\\[8pt]
    Or is $\bar{X}$ already the best we can do?
  };
\end{tikzpicture}
\end{center}

\vspace{0.3cm}
To answer this, we need to measure \textbf{how much information} one observation carries about $\theta$.

\vspace{0.2cm}
\begin{center}
\begin{tikzpicture}
  \node[draw=orange1!60, fill=orange1!6, rounded corners=5pt, text width=12cm, align=center, inner sep=8pt, font=\small] {
    \textbf{Roadmap:}\\[4pt]
    \textbf{Why log?}
    $\;\to\;$ \textbf{Score function} (sensitivity of the model to $\theta$)
    $\;\to\;$ \textbf{Fisher information}\\
    $\to\;$ \textbf{Cram\'er--Rao bound} (the variance floor)
  };
\end{tikzpicture}
\end{center}
\end{frame}

% --- Item 11: Why we use log ---
\begin{frame}
\frametitle{Why the Logarithm? From Products to Sums}

The likelihood for i.i.d.\ data is a \textbf{product}:
$$L(\theta) = \prod_{i=1}^n f(X_i \mid \theta)$$

\vspace{0.1cm}
Taking the log turns this into a \textbf{sum}:
$$\ell(\theta) = \log L(\theta) = \sum_{i=1}^n \log f(X_i \mid \theta)$$

\vspace{0.1cm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textcolor{sampred}{\textbf{Products are painful:}}
\begin{itemize}\setlength{\itemsep}{2pt}
  \small
  \item Multiplying tiny numbers $\to$ underflow
  \item Product rule for derivatives is messy
  \item Hard to work with analytically
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textcolor{paramgreen}{\textbf{Sums are friendly:}}
\begin{itemize}\setlength{\itemsep}{2pt}
  \small
  \item Numerically stable
  \item Derivative of a sum = sum of derivatives
  \item LLN, CLT apply directly
\end{itemize}
\end{column}
\end{columns}

\vspace{0.2cm}
\begin{center}
\fcolorbox{popblue}{popblue!5}{\parbox{10cm}{\centering\small
  \textbf{Key fact:} $\log$ is monotonically increasing, so\\
  $\arg\max_\theta L(\theta) = \arg\max_\theta \ell(\theta)$. Same maximizer!
}}
\end{center}
\end{frame}

\begin{frame}
\frametitle{The Score Function: How Sensitive Is the Model?}

\small
Given a model $f(x \mid \theta)$, the \textbf{score} measures how the log-probability changes with $\theta$:
$$s(\theta) = \frac{\partial}{\partial\theta} \log f(X \mid \theta)$$

\vspace{0.1cm}
\textbf{Concrete example:} $X \sim \text{Bernoulli}(p)$.

\vspace{0.1cm}
$\log f(x \mid p) = x \log p + (1{-}x)\log(1{-}p)$

$$s(p) = \frac{\partial}{\partial p}\left[x \log p + (1{-}x)\log(1{-}p)\right] = \frac{x}{p} - \frac{1-x}{1-p} = \frac{x - p}{p(1-p)}$$

\vspace{0.1cm}
\begin{itemize}\setlength{\itemsep}{3pt}
  \item If we observe $x = 1$ and $p$ is small, the score is \textbf{large positive} $\to$ ``$p$ should be higher''
  \item If we observe $x = 0$ and $p$ is large, the score is \textbf{large negative} $\to$ ``$p$ should be lower''
  \item On average: $\mathbb{E}[s(p)] = 0$ --- the score points in the right direction but \textbf{averages out}
\end{itemize}
\end{frame}

% --- Item 12: Better Bernoulli derivation for Fisher info ---
\begin{frame}
\frametitle{Fisher Information: How Informative Is One Observation?}

The score averages to zero, but it \textbf{varies}. More variation $=$ more information:

\vspace{-0.15cm}
$$\boxed{I(\theta) = \text{Var}[s(\theta)] = \mathbb{E}\!\left[\left(\frac{\partial}{\partial\theta}\log f(X \mid \theta)\right)^{\!2}\right]}$$

\vspace{-0.2cm}
\begin{columns}[T]
\begin{column}{0.53\textwidth}
\small
\textbf{Bernoulli derivation:} We found $s(p) = \frac{X - p}{p(1-p)}$.

\vspace{0.05cm}
Since $\mathbb{E}[s] = 0$:
\vspace{-0.1cm}
\begin{align*}
I(p) &= \mathbb{E}[s^2] = \mathbb{E}\!\left[\frac{(X-p)^2}{p^2(1{-}p)^2}\right]\\[2pt]
  &= \frac{\text{Var}(X)}{p^2(1{-}p)^2} = \frac{p(1{-}p)}{p^2(1{-}p)^2} = \boxed{\frac{1}{p(1{-}p)}}
\end{align*}
\end{column}
\begin{column}{0.45\textwidth}
\begin{center}
\begin{tikzpicture}
  \begin{axis}[
    width=6.5cm, height=4cm,
    xlabel={$p$},
    ylabel={$I(p)$},
    xmin=0.02, xmax=0.98,
    ymin=0, ymax=30,
    axis lines=left,
    grid=major, grid style={gray!15},
    every axis plot/.append style={line width=2pt, samples=100},
  ]
    \addplot[popblue, domain=0.02:0.98] {1/(x*(1-x))};
    \fill[sampred] (axis cs:0.5, 4) circle (3pt);
    \node[font=\scriptsize, sampred, above right] at (axis cs:0.5, 4) {min};
    \fill[orange1] (axis cs:0.1, 11.11) circle (3pt);
  \end{axis}
\end{tikzpicture}
\end{center}
\scriptsize\centering $p$ near 0 or 1: very informative. $p = 0.5$: max noise, min info.
\end{column}
\end{columns}
\end{frame}

% --- Item 13: Second derivative form ---
\begin{frame}
\frametitle{Fisher Information: Two Equivalent Forms}

\small
Under regularity conditions, there is an equivalent formula that's often easier to compute:

\vspace{0.1cm}
$$\boxed{I(\theta) = \mathbb{E}\!\left[s(\theta)^2\right] = -\mathbb{E}\!\left[\frac{\partial^2}{\partial\theta^2}\log f(X \mid \theta)\right]}$$

\vspace{0.05cm}
\textbf{Why are these the same?} Start from $\mathbb{E}[s(\theta)] = 0$ and differentiate both sides w.r.t.\ $\theta$:
$$0 = \frac{\partial}{\partial\theta}\mathbb{E}[s] = \mathbb{E}\!\left[\frac{\partial s}{\partial\theta}\right] + \mathbb{E}\!\left[s \cdot s\right] = \mathbb{E}[\ell''] + \mathbb{E}[s^2]$$

\vspace{-0.1cm}
So: $\;\mathbb{E}[s^2] = -\mathbb{E}[\ell'']$. \;\textcolor{paramgreen}{$\checkmark$}

\vspace{0.15cm}
\textbf{Verify for Bernoulli:} $\;\ell(p) = x\log p + (1{-}x)\log(1{-}p)$
$$\ell''(p) = -\frac{x}{p^2} - \frac{1-x}{(1-p)^2} \quad\Rightarrow\quad -\mathbb{E}[\ell''] = \frac{p}{p^2} + \frac{1-p}{(1-p)^2} = \frac{1}{p} + \frac{1}{1-p} = \frac{1}{p(1-p)} \;\;\textcolor{paramgreen}{\checkmark}$$
\end{frame}

% --- Item 14: Fix title overlap ---
\begin{frame}
\frametitle{Intuition: Sharp vs Flat Log-Likelihood}
\begin{center}
\begin{tikzpicture}
  % High information
  \begin{scope}[xshift=-5cm]
    \draw[thick, ->] (0, 0) -- (4.5, 0) node[right, font=\small] {$\theta$};
    \draw[thick, ->] (0, -0.2) -- (0, 3.2);
    \node[font=\small, rotate=90] at (-0.5, 1.5) {$\ell(\theta)$};
    \draw[very thick, paramgreen, smooth, domain=0:4, samples=50]
      plot (\x, {3*exp(-(\x-2)*(\x-2)/0.2)});
    \draw[dashed, popblue] (2, 0) -- (2, 3.1);
    \node[font=\small\bfseries, paramgreen] at (2, 3.5) {High info: sharp};
    \node[font=\small] at (2, -0.7) {Precise estimate};
  \end{scope}

  % Low information
  \begin{scope}[xshift=2cm]
    \draw[thick, ->] (0, 0) -- (4.5, 0) node[right, font=\small] {$\theta$};
    \draw[thick, ->] (0, -0.2) -- (0, 3.2);
    \node[font=\small, rotate=90] at (-0.5, 1.5) {$\ell(\theta)$};
    \draw[very thick, sampred, smooth, domain=0:4, samples=50]
      plot (\x, {2*exp(-(\x-2)*(\x-2)/1.5)});
    \draw[dashed, popblue] (2, 0) -- (2, 2.1);
    \node[font=\small\bfseries, sampred] at (2, 3.5) {Low info: flat};
    \node[font=\small] at (2, -0.7) {Uncertain estimate};
  \end{scope}
\end{tikzpicture}
\end{center}

\vspace{0.2cm}
\begin{center}
\small $I(\theta)$ measures the \textbf{curvature} of the log-likelihood at the true $\theta$.\\[4pt]
Sharp curve $\Rightarrow$ high $I(\theta)$ $\Rightarrow$ data is very informative $\Rightarrow$ estimator is precise.\\[4pt]
This connects the two forms: $I(\theta) = -\mathbb{E}[\ell'']$ is literally the expected curvature.
\end{center}
\end{frame}

% --- Item 15: Cram√©r-Rao in 2 slides ---
\begin{frame}
\frametitle{Cram\'er--Rao Lower Bound}

\small
Now we can answer the fundamental question. For any \textbf{unbiased} estimator $\hat\theta$ based on $n$ i.i.d.\ observations:

$$\boxed{\text{Var}(\hat\theta) \;\geq\; \frac{1}{n \cdot I(\theta)}}$$

\vspace{0.1cm}
\textbf{Intuition:} Why $\frac{1}{n \cdot I(\theta)}$?
\begin{itemize}\setlength{\itemsep}{3pt}
  \item \textbf{More observations ($n$ large)} $\Rightarrow$ bound gets smaller $\Rightarrow$ can estimate more precisely
  \item \textbf{More informative data ($I(\theta)$ large)} $\Rightarrow$ bound gets smaller $\Rightarrow$ each observation tells us more
  \item The bound is \textbf{tight} for many models --- it's the actual achievable precision
\end{itemize}

\vspace{0.15cm}
\textbf{Verify for Bernoulli:}

\vspace{-0.1cm}
\begin{center}
\small
$I(p) = \frac{1}{p(1-p)}$ \quad$\Rightarrow$\quad CR bound: $\text{Var}(\hat{p}) \geq \frac{1}{n \cdot \frac{1}{p(1-p)}} = \frac{p(1-p)}{n}$

\vspace{0.1cm}
Actual variance of $\hat{p} = \bar{X}$: $\;\text{Var}(\hat{p}) = \frac{p(1-p)}{n}$ \;\;\textcolor{paramgreen}{$\checkmark$ Hits the bound exactly!}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Cram\'er--Rao: Efficiency and Practical Use}

\begin{center}
\begin{tikzpicture}[
  cbox/.style={draw, rounded corners=5pt, minimum width=5cm, minimum height=1.5cm, align=center, text width=4.5cm, inner sep=6pt, font=\small}
]
  \node[cbox, fill=popblue!8] at (-4, 0) {
    \textbf{What it says:}\\[2pt]
    There is a \textbf{floor} on how precise any unbiased estimator can be
  };
  \node[cbox, fill=paramgreen!8] at (0, 0) {
    \textbf{Efficient estimator:}\\[2pt]
    Achieves the bound ---\\
    the \textbf{best possible}
  };
  \node[cbox, fill=orange1!8] at (4, 0) {
    \textbf{Practical use:}\\[2pt]
    Tells you whether to keep\\
    searching for a better estimator
  };
\end{tikzpicture}
\end{center}

\vspace{0.2cm}
\begin{center}
\renewcommand{\arraystretch}{1.8}
\begin{tabular}{lcccl}
  \textbf{Model} & \textbf{Estimator} & $\text{Var}(\hat\theta)$ & \textbf{CR bound} & \textbf{Efficient?} \\
  \hline
  $\text{Bern}(p)$ & $\hat{p} = \bar{X}$ & $\frac{p(1-p)}{n}$ & $\frac{p(1-p)}{n}$ & \textcolor{paramgreen}{\textbf{Yes}} \\
  $N(\mu, \sigma^2_0)$ & $\hat\mu = \bar{X}$ & $\frac{\sigma^2_0}{n}$ & $\frac{\sigma^2_0}{n}$ & \textcolor{paramgreen}{\textbf{Yes}} \\
  $\text{Exp}(\lambda)$ & $\hat\lambda = 1/\bar{X}$ & $\frac{\lambda^2}{n}$ & $\frac{\lambda^2}{n}$ & \textcolor{paramgreen}{\textbf{Yes}} \\
  \hline
\end{tabular}
\end{center}
\end{frame}

% --- Item 16: Regularity conditions, Armenian colors ---
\begin{frame}
\frametitle{Regularity Conditions: When Does CR Apply?}

\vspace{-0.2cm}
\small
The Cram\'er--Rao bound requires \textbf{regularity conditions}:

\vspace{0.05cm}
\begin{columns}[T]
\begin{column}{0.50\textwidth}
\footnotesize
\begin{enumerate}\setlength{\itemsep}{1pt}
  \item \textbf{Support} of $f(x \mid \theta)$ doesn't depend on $\theta$
  \item $\theta$ in the \textbf{interior} of the parameter space
  \item Can \textbf{differentiate under the integral} sign\\(swap $\frac{\partial}{\partial\theta}$ and $\int$)
  \item $0 < I(\theta) < \infty$ \;(finite, positive info)
\end{enumerate}

\vspace{0.1cm}
\small\textbf{Counterexample: $\text{Uniform}(0, \theta)$}
\begin{itemize}\setlength{\itemsep}{1pt}
  \footnotesize
  \item Support $[0, \theta]$ depends on $\theta$! (violates \#1)
  \item Suff.\ stat: $X_{(n)} = \max_i X_i$
  \item $\text{Var}(X_{(n)}) \sim 1/n^2$ --- \textbf{faster} than CR!\\
    {\scriptsize (CR would give $1/n$, but $1/n^2$ is possible here)}
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\vspace{-0.1cm}
\begin{center}
\begin{tikzpicture}[scale=0.65]
  % theta = 2 (RED - top of Armenian flag)
  \fill[sampred!15] (0, 2.2) rectangle (2.2, 2.8);
  \draw[very thick, sampred] (0, 2.5) -- (2.2, 2.5);
  \draw[thick, sampred, dashed] (2.2, 2.2) -- (2.2, 2.8);
  \node[left, font=\footnotesize] at (-0.1, 2.5) {$\theta\!=\!2$};
  \node[right, font=\footnotesize, sampred] at (2.3, 2.5) {$\frac{1}{2}$};

  % theta = 3 (BLUE - middle of Armenian flag)
  \fill[popblue!15] (0, 1.1) rectangle (3.3, 1.7);
  \draw[very thick, popblue] (0, 1.4) -- (3.3, 1.4);
  \draw[thick, popblue, dashed] (3.3, 1.1) -- (3.3, 1.7);
  \node[left, font=\footnotesize] at (-0.1, 1.4) {$\theta\!=\!3$};
  \node[right, font=\footnotesize, popblue] at (3.4, 1.4) {$\frac{1}{3}$};

  % theta = 5 (ORANGE - bottom of Armenian flag)
  \fill[orange1!15] (0, 0) rectangle (5.0, 0.6);
  \draw[very thick, orange1] (0, 0.3) -- (5.0, 0.3);
  \draw[thick, orange1, dashed] (5.0, 0) -- (5.0, 0.6);
  \node[left, font=\footnotesize] at (-0.1, 0.3) {$\theta\!=\!5$};
  \node[right, font=\footnotesize, orange1] at (5.1, 0.3) {$\frac{1}{5}$};

  % x-axis
  \draw[thick, ->] (0, -0.3) -- (5.8, -0.3) node[right, font=\small] {$x$};
  \node[below, font=\footnotesize] at (0, -0.3) {$0$};

  \node[font=\scriptsize, warnred, text width=4.5cm, align=center] at (2.5, -1.0) {Support boundary \textbf{moves} with $\theta$};
\end{tikzpicture}
\end{center}
\end{column}
\end{columns}

\vspace{-0.1cm}
\begin{center}
\fcolorbox{paramgreen}{paramgreen!5}{\parbox{10cm}{\centering\small
  \textbf{Good news:} All exponential family distributions automatically satisfy\\
  the regularity conditions. The CR bound always applies to them.
}}
\end{center}
\end{frame}

% ============================================================
\section{Admissibility and Minimax}

\begin{frame}
\frametitle{Admissibility}

\small
\textbf{Definition:} $\hat\theta_1$ is \textbf{inadmissible} if $\exists\;\hat\theta_2$ that \textbf{dominates} it:
$$\text{MSE}(\hat\theta_2, \theta) \leq \text{MSE}(\hat\theta_1, \theta) \;\;\forall\,\theta, \quad\text{with strict inequality for some } \theta$$

\vspace{-0.1cm}
An estimator is \textbf{admissible} if no other estimator dominates it.

\vspace{0.1cm}
\begin{center}
\begin{tikzpicture}
  \draw[thick, ->] (0, 0) -- (10, 0) node[right, font=\small] {$\theta$};
  \draw[thick, ->] (0, 0) -- (0, 3.5) node[above, font=\small] {MSE};

  \draw[very thick, sampred, smooth] plot coordinates {
    (0.5, 2.5) (2, 2.2) (4, 2.8) (6, 2.5) (8, 3.0) (9.5, 2.7)
  };
  \node[font=\small\bfseries, sampred] at (9, 3.5) {$\hat\theta_1$};

  \draw[very thick, paramgreen, smooth] plot coordinates {
    (0.5, 1.8) (2, 1.5) (4, 2.0) (6, 1.8) (8, 2.2) (9.5, 1.9)
  };
  \node[font=\small\bfseries, paramgreen] at (9, 1.4) {$\hat\theta_2$};

  \draw[-{Stealth}, thick, orange1] (5, 2.7) -- (5, 2.1);
  \node[font=\footnotesize, orange1, right] at (5.1, 2.4) {dominates};
\end{tikzpicture}
\end{center}

\vspace{0.1cm}
\begin{center}
\small $\hat\theta_1$ is \textbf{inadmissible} --- $\hat\theta_2$ is at least as good everywhere, and strictly better somewhere.
\end{center}
\end{frame}

% --- Item 17: Stein's Paradox in more detail (2 slides) ---
\begin{frame}
\frametitle{Stein's Paradox (1956)}

\vspace{-0.1cm}
\begin{center}
\begin{tikzpicture}
  \node[draw=warnred, fill=warnred!5, rounded corners=8pt, text width=12cm, align=center, inner sep=7pt] {
    {\large\bfseries Surprising fact:}\\[3pt]
    When estimating $\boldsymbol{\mu} = (\mu_1, \ldots, \mu_d)$ from $X_i \sim N(\mu_i, 1)$,\\[2pt]
    the sample mean $\hat\mu_i = X_i$ is \textbf{inadmissible} when $d \geq 3$!
  };
\end{tikzpicture}
\end{center}

\vspace{-0.05cm}
\begin{columns}[T]
\begin{column}{0.52\textwidth}
\small
The \textbf{James--Stein estimator} dominates it:
$$\hat\mu_i^{\,JS} = \left(1 - \frac{d-2}{\|\mathbf{X}\|^2}\right) X_i$$

\vspace{-0.25cm}
\begin{itemize}\setlength{\itemsep}{0pt}
  \small
  \item \textbf{Shrinks} each $X_i$ toward $0$
  \item Works even if $\mu_i$'s are unrelated!
  \item A little bias buys a lot of\\variance reduction
\end{itemize}
\end{column}
\begin{column}{0.46\textwidth}
\begin{center}
\begin{tikzpicture}[scale=0.78]
  \draw[gray!40, thin] (-2.2, 0) -- (2.2, 0);
  \draw[gray!40, thin] (0, -1.8) -- (0, 2.2);
  \fill[orange1] (0, 0) circle (3pt);
  \node[font=\scriptsize, orange1, below left] at (-0.05, -0.05) {$\mathbf{0}$};
  \fill[sampred] (1.8, 1.5) circle (3pt);
  \fill[paramgreen] (1.08, 0.9) circle (3pt);
  \draw[-{Stealth[length=5pt]}, popblue, thick] (1.7, 1.42) -- (1.18, 0.98);
  \fill[sampred] (-1.6, 1.2) circle (3pt);
  \fill[paramgreen] (-0.96, 0.72) circle (3pt);
  \draw[-{Stealth[length=5pt]}, popblue, thick] (-1.5, 1.14) -- (-1.06, 0.8);
  \fill[sampred] (0.8, -1.6) circle (3pt);
  \fill[paramgreen] (0.48, -0.96) circle (3pt);
  \draw[-{Stealth[length=5pt]}, popblue, thick] (0.74, -1.5) -- (0.54, -1.04);
  \node[font=\scriptsize, sampred] at (1.8, 2.1) {$X_i$ (raw)};
  \node[font=\scriptsize, paramgreen] at (1.8, 1.7) {$\hat\mu_i^{JS}$ (shrunk)};
\end{tikzpicture}
\end{center}
\end{column}
\end{columns}

\vspace{-0.05cm}
\begin{center}
\small Paradox: estimating the average temperature in Yerevan \emph{improves} if you\\
jointly estimate it with the price of tea in China and the height of the Eiffel Tower.
\end{center}
\end{frame}

\begin{frame}
\frametitle{Why Does Stein's Paradox Work?}

\small
\textbf{The MSE comparison} tells the whole story:

\vspace{0.05cm}
\begin{center}
\begin{tikzpicture}[
  cbox/.style={draw, rounded corners=5pt, minimum width=5.5cm, minimum height=1cm, align=center, inner sep=5pt, font=\normalsize}
]
  \node[cbox, fill=sampred!8] at (-4, 0) {$\text{MSE}(\mathbf{X}) = d$\\{\small ($1$ per coordinate)}};
  \node[cbox, fill=paramgreen!8] at (4, 0) {$\text{MSE}(\hat{\boldsymbol{\mu}}^{JS}) < d$\\{\small (always, when $d \geq 3$)}};
  \draw[-{Stealth}, very thick, popblue] (-1, 0) -- (1, 0);
  \node[font=\footnotesize, popblue, above] at (0, 0.1) {shrinkage helps};
\end{tikzpicture}
\end{center}

\vspace{0.05cm}
\textbf{Why $d \geq 3$?} The shrinkage factor $\frac{d-2}{\|\mathbf{X}\|^2}$ needs to be estimated from data.
\begin{itemize}\setlength{\itemsep}{1pt}
  \item In $d = 1$ or $2$: not enough ``room'' --- estimation error of the shrinkage factor wipes out the gain
  \item In $d \geq 3$: $\|\mathbf{X}\|^2$ concentrates well enough $\to$ shrinkage factor is accurate $\to$ net win
\end{itemize}

\vspace{0.05cm}
\begin{center}
\fcolorbox{violet1}{violet1!5}{\parbox{11cm}{\centering\small
  \textbf{Connection to ML:} James--Stein shrinkage is an early form of \textbf{regularization}.\\[1pt]
  Ridge regression ($L^2$ penalty) does the same thing: shrink coefficients toward zero.
  The bias-variance tradeoff in action: a little bias buys a lot of variance reduction.
}}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Minimax Estimators}

\small A \textbf{minimax} estimator minimizes the \textbf{worst-case} risk:
$$\hat\theta_{\text{minimax}} = \arg\min_{\hat\theta}\; \max_{\theta}\; \text{MSE}(\hat\theta, \theta)$$

\vspace{-0.2cm}
\begin{center}
\begin{tikzpicture}
  \draw[thick, ->] (0, 0) -- (10, 0) node[right, font=\small] {$\theta$};
  \draw[thick, ->] (0, 0) -- (0, 4) node[above, font=\small] {MSE};

  \draw[very thick, sampred, smooth] plot coordinates {
    (0.5, 1.0) (2, 0.8) (4, 1.5) (6, 3.5) (8, 3.8) (9.5, 4.0)
  };
  \node[font=\small\bfseries, sampred] at (9.5, 4.4) {$\hat\theta_1$};
  \draw[dashed, sampred!50] (0, 4.0) -- (10, 4.0);
  \node[left, font=\footnotesize, sampred] at (0, 4.0) {max risk};

  \draw[very thick, popblue, smooth] plot coordinates {
    (0.5, 2.0) (2, 2.1) (4, 2.3) (6, 2.2) (8, 2.4) (9.5, 2.3)
  };
  \node[font=\small\bfseries, popblue] at (9.5, 1.8) {$\hat\theta_{\text{mm}}$};
  \draw[dashed, popblue!50] (0, 2.4) -- (10, 2.4);
  \node[left, font=\footnotesize, popblue] at (0, 2.4) {max risk};
\end{tikzpicture}
\end{center}

\vspace{-0.1cm}
\begin{center}
\small Minimax = \textbf{conservative}: protects against the worst $\theta$. Minimax hedges.
\end{center}
\end{frame}

\begin{frame}
\frametitle{Three Philosophies of Estimation}
\begin{center}
\begin{tikzpicture}[
  rbox/.style={draw, rounded corners=5pt, minimum width=4.8cm, minimum height=2.6cm, align=center, text width=4.5cm, inner sep=5pt, font=\small}
]
  \node[rbox, fill=popblue!8] at (-4.5, 0) {
    \textbf{\normalsize Plug-in (unbiased)}\\[4pt]
    Use sample statistic directly\\
    ($\bar{X}$, $S^2$, $\hat{p}$)\\[4pt]
    Admissible in $d = 1$\\
    \textcolor{warnred}{Inadmissible in $d \geq 3$}
  };
  \node[rbox, fill=orange1!8] at (0, 0) {
    \textbf{\normalsize Shrinkage}\\[4pt]
    Pull estimates toward a\\
    central value (e.g.\ $0$)\\[4pt]
    Biased but lower MSE\\
    (James--Stein)
  };
  \node[rbox, fill=paramgreen!8] at (4.5, 0) {
    \textbf{\normalsize Minimax}\\[4pt]
    Minimize worst-case risk\\[4pt]
    Conservative guarantee\\
    No single $\theta$ can hurt you badly
  };
\end{tikzpicture}
\end{center}

\vspace{0.2cm}
\begin{center}
\fcolorbox{violet1}{violet1!5}{\parbox{11cm}{\centering\small
  \textbf{Takeaway:} In high dimensions ($d \geq 3$), shrinkage estimators are provably better\\
  than using each sample statistic on its own. We'll see more of this in later lectures.
}}
\end{center}
\end{frame}

% ============================================================
\section{Summary}

% --- Item 18: What we haven't covered ---
\begin{frame}
\frametitle{What We Haven't Covered (Yet)}

\small
This lecture focused on \textbf{point estimation} --- producing a single ``best guess'' for $\theta$.
But there's much more to statistical inference:

\vspace{0.1cm}
\begin{center}
\begin{tikzpicture}[
  fbox/.style={draw=popblue!60, fill=popblue!6, rounded corners=4pt, minimum width=12.5cm, align=left, inner sep=4pt, font=\small}
]
  \node[fbox] at (0, 2.15) {\textbf{Confidence intervals:} How uncertain is our estimate? (Lectures 5--6)};
  \node[fbox] at (0, 1.3) {\textbf{Hypothesis testing:} Is the effect real or just noise? (Lectures 7--8)};
  \node[fbox] at (0, 0.45) {\textbf{Bayesian estimation:} Incorporating prior beliefs (Lecture 5)};
  \node[fbox] at (0, -0.4) {\textbf{Bootstrap:} Resampling to estimate uncertainty without formulas (Lecture 6)};
  \node[fbox] at (0, -1.25) {\textbf{Asymptotic theory:} What happens as $n \to \infty$ in general? (Lecture 5)};
  \node[fbox] at (0, -2.1) {\textbf{Nonparametric estimation:} What if we don't assume a distribution at all?};
\end{tikzpicture}
\end{center}

\vspace{0.05cm}
\centering\small Today's tools (bias, MSE, CR bound, sufficiency) will be the \textbf{foundation} for all of these.
\end{frame}

\begin{frame}
\frametitle{Summary: How to Judge an Estimator}
\begin{center}
\begin{tikzpicture}[
  sbox/.style={draw=popblue!60, fill=popblue!5, rounded corners=4pt, minimum width=12.5cm, minimum height=0.6cm, align=left, inner sep=5pt, font=\small}
]
  \node[sbox] at (0, 3.15) {\textbf{Bias:} $\;\mathbb{E}[\hat\theta] - \theta$. Does it aim at the right place?};
  \node[sbox] at (0, 2.25) {\textbf{Variance:} $\;\text{Var}(\hat\theta)$. How much does it jump around?};
  \node[sbox] at (0, 1.35) {\textbf{MSE} $= \text{Bias}^2 + \text{Var}$. Total error. Biased can beat unbiased!};
  \node[sbox] at (0, 0.45) {\textbf{Consistency:} $\;\hat\theta_n \xrightarrow{P} \theta$. Converges to truth with enough data.};
  \node[sbox] at (0, -0.45) {\textbf{Sufficiency:} $T(\mathbf{X})$ captures everything about $\theta$. Compress without loss.};
  \node[sbox] at (0, -1.35) {\textbf{Cram\'er--Rao:} $\;\text{Var} \geq 1/(n \cdot I(\theta))$. The efficiency floor.};
  \node[sbox] at (0, -2.25) {\textbf{Admissibility:} No other estimator dominates it everywhere.};
  \node[sbox] at (0, -3.15) {\textbf{Minimax:} Best worst-case guarantee. Shrinkage often wins.};
\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Homework}
\begin{enumerate}
  \item Show that $\bar{X}$ is unbiased for $\mu$ and compute its MSE.

  \vspace{0.2cm}
  \item Show that $\hat\sigma^2_n = \frac{1}{n}\sum(X_i - \bar{X})^2$ is biased for $\sigma^2$. Find the bias.

  \vspace{0.2cm}
  \item Compute the Fisher information $I(\theta)$ for $\text{Poisson}(\lambda)$.\\
    Use it to find the Cram\'er--Rao lower bound for estimating $\lambda$.\\
    Is $\hat\lambda = \bar{X}$ efficient?

  \vspace{0.2cm}
  \item Suppose you shrink $\bar{X}$ toward 0: $\;\hat\mu_c = c\bar{X}$ for $0 < c < 1$.\\
    Find the bias, variance, and MSE as functions of $c$.\\
    For what value of $c$ is MSE minimized? Is the optimal estimator biased?

  \vspace{0.2cm}
  \item Use the factorization theorem to show that $T = \sum X_i$ is a sufficient statistic\\
    for $\lambda$ when $X_1, \ldots, X_n \sim \text{Poisson}(\lambda)$.
\end{enumerate}
\end{frame}

\begin{frame}
\begin{center}
  {\Huge\bfseries\textcolor{popblue}{Questions?}}
\end{center}
\end{frame}

\end{document}
