\documentclass[aspectratio=169]{beamer}

\usetheme{default}
\usecolortheme{dove}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}{%
  \hfill{\large\insertframenumber\,/\,\inserttotalframenumber}\hspace{0.8em}\vspace{0.5em}%
}

\definecolor{popblue}{RGB}{52, 101, 164}
\definecolor{sampred}{RGB}{204, 0, 0}
\definecolor{paramgreen}{RGB}{0, 140, 70}
\definecolor{warnred}{RGB}{180, 40, 40}
\definecolor{orange1}{RGB}{220, 120, 0}
\definecolor{violet1}{RGB}{120, 50, 160}
\definecolor{lightbg}{RGB}{245, 245, 250}

\setbeamercolor{frametitle}{fg=popblue}
\setbeamercolor{title}{fg=popblue}

\usepackage{pgfplots}
\usepackage{tikz}
\usetikzlibrary{shapes, arrows.meta, positioning, calc, decorations.pathreplacing, patterns}
\pgfplotsset{compat=1.18}
\usepackage{amsmath, amssymb}
\usepackage{fontenc}

\title{Lecture 6: MAP Estimation}
\subtitle{Priors $\cdot$ Posteriors $\cdot$ Regularization Connection}
\date{}

\begin{document}

% ============================================================
\begin{frame}
\titlepage
\end{frame}

% ============================================================
\begin{frame}
\frametitle{Previously, on Lecture 5\ldots}

\begin{center}
\begin{tikzpicture}[
  sbox/.style={draw=popblue!60, fill=popblue!5, rounded corners=4pt, minimum width=12.5cm, minimum height=0.5cm, align=left, inner sep=5pt, font=\small}
]
  \node[sbox] at (0, 2.0) {\textbf{MoM:} Set population moments $=$ sample moments. Simple but can give impossible values.};
  \node[sbox] at (0, 1.2) {\textbf{MLE:} $\hat\theta = \arg\max \ell(\theta)$. Pick the $\theta$ that makes the data most probable.};
  \node[sbox] at (0, 0.4) {\textbf{Properties:} Consistent, asymptotically normal, efficient, invariant.};
  \node[sbox] at (0, -0.4) {\textbf{MLE = ML:} Gaussian noise $\to$ MSE loss. Bernoulli $\to$ cross-entropy.};
  \node[sbox] at (0, -1.2) {\textbf{But:} MLE can overfit with small $n$ or flexible models.};
\end{tikzpicture}
\end{center}

\vspace{0.3cm}
\begin{center}
{\large\textbf{Today:}} What if we have \textbf{prior knowledge} about $\theta$?\\[4pt]
Can we do better than MLE by incorporating beliefs \textit{before} seeing data?
\end{center}
\end{frame}

% ============================================================
\begin{frame}
\frametitle{Where We Are}
\begin{center}
\begin{tikzpicture}[
  box/.style={draw, rounded corners=6pt, minimum width=3.8cm, minimum height=1.2cm, align=center, font=\normalsize}
]
  \node[box, fill=popblue!15] (mle) at (-4, 0) {\textbf{MLE}\\{\small maximize $P(\text{data} \mid \theta)$}};
  \node[box, fill=orange1!15] (map) at (4, 0) {\textbf{MAP}\\{\small maximize $P(\theta \mid \text{data})$}};

  \draw[-{Stealth}, very thick, line width=2pt] (mle) -- (map)
    node[midway, above, font=\normalsize\bfseries] {What if we have prior beliefs?};

  \node[font=\small] at (-4, -1.2) {Lecture 5};
  \node[font=\small, orange1] at (4, -1.2) {This lecture};
\end{tikzpicture}
\end{center}
\end{frame}

% ============================================================
\section{Bayes' Theorem for Parameters}

\begin{frame}
\frametitle{Bayes' Theorem for Parameters}
\begin{center}
\begin{tikzpicture}
  \node[draw=violet1, fill=violet1!5, rounded corners=10pt, text width=12cm, align=center, inner sep=15pt] {
    {\Large
    $\underbrace{P(\theta \mid \text{data})}_{\text{posterior}} \;=\; \frac{\overbrace{P(\text{data} \mid \theta)}^{\text{likelihood}} \;\cdot\; \overbrace{P(\theta)}^{\text{prior}}}{\underbrace{P(\text{data})}_{\text{evidence}}}$
    }\\[15pt]
    \small Or simply: \quad $\text{posterior} \;\propto\; \text{likelihood} \;\times\; \text{prior}$
  };
\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}
\frametitle{The Three Ingredients}
\begin{center}
\begin{tikzpicture}[
  ibox/.style={draw, rounded corners=6pt, minimum width=3.5cm, minimum height=3cm, align=center, text width=3.2cm, inner sep=6pt}
]
  \node[ibox, fill=popblue!10] at (-5, 0) {
    \textbf{\large Prior $P(\theta)$}\\[6pt]
    \small What you believed\\
    \textit{before} seeing data\\[8pt]
    \footnotesize ``Coins are usually\\
    close to fair''
  };
  \node[ibox, fill=sampred!10] at (0, 0) {
    \textbf{\large Likelihood}\\
    \textbf{\large $P(\text{data} \mid \theta)$}\\[6pt]
    \small What the data says\\
    (same as MLE)\\[8pt]
    \footnotesize ``7 heads in 10 flips''
  };
  \node[ibox, fill=violet1!10] at (5, 0) {
    \textbf{\large Posterior}\\
    \textbf{\large $P(\theta \mid \text{data})$}\\[6pt]
    \small Updated belief\\
    \textit{after} seeing data\\[8pt]
    \footnotesize ``Coin is probably\\
    slightly biased''
  };

  \draw[-{Stealth}, thick] (-3, 0) -- (-1.8, 0);
  \node[font=\Large] at (-2.4, 0.5) {$\times$};
  \draw[-{Stealth}, thick] (1.8, 0) -- (3, 0);
  \node[font=\Large] at (2.4, 0.5) {$=$};
\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Visualizing the Update: Coin Bias}
\begin{center}
\begin{tikzpicture}
  \begin{axis}[
    width=13cm, height=6cm,
    xlabel={$p$ (probability of heads)},
    ylabel={Density},
    xmin=0, xmax=1,
    ymin=0, ymax=5.5,
    axis lines=left,
    legend style={at={(0.02,0.95)}, anchor=north west, font=\small},
    grid=major, grid style={gray!15},
    every axis plot/.append style={line width=2pt, samples=100}
  ]
    % Prior: Beta(5,5) - centered at 0.5
    \addplot[popblue, domain=0.01:0.99] {(x^4 * (1-x)^4) / 0.00397};
    \addlegendentry{Prior: Beta(5,5)}

    % Likelihood: 7 heads in 10 (proportional)
    \addplot[sampred, dashed, domain=0.01:0.99] {3.2 * x^7 * (1-x)^3 / 0.267};
    \addlegendentry{Likelihood (7H, 3T)}

    % Posterior: Beta(12,8)
    \addplot[violet1, domain=0.01:0.99] {(x^11 * (1-x)^7) / 0.0000516};
    \addlegendentry{Posterior: Beta(12,8)}

    % MAP and MLE markers
    \draw[thick, violet1, dashed] (axis cs:0.611, 0) -- (axis cs:0.611, 4.5);
    \node[font=\footnotesize\bfseries, violet1] at (axis cs:0.58, 4.8) {MAP = 0.61};

    \draw[thick, sampred, dashed] (axis cs:0.7, 0) -- (axis cs:0.7, 2.8);
    \node[font=\footnotesize\bfseries, sampred] at (axis cs:0.78, 3.1) {MLE = 0.7};
  \end{axis}
\end{tikzpicture}
\end{center}
\small Prior pulls the estimate from 0.7 toward 0.5. The posterior is a \textbf{compromise}.
\end{frame}

\begin{frame}
\frametitle{Conjugate Priors as Pseudo-Observations}

\small
Why did the math work out so neatly? Because Beta is \textbf{conjugate} to Binomial:

\vspace{0.15cm}
\begin{center}
\begin{tikzpicture}[
  cbox/.style={draw, rounded corners=6pt, minimum height=1.2cm, align=center, inner sep=8pt, font=\normalsize}
]
  \node[cbox, fill=popblue!10] (prior) at (-4.5, 0) {Prior: $\text{Beta}(\alpha, \beta)$};
  \node[font=\Large] at (-2, 0) {$+$};
  \node[cbox, fill=sampred!10] (data) at (0, 0) {Data: $k$ H, $n{-}k$ T};
  \node[font=\Large] at (2, 0) {$=$};
  \node[cbox, fill=violet1!10] (post) at (5, 0) {Posterior: $\text{Beta}(\alpha{+}k,\; \beta{+}n{-}k)$};
\end{tikzpicture}
\end{center}

\pause
\vspace{0.15cm}
\begin{center}
\fcolorbox{orange1}{orange1!5}{\parbox{12cm}{\centering
  \textbf{The prior acts like ``fake data'' you've already seen:}\\[4pt]
  \small Beta$(\alpha, \beta)$ $\;=\;$ pretend you already observed $\alpha{-}1$ heads and $\beta{-}1$ tails.\\[3pt]
  Beta$(5,5)$: ``I've seen 4H and 4T'' (8 pseudo-observations).\\[3pt]
  After 7H, 3T (10 real obs): posterior = Beta$(12, 8)$ $\;=\;$ ``11H, 7T out of 18 total.''
}}
\end{center}

\vspace{0.15cm}
\small As $n \to \infty$, the pseudo-observations become negligible $\Rightarrow$ posterior $\to$ likelihood $\Rightarrow$ MAP $\to$ MLE.
\end{frame}

% ============================================================
\section{MAP Estimation}

\begin{frame}
\frametitle{MAP = Mode of the Posterior}
\begin{center}
\begin{tikzpicture}
  \node[draw=violet1, fill=violet1!5, rounded corners=10pt, text width=12cm, align=center, inner sep=15pt] {
    {\Large
    $\hat{\theta}_{\text{MAP}} = \arg\max_\theta \; P(\theta \mid \text{data}) = \arg\max_\theta \bigl[\ell(\theta) + \log P(\theta)\bigr]$
    }\\[12pt]
    \small Maximize: \quad log-likelihood $+$ log-prior
  };
\end{tikzpicture}
\end{center}

\vspace{0.5cm}
\begin{center}
\begin{tikzpicture}[
  cbox/.style={draw, rounded corners=4pt, minimum height=0.8cm, align=center, font=\normalsize, inner sep=6pt}
]
  \node[cbox, fill=popblue!10] at (-3, 0) {MLE: $\arg\max_\theta \; \ell(\theta)$};
  \node[cbox, fill=violet1!10] at (4, 0) {MAP: $\arg\max_\theta \; \ell(\theta) + \log P(\theta)$};
  \draw[-{Stealth}, thick] (0.3, 0) -- (1, 0);
  \node[font=\small] at (0.65, 0.5) {$+$ prior};
\end{tikzpicture}
\end{center}

\vspace{0.3cm}
\small MAP = MLE with an extra penalty/bonus term from the prior.
\end{frame}

\begin{frame}
\frametitle{When Does the Prior Matter?}
\begin{center}
\begin{tikzpicture}[xscale=0.95]
  % Small n
  \begin{scope}[xshift=-4.5cm]
    \node[font=\bfseries] at (2, 3.6) {Small $n$ (e.g., $n=5$)};
    \begin{axis}[
      width=6cm, height=4cm,
      xmin=0, xmax=1, ymin=0, ymax=5,
      axis lines=left,
      xtick={0,0.5,1}, ytick=\empty,
      every axis plot/.append style={line width=1.5pt, samples=80},
      at={(0,0)}
    ]
      \addplot[popblue, domain=0.01:0.99] {(x^4*(1-x)^4)/0.00397};
      \addplot[violet1, domain=0.01:0.99] {(x^7*(1-x)^6)/0.00116};
    \end{axis}
    \node[font=\small] at (2, -0.5) {Prior has \textbf{large} influence};
  \end{scope}

  % Large n
  \begin{scope}[xshift=4.5cm]
    \node[font=\bfseries] at (2, 3.6) {Large $n$ (e.g., $n=500$)};
    \begin{axis}[
      width=6cm, height=4cm,
      xmin=0.55, xmax=0.85, ymin=0, ymax=25,
      axis lines=left,
      xtick={0.6, 0.7, 0.8}, ytick=\empty,
      every axis plot/.append style={line width=1.5pt, samples=80},
      at={(0,0)}
    ]
      % Posterior with n=500: very narrow, centered near MLE
      \addplot[violet1, domain=0.55:0.85] {exp(-500*(x-0.7)^2 / (2*0.7*0.3)) * 18};
      % Prior: almost flat in this range
      \addplot[popblue, domain=0.55:0.85] {2.5};
    \end{axis}
    \node[font=\small] at (2, -0.5) {Prior has \textbf{negligible} influence};
  \end{scope}

  % Bottom message
  \node[draw=popblue, fill=popblue!5, rounded corners=4pt, text width=10cm, align=center, font=\small, inner sep=6pt] at (0, -1.8) {
    With enough data, the likelihood dominates $\Rightarrow$ MAP $\approx$ MLE.\\
    The prior is ``washed out'' by the data.
  };
\end{tikzpicture}
\end{center}
\end{frame}

% ============================================================
\section{Regularization as MAP}

\begin{frame}
\frametitle{The Key Connection: Regularization = MAP}
\begin{center}
\begin{tikzpicture}
  \node[draw=orange1, fill=orange1!5, rounded corners=10pt, text width=12.5cm, align=center, inner sep=12pt] {
    {\large MAP: $\;\hat{\theta} = \arg\max_\theta \bigl[\ell(\theta) + \log P(\theta)\bigr]$}\\[8pt]
    is the same as\\[8pt]
    {\large Regularization: $\;\hat{\theta} = \arg\min_\theta \bigl[-\ell(\theta) + \lambda \cdot \text{penalty}(\theta)\bigr]$}\\[10pt]
    \small The log-prior acts as a \textbf{penalty on the parameters}.
  };
\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Gaussian Prior $\Leftrightarrow$ Ridge (L2) Regression}
\begin{center}
\begin{tikzpicture}
  % Left: prior
  \begin{scope}[xshift=-5cm]
    \node[font=\bfseries, popblue] at (1.8, 3.2) {Gaussian prior};
    \begin{axis}[
      width=5cm, height=4cm,
      xmin=-3, xmax=3, ymin=0, ymax=0.5,
      axis lines=middle,
      xtick=\empty, ytick=\empty,
      every axis plot/.append style={line width=2pt, samples=80},
      at={(0,0)},
      xlabel={$\theta$}
    ]
      \addplot[popblue, domain=-3:3] {exp(-x^2/2)/sqrt(2*pi)};
    \end{axis}
    \node[font=\small] at (1.8, -0.8) {$P(\theta) = \mathcal{N}(0, \tau^2)$};
  \end{scope}

  % Arrow
  \node[font=\Huge] at (0, 1.5) {$\Rightarrow$};

  % Right: penalty
  \begin{scope}[xshift=4cm]
    \node[font=\bfseries, sampred] at (2.5, 3.2) {L2 penalty};
    \begin{axis}[
      width=5cm, height=4cm,
      xmin=-3, xmax=3, ymin=0, ymax=5,
      axis lines=middle,
      xtick=\empty, ytick=\empty,
      every axis plot/.append style={line width=2pt, samples=80},
      at={(0,0)},
      xlabel={$\theta$}
    ]
      \addplot[sampred, domain=-3:3] {x^2};
    \end{axis}
    \node[font=\small] at (2.5, -0.8) {$-\log P(\theta) \propto \|\theta\|_2^2$};
  \end{scope}

  % Bottom equation
  \node[draw=popblue, fill=popblue!5, rounded corners=6pt, text width=11cm, align=center, inner sep=10pt] at (0, -2.5) {
    $\hat{\theta}_{\text{MAP}} = \arg\min_\theta \left[\sum_{i=1}^n(y_i - \mathbf{x}_i^\top\theta)^2 + \lambda\|\theta\|_2^2\right]$\\[4pt]
    \small This is exactly \textbf{Ridge regression}! $\lambda = \sigma^2/\tau^2$
  };
\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Laplace Prior $\Leftrightarrow$ Lasso (L1) Regression}
\begin{center}
\begin{tikzpicture}
  % Left: prior
  \begin{scope}[xshift=-5cm]
    \node[font=\bfseries, paramgreen] at (1.8, 3.2) {Laplace prior};
    \begin{axis}[
      width=5cm, height=4cm,
      xmin=-3, xmax=3, ymin=0, ymax=0.55,
      axis lines=middle,
      xtick=\empty, ytick=\empty,
      every axis plot/.append style={line width=2pt, samples=80},
      at={(0,0)},
      xlabel={$\theta$}
    ]
      \addplot[paramgreen, domain=-3:3] {0.5*exp(-abs(x))};
    \end{axis}
    \node[font=\small] at (1.8, -0.8) {$P(\theta) \propto e^{-|\theta|/b}$};
  \end{scope}

  % Arrow
  \node[font=\Huge] at (0, 1.5) {$\Rightarrow$};

  % Right: penalty
  \begin{scope}[xshift=4cm]
    \node[font=\bfseries, orange1] at (2.5, 3.2) {L1 penalty};
    \begin{axis}[
      width=5cm, height=4cm,
      xmin=-3, xmax=3, ymin=0, ymax=3.5,
      axis lines=middle,
      xtick=\empty, ytick=\empty,
      every axis plot/.append style={line width=2pt, samples=80},
      at={(0,0)},
      xlabel={$\theta$}
    ]
      \addplot[orange1, domain=-3:3] {abs(x)};
    \end{axis}
    \node[font=\small] at (2.5, -0.8) {$-\log P(\theta) \propto \|\theta\|_1$};
  \end{scope}

  % Bottom equation
  \node[draw=paramgreen, fill=paramgreen!5, rounded corners=6pt, text width=11cm, align=center, inner sep=10pt] at (0, -2.5) {
    $\hat{\theta}_{\text{MAP}} = \arg\min_\theta \left[\sum_{i=1}^n(y_i - \mathbf{x}_i^\top\theta)^2 + \lambda\|\theta\|_1\right]$\\[4pt]
    \small This is exactly \textbf{Lasso regression}! Encourages \textbf{sparse} solutions ($\theta_j = 0$).
  };
\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}
\frametitle{The Regularization Map}
\begin{center}
\begin{tikzpicture}[
  mbox/.style={draw, rounded corners=5pt, minimum width=4cm, minimum height=1.4cm, align=center, font=\normalsize, inner sep=5pt}
]
  % Prior column
  \node[font=\large\bfseries, popblue] at (-3.5, 3.5) {Prior (Bayesian)};
  \node[mbox, fill=popblue!10] (g) at (-3.5, 2) {Gaussian $\mathcal{N}(0, \tau^2)$};
  \node[mbox, fill=popblue!10] (l) at (-3.5, 0) {Laplace$(0, b)$};
  \node[mbox, fill=popblue!10] (u) at (-3.5, -2) {Uniform (flat)};

  % Regularizer column
  \node[font=\large\bfseries, sampred] at (3.5, 3.5) {Penalty (Frequentist)};
  \node[mbox, fill=sampred!10] (r) at (3.5, 2) {Ridge: $\lambda\|\theta\|_2^2$\\[1pt]\footnotesize\textit{shrinks toward 0}};
  \node[mbox, fill=sampred!10] (la) at (3.5, 0) {Lasso: $\lambda\|\theta\|_1$\\[1pt]\footnotesize\textit{sets some to exactly 0}};
  \node[mbox, fill=sampred!10] (no) at (3.5, -2) {No penalty\\[1pt]\footnotesize\textit{= MLE, no shrinkage}};

  \draw[-{Stealth}, thick, popblue] (g) -- (r);
  \draw[-{Stealth}, thick, paramgreen] (l) -- (la);
  \draw[-{Stealth}, thick, orange1] (u) -- (no);
\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Visualizing Ridge Shrinkage}
\begin{center}
\begin{tikzpicture}
  \begin{axis}[
    width=12cm, height=6cm,
    xlabel={$\lambda$ (regularization strength = prior confidence)},
    ylabel={Coefficient $\hat{\theta}_j$},
    xmin=0, xmax=10,
    ymin=-1.5, ymax=3.5,
    axis lines=left,
    grid=major, grid style={gray!15},
    every axis plot/.append style={line width=1.5pt, samples=80},
    legend style={at={(0.98,0.95)}, anchor=north east, font=\small}
  ]
    % Several coefficients shrinking toward 0
    \addplot[popblue, domain=0:10] {3 * exp(-0.15*x)};
    \addlegendentry{$\theta_1$}
    \addplot[sampred, domain=0:10] {2 * exp(-0.2*x)};
    \addlegendentry{$\theta_2$}
    \addplot[paramgreen, domain=0:10] {-1 * exp(-0.12*x)};
    \addlegendentry{$\theta_3$}
    \addplot[orange1, domain=0:10] {0.5 * exp(-0.25*x)};
    \addlegendentry{$\theta_4$}

    % Zero line
    \draw[dashed, gray] (axis cs:0,0) -- (axis cs:10,0);

    % Labels
    \node[font=\small] at (axis cs:1, -1.2) {$\lambda = 0$: MLE};
    \node[font=\small] at (axis cs:9, -1.2) {$\lambda \to \infty$: all $\to 0$};
  \end{axis}
\end{tikzpicture}
\end{center}
\small Increasing $\lambda$ = stronger prior = more shrinkage = less overfitting (but more bias).
\end{frame}

\begin{frame}
\frametitle{MLE vs MAP: The Overfitting Story}

\small
Fit a polynomial to noisy data. MLE uses all parameters freely; MAP penalizes large coefficients.

\vspace{0.1cm}
\begin{center}
\begin{tikzpicture}
  \begin{axis}[
    width=12cm, height=5.5cm,
    xlabel={$x$}, ylabel={$y$},
    xmin=-0.1, xmax=1.1, ymin=-1.5, ymax=2.5,
    axis lines=left,
    grid=major, grid style={gray!15},
    legend style={at={(0.02,0.95)}, anchor=north west, font=\small, fill=white, fill opacity=0.9},
  ]
    % True function: sin(2*pi*x)
    \addplot[gray, dashed, line width=1pt, domain=0:1, samples=100] {sin(deg(2*pi*x))};
    \addlegendentry{True function}

    % Noisy data points
    \addplot[only marks, mark=*, mark size=2.5pt, popblue] coordinates {
      (0.05, 0.4) (0.1, 0.7) (0.15, 0.5) (0.2, 1.1) (0.25, 0.8)
      (0.3, 1.0) (0.35, 0.6) (0.4, 0.3) (0.5, -0.1) (0.55, -0.3)
      (0.6, -0.7) (0.65, -0.9) (0.7, -0.6) (0.75, -1.0) (0.8, -0.8)
      (0.85, -0.3) (0.9, 0.1) (0.95, 0.5) (1.0, 0.2)
    };
    \addlegendentry{Data}

    % MLE (high-degree, overfitting) - wiggly
    \addplot[sampred, line width=1.5pt, domain=0:1, samples=200] {
      sin(deg(2*pi*x)) + 0.6*sin(deg(8*pi*x)) + 0.4*cos(deg(12*pi*x))
    };
    \addlegendentry{MLE (degree 15): overfit}

    % MAP/Ridge (smooth, close to truth)
    \addplot[paramgreen, line width=2pt, domain=0:1, samples=100] {
      0.95*sin(deg(2*pi*x))
    };
    \addlegendentry{MAP / Ridge: smooth}
  \end{axis}
\end{tikzpicture}
\end{center}

\vspace{-0.1cm}
\centering\small The prior says ``coefficients should be small'' $\Rightarrow$ smoother fit $\Rightarrow$ better generalization.
\end{frame}

% ============================================================
\section{MLE vs MAP vs Full Bayesian}

\begin{frame}
\frametitle{Three Philosophies}
\begin{center}
\begin{tikzpicture}[
  pbox/.style={draw, rounded corners=6pt, minimum width=3.5cm, minimum height=3.5cm, align=center, text width=3.2cm, inner sep=6pt}
]
  \node[pbox, fill=sampred!10] at (-5, 0) {
    \textbf{\large MLE}\\[6pt]
    \small Mode of likelihood\\[4pt]
    No prior info used\\[4pt]
    Risk: overfitting\\with small $n$\\[4pt]
    \footnotesize $\hat{\theta} = \arg\max \ell(\theta)$
  };
  \node[pbox, fill=violet1!10] at (0, 0) {
    \textbf{\large MAP}\\[6pt]
    \small Mode of posterior\\[4pt]
    Uses prior beliefs\\[4pt]
    Still a point estimate\\[4pt]
    \footnotesize $\hat{\theta} = \arg\max [\ell + \log P]$
  };
  \node[pbox, fill=paramgreen!10] at (5, 0) {
    \textbf{\large Full Bayesian}\\[6pt]
    \small Entire posterior\\[4pt]
    Full uncertainty\\[4pt]
    Computationally\\expensive (MCMC)\\[4pt]
    \footnotesize $P(\theta \mid \text{data})$
  };

  \draw[-{Stealth}, thick] (-3, -0.3) -- (-2, -0.3) node[midway, below, font=\footnotesize] {add prior};
  \draw[-{Stealth}, thick] (2, -0.3) -- (3, -0.3) node[midway, below, font=\footnotesize] {keep full dist.};
\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}
\frametitle{When to Use What}
\begin{center}
\begin{tikzpicture}[
  when/.style={draw, rounded corners=4pt, minimum height=1.5cm, text width=5.2cm, align=left, inner sep=8pt, font=\small}
]
  \node[when, fill=sampred!8] at (-4, 2) {
    \textbf{MLE} when:\\
    -- Large $n$ (prior doesn't matter)\\
    -- No reliable prior info\\
    -- Simplicity is valued
  };
  \node[when, fill=violet1!8] at (4, 2) {
    \textbf{MAP} when:\\
    -- Small $n$ (need regularization)\\
    -- Have domain knowledge\\
    -- Want a point estimate fast
  };
  \node[when, fill=paramgreen!8] at (0, -0.8) {
    \textbf{Full Bayesian} when:\\
    -- Uncertainty quantification matters (medical, safety)\\
    -- Model comparison needed\\
    -- Computational cost is acceptable
  };
\end{tikzpicture}
\end{center}
\end{frame}

% ============================================================
\section{Practical}

\begin{frame}
\frametitle{Practical: Priors and Posteriors}
\begin{center}
\begin{tikzpicture}
  \node[draw=popblue, fill=popblue!3, rounded corners=10pt, text width=12cm, align=left, inner sep=15pt] {
    \begin{enumerate}
      \item \textbf{Coin bias estimation:}
        \begin{itemize}
          \item Start with Beta(1,1), Beta(5,5), Beta(50,50) priors
          \item Observe 7 heads in 10 flips
          \item Plot prior, likelihood, and posterior for each
          \item Compare the MAP estimates --- how much does the prior pull?
        \end{itemize}
      \item \textbf{Ridge regression as MAP:}
        \begin{itemize}
          \item Fit linear regression with $\lambda = 0, 0.1, 1, 10, 100$
          \item Plot coefficients vs $\lambda$ (shrinkage path)
          \item Observe: larger $\lambda$ = stronger prior = more shrinkage
        \end{itemize}
      \item \textbf{Visualize:} Plot the prior/likelihood/posterior for a simple 1D Normal with known $\sigma^2$, varying the prior variance $\tau^2$
    \end{enumerate}
  };
\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Homework}
\begin{enumerate}
  \item For $X_1, \ldots, X_n \sim N(\mu, \sigma_0^2)$ (known $\sigma_0^2$) with prior $\mu \sim N(m, \tau^2)$:\\
    derive the MAP estimator $\hat\mu_{\text{MAP}}$. Show it is a weighted average of $\bar{X}$ and $m$.\\
    What happens as $\tau^2 \to \infty$? As $n \to \infty$?

  \vspace{0.15cm}
  \item A coin is flipped 20 times with 14 heads. Compute the MAP estimate of $p$ under:\\
    (a) Beta$(1,1)$, \;(b) Beta$(5,5)$, \;(c) Beta$(50,50)$ priors.\\
    Compare with the MLE. Which prior has the most influence?

  \vspace{0.15cm}
  \item Show that Ridge regression $\hat\theta = \arg\min \bigl[\|y - X\theta\|^2 + \lambda\|\theta\|^2\bigr]$\\
    has the closed-form solution $\hat\theta = (X^\top X + \lambda I)^{-1}X^\top y$.\\
    Why does this always have a unique solution, even when $X^\top X$ is singular?
\end{enumerate}
\end{frame}

\begin{frame}
\begin{center}
  {\Huge\bfseries\textcolor{popblue}{Questions?}}

  \vspace{1cm}

  {\large Next: Sampling distributions and confidence intervals}
\end{center}
\end{frame}

\end{document}
