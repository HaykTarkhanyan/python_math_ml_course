\documentclass[aspectratio=169]{beamer}

\usetheme{Madrid}
\usecolortheme{default}
\usefonttheme{professionalfonts}

\usepackage{amsmath, amsfonts, amssymb}
\usepackage{booktabs}
\usepackage{bm}

\title[Statistics for ML: Topic 0]{Statistics for Machine Learning\\Topic 0: Foundations \& the Statistics Mindset}
\author{Instructor: \;\;David Tarkhanyan}
\date{}

\begin{document}

% ---------------- Title ----------------
\begin{frame}
  \titlepage
\end{frame}

% ---------------- Roadmap ----------------
\begin{frame}{Topic 0 Roadmap}
\begin{itemize}
  \item 0.1 From probability to data: what statistics is doing
  \item 0.2 Parameters, estimators, and loss (why mean/median appear)
  \item 0.3 What can go wrong: dependence, drift, and selection
\end{itemize}
\end{frame}

% ---------------- Learning Objectives ----------------
\begin{frame}{Learning Objectives (Topic 0)}
By the end of this topic, students should be able to:
\begin{itemize}
  \item Explain the \textbf{population vs sample} distinction and the role of a \textbf{data-generating process}.
  \item Define \textbf{estimand}, \textbf{estimator}, \textbf{loss}, and \textbf{risk}.
  \item Relate \textbf{empirical risk minimization (ERM)} to standard ML training objectives.
  \item Identify common \textbf{assumption failures}: dependence, dataset shift, and selection bias.
\end{itemize}
\end{frame}

% =====================================================
% 0.1 From probability to data
% =====================================================
\section{0.1 From Probability to Data}

\begin{frame}{Why Statistics (in ML)?}
\textbf{Probability}:
\begin{itemize}
  \item Starts with a model/distribution and deduces consequences.
  \item Example: If $X \sim \mathcal{N}(\mu,\sigma^2)$, compute $\mathbb{P}(X \le x)$.
\end{itemize}

\vspace{0.4em}
\textbf{Statistics}:
\begin{itemize}
  \item Starts with data and infers unknown quantities (parameters, predictions, uncertainty).
  \item Example: Given samples $x_1,\dots,x_n$, estimate $\mu$ and quantify uncertainty.
\end{itemize}

\vspace{0.4em}
\textbf{ML connection:} training is typically estimating parameters by optimizing a criterion.
\end{frame}

\begin{frame}{Population vs Sample}
\textbf{Population} (or data-generating distribution):
\[
X \sim P \quad\text{(unknown in practice)}
\]

\textbf{Sample}:
\[
X_1,\dots,X_n \overset{\text{i.i.d.}}{\sim} P
\]

\begin{itemize}
  \item Population: ``all possible observations'' under the process of interest.
  \item Sample: the finite dataset we actually observe.
  \item \textbf{Goal:} learn something about $P$ (or parameters of a model for $P$) from the sample.
\end{itemize}
\end{frame}

\begin{frame}{The Data-Generating Process (DGP)}
A useful mental model:
\begin{enumerate}
  \item Nature chooses a distribution $P$.
  \item We observe data $X_1,\dots,X_n$ as draws from $P$ (sometimes approximately).
  \item We compute an estimator $\hat{\theta} = g(X_1,\dots,X_n)$.
  \item We report a value \emph{and} (ideally) uncertainty.
\end{enumerate}

\vspace{0.5em}
\textbf{Key point:} the dataset is a \textbf{realization of random variables}. Estimators are random too.
\end{frame}

\begin{frame}{The i.i.d. Assumption (and Why ML Loves It)}
\textbf{i.i.d.} means:
\begin{itemize}
  \item \textbf{Independent:} $X_i$ does not provide information about $X_j$ for $i\neq j$.
  \item \textbf{Identically distributed:} each $X_i$ comes from the same $P$.
\end{itemize}

\vspace{0.6em}
\textbf{Why it matters:}
\begin{itemize}
  \item Enables Laws of Large Numbers (stability of averages).
  \item Enables CLT-based approximations (uncertainty / standard errors).
  \item Justifies train/test splitting under ``same distribution'' assumption.
\end{itemize}
\end{frame}

\begin{frame}{Mini-Example: Conversion Rate}
Let $X_i \in \{0,1\}$ indicate whether user $i$ converts.

\begin{itemize}
  \item Estimand (parameter of interest): $p = \mathbb{P}(X=1)$.
  \item Natural estimator: $\hat{p} = \frac{1}{n}\sum_{i=1}^n X_i$.
\end{itemize}

\vspace{0.6em}
Interpretation:
\begin{itemize}
  \item $p$ is fixed but unknown (population truth).
  \item $\hat{p}$ is random (depends on which users appear in the sample).
\end{itemize}
\end{frame}

\begin{frame}{Quick Check: Identify Estimand vs Estimator}
For each scenario, identify (i) estimand and (ii) estimator.
\begin{enumerate}
  \item Average session length from logs.
  \item Fraud rate for transactions this month.
  \item Mean latency of an API endpoint.
  \item Click-through rate for a new UI variant.
\end{enumerate}

\vspace{0.6em}
\textbf{Rule of thumb:}
\[
\text{estimand} = \text{population quantity}, \qquad
\text{estimator} = \text{function of the sample}.
\]
\end{frame}

% =====================================================
% 0.2 Parameters, estimators, and loss
% =====================================================
\section{0.2 Estimation, Loss, and Risk}

\begin{frame}{Estimand, Estimator, and Error}
\textbf{Estimand:} $\theta$ (a property of $P$), e.g., mean $\mu$ or variance $\sigma^2$.

\textbf{Estimator:} $\hat{\theta} = g(X_1,\dots,X_n)$.

\textbf{Estimation error:} $\hat{\theta}-\theta$ (random).

\vspace{0.6em}
\textbf{Why we need a criterion:} which estimator is ``better'' depends on what we penalize.
\end{frame}

\begin{frame}{Loss and Risk}
\textbf{Loss function:} $L(\theta, x)$ measures how bad it is to choose $\theta$ when observing $x$.

\textbf{Risk (expected loss):}
\[
R(\theta) = \mathbb{E}_{X\sim P}\big[L(\theta, X)\big].
\]

\textbf{Empirical risk:}
\[
\hat{R}_n(\theta) = \frac{1}{n}\sum_{i=1}^n L(\theta, X_i).
\]

\vspace{0.4em}
\textbf{ML connection:} training often minimizes $\hat{R}_n(\theta)$ (ERM).
\end{frame}

\begin{frame}{Why the Mean Appears: Squared Loss}
Consider $L(\theta,x)=(x-\theta)^2$.

Empirical risk:
\[
\hat{R}_n(\theta) = \frac{1}{n}\sum_{i=1}^n (X_i-\theta)^2.
\]

Minimizer:
\[
\hat{\theta} = \arg\min_{\theta}\sum_{i=1}^n (X_i-\theta)^2 = \bar{X}.
\]

\vspace{0.4em}
\textbf{Interpretation:} the sample mean is the best constant predictor under squared loss.
\end{frame}

\begin{frame}{Why the Median Appears: Absolute Loss}
Consider $L(\theta,x)=|x-\theta|$.

Empirical risk:
\[
\hat{R}_n(\theta) = \frac{1}{n}\sum_{i=1}^n |X_i-\theta|.
\]

Minimizer:
\[
\hat{\theta} = \arg\min_{\theta}\sum_{i=1}^n |X_i-\theta| = \text{any median of } \{X_i\}.
\]

\vspace{0.4em}
\textbf{Interpretation:} the median is robust to outliers compared to the mean.
\end{frame}

\begin{frame}{Mode and MAP (Brief but Useful)}
For discrete $X$, the \textbf{mode} of a distribution maximizes probability mass.

In Bayesian settings:
\begin{itemize}
  \item Prior: $\pi(\theta)$
  \item Likelihood: $p(x\mid \theta)$
  \item Posterior: $p(\theta\mid x) \propto p(x\mid \theta)\,\pi(\theta)$
\end{itemize}

\textbf{MAP estimate:}
\[
\hat{\theta}_{\text{MAP}} = \arg\max_{\theta} \; p(\theta\mid x)
= \arg\max_{\theta}\big( \log p(x\mid \theta)+\log \pi(\theta)\big).
\]

\vspace{0.3em}
\textbf{ML connection:} MAP often corresponds to \textbf{regularized} optimization.
\end{frame}

\begin{frame}{In-Class Exercise: Mean vs Median Under Outliers}
Dataset A: $[10, 11, 9, 10, 10, 11, 9]$ \\
Dataset B: add one outlier: $[10, 11, 9, 10, 10, 11, 9, 100]$

\begin{itemize}
  \item Compute mean and median for A and B.
  \item Which estimator changes more? Why?
  \item What loss function would you choose if outliers reflect measurement errors?
\end{itemize}
\end{frame}

% =====================================================
% 0.3 What can go wrong
% =====================================================
\section{0.3 Assumptions and Failure Modes}

\begin{frame}{When i.i.d. Breaks: Three Common Failure Modes}
\begin{enumerate}
  \item \textbf{Dependence:} observations influence each other or are correlated (time series, grouped users).
  \item \textbf{Dataset shift:} training and deployment/test distributions differ.
  \item \textbf{Selection bias:} the sample is not representative of the target population.
\end{enumerate}

\vspace{0.4em}
\textbf{Practical consequence:} estimates and uncertainty can become systematically wrong.
\end{frame}

\begin{frame}{Dependence (Examples and Implications)}
Examples:
\begin{itemize}
  \item Time dependence: $X_t$ and $X_{t+1}$ correlated (latency, demand).
  \item Clustered data: many rows per user or per device.
  \item Network effects: one user’s treatment affects others (spillovers).
\end{itemize}

Implications:
\begin{itemize}
  \item Effective sample size is smaller than $n$.
  \item Naive standard errors / p-values can be overly optimistic.
\end{itemize}

Mitigation ideas (preview):
\begin{itemize}
  \item Blocked splits, cluster-robust methods, block bootstrap.
\end{itemize}
\end{frame}

\begin{frame}{Dataset Shift (High-Level Taxonomy)}
Let training distribution be $P_{\text{train}}(X,Y)$ and deployment be $P_{\text{test}}(X,Y)$.

Common cases:
\begin{itemize}
  \item \textbf{Covariate shift:} $P(X)$ changes, $P(Y\mid X)$ stable.
  \item \textbf{Concept drift:} $P(Y\mid X)$ changes over time.
  \item \textbf{Label shift:} $P(Y)$ changes, $P(X\mid Y)$ stable (sometimes plausible).
\end{itemize}

\textbf{ML impact:} model evaluation and calibration can degrade unexpectedly.
\end{frame}

\begin{frame}{Selection Bias (Sampling Is Part of the DGP)}
Selection bias occurs when inclusion in the dataset depends on variables related to the outcome.

Examples:
\begin{itemize}
  \item Only observing users who remain active (survivorship bias).
  \item Feedback loops in recommenders: what you show affects what you observe.
  \item Convenience samples: data from one region or device type only.
\end{itemize}

\vspace{0.3em}
\textbf{Key message:} ``more data'' does not fix biased sampling.
\end{frame}

\begin{frame}{Red-Flags Checklist (Operational)}
Ask before trusting an estimate:
\begin{itemize}
  \item Are observations independent (or clustered/time-correlated)?
  \item Is the sampling mechanism representative of the target population?
  \item Is the distribution stable over time (drift/seasonality)?
  \item Are train and test conditions aligned (same pipeline, same definition of labels)?
  \item Is there leakage (future information used in features)?
\end{itemize}
\end{frame}

\begin{frame}{Classification Exercise: What’s Wrong Here?}
For each scenario, classify the dominant issue:
\begin{itemize}
  \item Dependence
  \item Dataset shift
  \item Selection bias
\end{itemize}

\begin{enumerate}
  \item Train on last year’s data; deploy during a new marketing campaign.
  \item Multiple rows per user; random row-wise train/test split.
  \item Evaluate churn model only on users who opened the app last week.
  \item Compare two models on the same test set after many rounds of tuning.
\end{enumerate}
\end{frame}

% ---------------- Summary ----------------
\begin{frame}{Topic 0 Summary}
\begin{itemize}
  \item Statistics starts from data and infers population quantities.
  \item Estimators are random; we need loss/risk to define ``best''.
  \item Mean/median arise as optimal constants under different losses.
  \item i.i.d. is powerful but fragile: dependence, shift, and selection bias matter.
\end{itemize}

\vspace{0.5em}
\textbf{Next:} descriptive summaries, quantiles, ECDF, and sampling variability intuition.
\end{frame}

\end{document}
