---
title: "07 Calculus: Multivariate"
format:
  html:
    css: homework-styles.css
---

<script src="homework-scripts.js"></script>

![image.png](../background_photos/math_07_xaxoxish_pat.jpg)
Ô³ÕµÕ¸Ö‚Õ´Ö€Õ«, [Õ¬Õ¸Ö‚Õ½Õ¡Õ¶Õ¯Õ¡Ö€Õ« Õ°Õ²Õ¸Ö‚Õ´Õ¨](https://unsplash.com/photos/b8Iwra4rnRs), Õ€Õ¥Õ²Õ«Õ¶Õ¡Õ¯Õ [Naren Hakobyan](https://unsplash.com/@naren735)

# ğŸ“š Õ†ÕµÕ¸Ö‚Õ©Õ¨ ToDo

- [ğŸ“š Ô±Õ´Õ¢Õ¸Õ²Õ»Õ¡Õ¯Õ¡Õ¶ Õ¶ÕµÕ¸Ö‚Õ©Õ¨](01_vectors_linear_algebra.qmd)
- [ğŸ“º ÕÕ¥Õ½Õ¡Õ£Ö€Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶Õ¨](https://youtu.be/vectors_lecture)
- [ğŸï¸ ÕÕ¬Õ¡ÕµÕ¤Õ¥Ö€ - Vectors](Lectures/L01_Vectors.pdf)
- [ğŸï¸ ÕÕ¬Õ¡ÕµÕ¤Õ¥Ö€ - Geometry](Lectures/L02_Geometry_of_Vectors__Matrices.pdf)
- [ğŸ› ï¸ğŸ“º Ô³Õ¸Ö€Õ®Õ¶Õ¡Õ¯Õ¡Õ¶Õ« Õ¿Õ¥Õ½Õ¡Õ£Ö€Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶Õ¨](https://youtu.be/vectors_practical)
- [ğŸ› ï¸ğŸ—‚ï¸ Ô³Õ¸Ö€Õ®Õ¶Õ¡Õ¯Õ¡Õ¶Õ« PDF-Õ¨](Homeworks/hw_01_vectors.pdf)
  

# ğŸ¡ ÕÕ¶Õ¡ÕµÕ«Õ¶

::: {.callout-note collapse="false"}
1. â—â—â— DON'T CHECK THE SOLUTIONS BEFORE TRYING TO DO THE HOMEWORK BY YOURSELFâ—â—â—
2. Please don't hesitate to ask questions, never forget about the ğŸŠkaralyokğŸŠ principle!
3. The harder the problem is, the more ğŸ§€cheesesğŸ§€ it has.
4. Problems with ğŸ are just extra bonuses. It would be good to try to solve them, but also it's not the highest priority task.
5. If the problem involve many boring calculations, feel free to skip them - important part is understanding the concepts.
6. Submit your solutions [here](https://forms.gle/CFEvNqFiTSsDLiFc6) (even if it's unfinished)
:::

### 01 Gradient Descent Algorithm {data-difficulty="2"}

Consider the function $f(x_1, x_2) = x_1^2 + 2x_2^2 + x_1 x_2 + 4x_1 - 2x_2 + 5$.

We want to find the minimum. We could do it analytically, it's just a quadratic function, but quite soon we'll start to work with function for which we can't just find the optimum with pen and paper. So let's use an iterative algorithm. 

0. Open up VS code, or you're favorite IDE.
1. Pick a random starting point. 
2. In what direction should you move so that the function value decreases in the fastest way? 
3. Move in that direction (you may consider first multiplying that direction by some small value, let's say 0.1, that's the so called $\alpha$'learning rate' / 'step size', which we'll learn about later, but it basically controls how big steps you take. Too big step size -> you may overshoot the optimum value and diverge, too small, you may take too long time to converge, but often **"Let it be late, let it be almond"** principle holds) 
4. Keep on iterating like that. Can you come up with a stopping criteria? (e. g. if improvement / change smaller then x let's just stop the algorithm)
5. Plot and print interesting staff, e. g. function value vs iteration number
6. Play around with $\alpha$ and the starting point to see how it affects convergence.


### 02 Boat in Sevan {data-difficulty="1"}
ÕÖ‡Õ¡Õ¶Õ¡ Õ¬Õ³Õ« $(x, y)$ Õ¯Õ¸Õ¸Ö€Õ¤Õ«Õ¶Õ¡Õ¿Õ¶Õ¥Ö€Õ¸Õ¾ Õ¯Õ¥Õ¿Õ¸Ö‚Õ´ Õ»Ö€Õ« Õ­Õ¸Ö€Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶Õ¨

$$
f(x, y) = xy^2 - 6x^2 - 3y^2
$$

Õ´Õ¥Õ¿Ö€ Õ§Ö‰ $(5, 3)$ Õ¯Õ¥Õ¿Õ¸Ö‚Õ´ Õ£Õ¿Õ¶Õ¾Õ¸Õ² Â«Õ†Õ¸Ö€Õ¡Õ¿Õ¸Ö‚Õ½Â» Õ¡Õ¼Õ¡Õ£Õ¡Õ½Õ¿Õ¡Õ¶Õ¡Õ¾Õ« Õ¶Õ¡Õ¾Õ¡ÕºÕ¥Õ¿Õ¨ ÖÕ¡Õ¶Õ¯Õ¡Õ¶Õ¸Ö‚Õ´ Õ§ Õ·Õ¡Ö€ÕªÕ¾Õ¥Õ¬ Õ¤Õ¥ÕºÕ« Õ´Õ« Õ¡ÕµÕ¶ÕºÕ«Õ½Õ« Õ¯Õ¥Õ¿, Õ¸Ö€Õ¸Ö‚Õ´ Õ»Õ¸Ö‚Ö€Õ¶ Õ¡Õ¾Õ¥Õ¬Õ«
Õ­Õ¸Ö€ Õ§Ö‰ Õ†Ö€Õ¡ Õ¡Õ¼Õ¡Õ»Õ«Õ¶ Ö…Õ£Õ¶Õ¡Õ¯Õ¡Õ¶Õ¶ Õ¡Õ¼Õ¡Õ»Õ¡Ö€Õ¯Õ¸Ö‚Õ´ Õ§ Õ¶Õ¡Õ¾Õ¡Ö€Õ¯Õ¥Õ¬ Õ¤Õ¥ÕºÕ« Õ°ÕµÕ¸Ö‚Õ½Õ«Õ½,
Õ«Õ½Õ¯ Õ¥Ö€Õ¯Ö€Õ¸Ö€Õ¤Õ¨Õ Õ¤Õ¥ÕºÕ« Õ°Õ¡Ö€Õ¡Õ¾Ö‰ Õ•Õ£Õ¶Õ¡Õ¯Õ¡Õ¶Õ¶Õ¥Ö€Õ«Ö Õ¸Ö€Õ«Õ Õ­Õ¸Ö€Õ°Õ¸Ö‚Ö€Õ¤Õ¨ ÕºÕ¥Õ¿Ö„ Õ§
Õ¬Õ½Õ« Õ¶Õ¡Õ¾Õ¡ÕºÕ¥Õ¿Õ¨Ö‰

### 03 Topless box {data-difficulty="2"}

![topless_box](assets/7_4_box_withou_a_top.png)


Hint:
You may want to first do [this](https://hayktarkhanyan.github.io/python_math_ml_course/math/05_calc_extrema_convexity_taylor.html#box-problem) exercise.
### 04 Smooth Functions and Gradient Properties {data-difficulty="3"}


::: {.callout-note collapse="true"}
### Context: Smoothness
A function $f: \mathbb{R}^n \to \mathbb{R}$ is called **smooth** (or $C^\infty$) if it has continuous partial derivatives of all orders. In practice, we often work with $C^1$ functions (continuously differentiable) or $C^2$ functions (twice continuously differentiable).
:::

Consider the bivariate function $f : \mathbb{R}^2 \to \mathbb{R}$, $(x_1, x_2) \mapsto x_1^2 + 0.5x_2^2 + x_1x_2$.

1. Show that $f$ is smooth (continuously differentiable).
2. Find the direction of greatest increase of $f$ at $\mathbf{x} = (1, 1)$.
3. Find the direction of greatest decrease of $f$ at $\mathbf{x} = (1, 1)$.
4. Find a direction in which $f$ does not instantly change at $\mathbf{x} = (1, 1)$.
5. Assume there exists a differentiable parametrization of a curve $\tilde{\mathbf{x}} : \mathbb{R} \to \mathbb{R}^2$, $t \mapsto \tilde{\mathbf{x}}(t)$ such that $\forall t \in \mathbb{R} : f(\tilde{\mathbf{x}}(t)) = f(1,1)$. Show that at each point of the curve $\tilde{\mathbf{x}}$ the tangent vector $\frac{\partial \tilde{\mathbf{x}}}{\partial t}$ is perpendicular to $\nabla f(\tilde{\mathbf{x}})$.
6. Interpret parts (d) and (e) geometrically.

### 05 Contour Plots, Hessians, and Convexity {data-difficulty="3"}

::: {.callout-note collapse="true"}
### Context: Contour Plots and Convexity
**Contour plots** (or level curves) visualize multivariate functions by showing curves where $f(x_1, x_2) = c$ for various constants $c$. They're essential for understanding the shape of loss landscapes in machine learning.
:::

Let $f : \mathbb{R}^2 \to \mathbb{R}$, $(x_1, x_2) \mapsto -\cos(x_1^2 + x_2^2 + x_1x_2)$.

1. Create a contour plot of $f$ in the range $[-2,2] \times [-2,2]$ with R.
2. Compute $\nabla f$.
3. Compute $\nabla^2 f$ (the Hessian matrix).

Now, define the restriction of $f$ to $S_r = \{(x_1, x_2) \in \mathbb{R}^2 \mid x_1^2 + x_2^2 + x_1x_2 < r\}$ with $r \in \mathbb{R}, r > 0$, i.e., $f|_{S_r} : S_r \to \mathbb{R}, (x_1, x_2) \mapsto f(x_1, x_2)$.

4. Show that $f|_{S_r}$ with $r = \pi/4$ is convex.
5. Find the local minimum $\mathbf{x}^*$ of $f|_{S_r}$.
6. Is $\mathbf{x}^*$ a global minimum of $f$?


### 06 Taylor Expansion {data-difficulty="2"}

Consider the bivariate function $f : \mathbb{R}^2 \to \mathbb{R}, (x_1,x_2) \mapsto \exp(\pi \cdot x_1) - \sin(\pi \cdot x_2) + \pi \cdot x_1 \cdot x_2$.

1. Compute the gradient of $f$ for an arbitrary $x$.
2. Compute the Hessian of $f$ for an arbitrary $x$.
3. State the first order taylor polynomial $T_{1,a}(x)$ expanded around the point $a = (0,1)$.
4. State the second order taylor polynomial $T_{2,a}(x)$ expanded around the point $a = (0,1)$.
5. Determine if $T_{2,a}$ is a convex function.


# ğŸ› ï¸ Ô³Õ¸Ö€Õ®Õ¶Õ¡Õ¯Õ¡Õ¶
- [ğŸ› ï¸ğŸ“º Ô³Õ¸Ö€Õ®Õ¶Õ¡Õ¯Õ¡Õ¶Õ« Õ¿Õ¥Õ½Õ¡Õ£Ö€Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶Õ¨]()
- [ğŸ› ï¸ğŸ—‚ï¸ Ô³Õ¸Ö€Õ®Õ¶Õ¡Õ¯Õ¡Õ¶Õ« PDF-Õ¨]()

# ğŸ² 43 (07)
- ğŸ”¥[Landmark.am](https://landmark.am/)
- ğŸ”—[Random link](https://youtu.be/9_Gl0MBKrUU?si=ap5LjOjf7UM1hq8a)
- ğŸ‡¦ğŸ‡²ğŸ¶[Dorians (ÔµÕ½ Õ¯Õ¸Ö‚Õ¬Õ¡Õ´)](https://www.youtube.com/watch?v=0syTeqk9Hes)
- ğŸŒğŸ¶[Noize MC (Ğ’ÑĞµĞ»ĞµĞ½Ğ½Ğ°Ñ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ°?)](https://www.youtube.com/watch?v=DBnwy46OPFU)
- ğŸ¤Œ[Ô¿Õ¡Ö€Õ£Õ«Õ¶ ToDo]()

<a href="http://s01.flagcounter.com/more/1oO"><img src="https://s01.flagcounter.com/count2/1oO/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_0/pageviews_1/flags_0/percent_0/" alt="Flag Counter"></a>