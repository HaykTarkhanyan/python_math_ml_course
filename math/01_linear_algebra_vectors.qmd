---
title: "01 Vectors and Linear Algebra Fundamentals"
format:
  html:
    css: homework-styles.css
---

<script src="homework-scripts.js"></script>

![image.png](../background_photos/math_01_shinararutun.jpg)
[’¨’∏÷Ç’Ω’°’∂’Ø’°÷Ä’´ ’∞’≤’∏÷Ç’¥’®](https://unsplash.com/photos/black-and-yellow-crane-near-building-during-daytime-JcRhkLqvICA), ’Ä’•’≤’´’∂’°’Ø’ù [Suren Sargsyan](https://unsplash.com/@s_u_ren)
      

# üìö ’Ü’µ’∏÷Ç’©’®

- [üìö ‘±’¥’¢’∏’≤’ª’°’Ø’°’∂ ’∂’µ’∏÷Ç’©’®](01_vectors_linear_algebra.qmd)
- [üì∫ ’è’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’® (ToDo)]()
- [üéûÔ∏è ’ç’¨’°’µ’§’•÷Ä - Vectors ToDo](Lectures/L01_Vectors.pdf)
- [üéûÔ∏è ’ç’¨’°’µ’§’•÷Ä - Geometry ToDo ](Lectures/L02_Geometry_of_Vectors__Matrices.pdf)
- [üõ†Ô∏èüì∫ ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ ’ø’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’® ToDo](https://youtu.be/vectors_practical)
- [üõ†Ô∏èüóÇÔ∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ PDF-’® ToDo](Homeworks/hw_01_vectors.pdf)
  

üìö ’è’°’∂’® ’Ø’°÷Ä’§’∏÷Ç’¥ ’•’∂÷Ñ’ù

1
- [Poole](bibliography/Poole - Linear Algebra-1-400.pdf), 2-12 ’ß’ª’•÷Ä’® (’æ’•’Ø’ø’∏÷Ä’∂’•÷Ä)
- [Johnston](bibliography/Nathaniel Johnston - Introduction to Linear and Matrix Algebra-Springer (2021).pdf), 10-14 ’ß’ª’•÷Ä’® (’∂’∏÷Ä’¥)

÷á ’§’´’ø’∏÷Ç’¥ 3b1b-’´ 1-’´’∂ [’ø’•’Ω’°’§’°’Ω’®](https://youtu.be/fNk_zzaMoSs) ’£’Æ’°’µ’´’∂ ’∞’°’∂÷Ä’°’∞’°’∑’æ’´÷Å(’∂’∏÷Ç’µ’∂’® [’∞’°’µ’•÷Ä’•’∂](https://youtu.be/7-r7Z2iH0Ps))


2 
- [Johnston](bibliography/Nathaniel Johnston - Introduction to Linear and Matrix Algebra-Springer (2021).pdf), 15-19 ’ß’ª’•÷Ä’® (‘ø’∏’∑’´-’á’æ’°÷Ä÷Å, ’°’∂’Ø’µ’∏÷Ç’∂)
- [Poole](bibliography/Poole - Linear Algebra-1-400.pdf), 26-28 ’ß’ª’•÷Ä’® (’ä’µ’∏÷Ç’©’°’£’∏÷Ä’°’Ω’´ ’©’•’∏÷Ä’•’¥, ’∫÷Ä’∏’µ’•’Ø÷Å’´’°)
- [Johnston](bibliography/Nathaniel Johnston - Introduction to Linear and Matrix Algebra-Springer (2021).pdf), 121-124 ’ß’ª’•÷Ä’® (’£’Æ’°’µ’´’∂ ’ø’°÷Ä’°’Æ’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä)
÷á ÷Å’°’∂’Ø’∏÷Ç’©’µ’°’∂ ’§’•’∫÷Ñ’∏÷Ç’¥ ’§’´’ø’∏÷Ç’¥ StatQuest-’´ ’ø’•’Ω’°’§’°’Ω’® (https://youtu.be/e9U0QAFbfLI) ’Ø’∏’Ω’´’∂’∏÷Ç’Ω’°’µ’´’∂ ’∂’¥’°’∂’∏÷Ç’©’µ’°’∂ ’¥’°’Ω’´’∂

‘≤’∏’¨’∏÷Ä ’£÷Ä÷Ñ’•÷Ä’® [’°’µ’Ω’ø’•’≤](https://drive.google.com/drive/folders/14ib_UZSDQ4UPW6XgncURhhbtWLs3-qV3?usp=drive_link) ’•’∂÷â




# üè° ’è’∂’°’µ’´’∂

::: {.callout-note collapse="false"}
1. ‚ùó‚ùó‚ùó DON'T CHECK THE SOLUTIONS BEFORE TRYING TO DO THE HOMEWORK BY YOURSELF‚ùó‚ùó‚ùó
2. Please don't hesitate to ask questions, never forget about the üçäkaralyoküçä principle!
3. The harder the problem is, the more üßÄcheesesüßÄ it has.
4. Problems with üéÅ are just extra bonuses. It would be good to try to solve them, but also it's not the highest priority task.
5. If the problem involve many boring calculations, feel free to skip them - important part is understanding the concepts.
6. Submit your solutions [here](https://forms.gle/CFEvNqFiTSsDLiFc6) (even if it's unfinished)
:::


## Vector Operations

### 01 RGB color mixing with vectors {data-difficulty="1"}

::: {.callout-tip collapse="true" appearance="minimal"}
#### Context
In computer graphics and image processing, colors can be represented as RGB vectors where each component (Red, Green, Blue) ranges from 0 to 255. Vector operations on these RGB values correspond to color mixing and transformations.
:::

Consider these RGB color vectors:

- Red: $\vec{r} = (255, 0, 0)$
- Cyan: $\vec{c} = (0, 255, 255)$

1. Calculate what color you get by adding red and cyan: $\vec{r} + \vec{c}$.
2. Find the "average" color between red and cyan: $\frac{1}{2}(\vec{r} + \vec{c})$.
3. Use a [color picker](https://share.google/yadDErXuKGKRwIHnq) to verify your answers from parts (1) and (2). What colors do you actually see?

### 02 Dot product {data-difficulty="1"}
A translation office translated $a = [24, 17, 9, 13]$ documents from English,
French, German and Russian, respectively. For each of those languages, it takes about
$b = [5, 10, 11, 7]$ minutes to translate one page.
How much time did they spend translating in total? How much did each of the translators
spend on average if there are 4 translators in the office? Write an expression for this amount
in terms of the vectors $a$ and $b$.


### 03 Feature vector normalization {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
In machine learning, we often work with data that has very different scales - like comparing a person's age (around 20-80) with their salary (around 20,000-100,000). Without normalization (bringing all the values to a similar scale (e.g. having length of 1)), algorithms might think salary is much more important just because the numbers are bigger. Normalizing vectors to unit length helps ensure all features are treated equally.
:::

A customer is represented by the vector $\vec{v} = (25, 50000, 3)$ where components represent [age, income in $, number of purchases].

1. Calculate the Euclidean norm (magnitude) $||\vec{v}||_2$
2. Find the unit vector $\hat{v} = \frac{\vec{v}}{||\vec{v}||_2}$
3. Verify that $||\hat{v}||_2 = 1$

*Note:* No need to carry out the calculations explicitly.



### 04 Triangle inequality {data-difficulty="2"}
For vectors $\vec{u} = (3, 4)$ and $\vec{v} = (5, -12)$:

1. Calculate $||\vec{u}||$, $||\vec{v}||$, and $||\vec{u} + \vec{v}||$
2. Verify the triangle inequality: $||\vec{u} + \vec{v}|| \leq ||\vec{u}|| + ||\vec{v}||$
3. When does equality hold in the triangle inequality?


### 05 Model selection with regularization {data-difficulty="2"}
::: {.callout-important collapse="true" appearance="minimal"}
#### Context
In machine learning, we constantly face a tradeoff: should we use a complex model that fits our training data very well, or a simpler model that captures the general pattern? This is where **regularization** comes in.

Imagine you're Netflix trying to predict movie ratings. You could create an extremely complex formula with thousands of parameters that perfectly predicts every rating in your training data. But when a new user comes along, your model might fail spectacularly - it memorized the training data instead of learning the underlying patterns. This is called **overfitting**. ([Kargin example](https://www.youtube.com/watch?v=723rlQAhXqc))

**Regularization** prevents overfitting by adding a penalty for model complexity to our optimization goal:

$$\text{Total Error} = \text{Prediction Error} + \lambda \cdot \text{Complexity Penalty}$$

where $\lambda$ controls how much we penalize complexity (having large parameter values).

The two most common regularization methods use different norms to measure complexity:

- **L1 Regularization (Lasso)**: Uses the sum of absolute values
   $$\text{L1 penalty} = \lambda \sum_{i=1}^{n} |w_i|$$
   
- **L2 Regularization (Ridge)**: Uses the sum of squares
   $$\text{L2 penalty} = \lambda \sum_{i=1}^{n} w_i^2$$

**Real-world example:** Suppose you're predicting house prices using features like size, location, age, etc. Without regularization, your model might learn that "houses with exactly 2,347 sq ft, built in 1987, with 3.5 bathrooms, facing north-northeast, with blue doors" sell for $523,456. With regularization, it learns more general rules like "larger houses in good neighborhoods cost more."
:::

’é’Ω’ø’°’∞ ’π’•’¥ ’∏÷Ä ’¨’°’æ ’•’¥ ’±÷á’°’Ø’•÷Ä’∫’•’¨ (’∞’°’ø’Ø’°’∫’•’Ω) ’ß’Ω ’≠’∂’§’´÷Ä’® , ’•’©’• ’∞’°÷Ä÷Å’•÷Ä ’¨’´’∂’•’∂’ù ’≠’°’¢’°÷Ä ’°÷Ä’•÷Ñ÷â


You're comparing two models that predict house prices:

- Model A: Complex formula with weights (coefficients) $\vec{w_A} = (10, -8, 4)$ (this can correspond to equation ($10x^2 - 8x + 4$ (quadratic))) and prediction error = 100
- Model B: Simpler formula with weights $\vec{w_B} = (0.1, -3, 1)$ $(0.1x^2 - 3x + 1)$ (almost just a linear function) and prediction error = 120

Model B makes slightly worse predictions, but which model is better when considering both error and simplicity?

a) **L1 Regularization (Œª = 0.5):** Calculate the total error for each model
   - Model A: $\text{Error} + \lambda \cdot ||\vec{w_A}||_1 = ?$
   - Model B: $\text{Error} + \lambda \cdot ||\vec{w_B}||_1 = ?$

b) **L2 Regularization (Œª = 0.5):** Calculate the total error for each model
   - Model A: $\text{Error} + \lambda \cdot ||\vec{w_A}||_2^2 = ?$
   - Model B: $\text{Error} + \lambda \cdot ||\vec{w_B}||_2^2 = ?$

c) **Model Selection:** Which model would you choose under each regularization method? How does the choice of $\lambda$ affect your decision?
d) **Practical Insight:** In production systems, why might we prefer a model with slightly worse accuracy but much simpler weights?



### 06 k-Nearest Neighbors Classification {data-difficulty="3"}


‘ø÷Å’æ’°’Æ ’Ø’£’ø’∂’•÷Ñ [csv ÷Ü’°’µ’¨](assets/knn.csv) ’•÷Ä’•÷Ñ ’Ω’µ’∏÷Ç’∂’∏’æ’ù feature_1, feature_2, label÷â 
‘ø’°÷Ä’∏’≤ ’•÷Ñ ’∫’°’ø’Ø’•÷Ä’°÷Å’∂’•’¨  ’∏÷Ä feature_1-’® ’´÷Ä’°’∂’´÷Å ’∂’•÷Ä’Ø’°’µ’°÷Å’∂’∏÷Ç’¥ ’° ’Æ’°’≤’Ø’´ ’¢’°÷Ä’±÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’®, feature_2-’®’ù ’¨’°’µ’∂’∏÷Ç’©’µ’∏÷Ç’∂’® ’∏÷Ç label (’∫’´’ø’°’Ø’®)  ’∂’•÷Ä’Ø’°’µ’°÷Å’∂’∏÷Ç’¥ ’° ’©’• 4 ’Æ’°’≤’Ø’´ ’ø’•’Ω’°’Ø’∂’•÷Ä’´÷Å (0,1,2,3) ’∏÷Ä ’¥’•’Ø’∂ ’°÷â

 ’ä’•’ø÷Ñ ’° ’Ω’ø’•’≤’Æ’•’¨ ’¥’∏’§’•’¨ (’°’¨’£’∏÷Ä’´’©’¥) ’∏÷Ä’® ’Ω’ø’°’∂’°’¨’∏’æ feature_1, feature_2 ’°÷Ä’™’•÷Ñ’∂’•÷Ä’® ’Ø’£’∏÷Ç’∑’°’Ø’´ ’Æ’°’≤’Ø’´ ’ø’•’Ω’°’Ø’®÷â 

’Ä’•’ø÷á’µ’°’¨ ’Ø’•÷Ä’∫’∏’æ’ù ’∂’∏÷Ä ’Æ’°’≤’Ø’´ ’∞’°’¥’°÷Ä ’£’ø’∂’•’¨ K ’∞’°’ø ’°’¥’•’∂’°’¥’∏’ø’´’Ø ’Æ’°’≤’´’Ø’∂’•÷Ä’® ’¥’•÷Ä ’∏÷Ç’∂’•÷Å’°’Æ ’ø’æ’µ’°’¨’∂’•÷Ä’´÷Å ’∏÷Ç ’∂’°’µ’•’¨ ’©’• ’ß’§ k ’∞’°÷Ä÷á’°’∂’∂’•÷Ä’´÷Å ’∏÷Ä ’ø’•’Ω’°’Ø’´ ’Æ’°’≤’´’Ø’∂ ’° ’£’•÷Ä’°’Ø’∑’º’∏÷Ç’¥’ù ’∏÷Ç ’§’° ÷Ö’£’ø’°’£’∏÷Ä’Æ’•’¨ ’∏÷Ä’∫’•’Ω ’£’∏÷Ç’∑’°’Ø’∏÷Ç’©’µ’∏÷Ç’∂, 

’Ä’•’º’°’æ’∏÷Ä’∏÷Ç’©’µ’∏÷Ç’∂ ’∏÷Ä’∫’•’Ω ÷Ö’£’ø’°’£’∏÷Ä’Æ’•÷Ñ ’¥’´ ’§’•’∫÷Ñ’∏÷Ç’¥ L1-’® (Manhattan), ’¥’´ ’§’•’∫÷Ñ’∏÷Ç’¥ L2-’® (Euclidean): K-’´ ’∞’°’¥’°÷Ä ’ß’¨ ’ø’°÷Ä’¢’•÷Ä ’°÷Ä’™’•÷Ñ’∂’•÷Ä ’¢’¶’¢’°÷Å’•÷Ñ’ù 2,3, 5, 10 .

‘π’•’©÷á ’∞’°’æ’•’¨’µ’°’¨ ’∂’∑’∏÷Ç’¥’∂’•÷Ä
1. ‘±’¨’£’∏÷Ä’´’¥’©’´ ’°’∂’∏÷Ç’∂’∂ ’° K Nearest Neighbors ’∏÷Ç ’¶’∏÷Ç’ø "’°’Ω’° ’´’∂’± ’∏’æ÷Ñ’•÷Ä ’•’∂ ÷Ñ’∏ ’®’∂’Ø’•÷Ä’∂’•÷Ä’®, ’•’Ω ’Ø’°’Ω’•’¥ ’∏’æ ’•’Ω ’§’∏÷Ç" ’Ω’Ø’¶’¢’∏÷Ç’¥÷Ñ’∏’æ ’° ’°’∑’≠’°’ø’∏÷Ç’¥, ’∫÷Ä’°’Ø’ø’´’Ø’°’µ’∏÷Ç’¥ ’∞’°’¥’°÷Ä’µ’° ’•÷Ä’¢’•÷Ñ ’π’´ ÷Ö’£’ø’°’£’∏÷Ä’Æ’æ’∏÷Ç’¥ ’¢’°’µ÷Å ’ø’∂’°’µ’´’∂’´ ’∞’°’¥’°÷Ä ’Ø’°÷Ä’° ’∞’°’æ’•’Ω ’¨’´’∂’´ 
2. ’ä’°’ø’≥’°’º’∂’•÷Ä’´÷Å ’¥’•’Ø’® ’©’• ’´’∂’π’´ ’π’´ ÷Ö’£’ø’°’£’∏÷Ä’Æ’æ’∏÷Ç’¥ ’§’° "’â’°÷É’∏’≤’°’Ø’°’∂’∏÷Ç’©’µ’°’∂ ’°’∂’•’Æ÷Ñ’∂" ’° (Curse of dimenionality), ’∑’°’ø ’∞’°’æ’•’Ω ’ß÷Ü’•’Ø’ø ’° ’®’Ω’ø ’∏÷Ä’´ ’•÷Ä’¢ ’£’∏÷Ä’Æ ’•’∂÷Ñ ’∏÷Ç’∂’•’∂’∏÷Ç’¥ ’¢’°÷Ä’±÷Ä ’π’°÷É’°’∂’´ ’ø’°÷Ä’°’Æ’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’´ ’∞’•’ø, ’ø’æ’µ’°’¨’∂’•÷Ä’® ’∞’´’¥’∂’°’Ø’°’∂’∏÷Ç’¥ ’´÷Ä’°÷Ä’´÷Å ’∞’°’¥’°÷Ä’µ’° ’∞’°’æ’°’Ω’°÷Ä’°’∞’•’º ’•’∂ ’§’°’º’∂’∏÷Ç’¥ ’∏÷Ç ’°’∂’Ø’µ’∏÷Ç’∂’∂’•÷Ä’∏÷Ç’¥ ’•’∂ ’Ø’∏÷Ç’ø’°’Ø’æ’∏÷Ç’¥ (’°’µ’¨ ’Ø’•÷Ä’∫ ’°’Ω’°’Æ’ù ’•’©’• ’¢’°÷Ä’±÷Ä’°’π’°÷É ’∂’°÷Ä’´’∂’ª’® ’Ø’¨’∫’•’∂÷Ñ’ù ’ø’°’Ø’® ’¢’°’∂ ’π’´ ’¥’∂’°)÷â ‘±’≤’¢’µ’∏÷Ç÷Ä (https://slds-lmu.github.io/i2ml/chapters/14_cod/)

## Dot Products and Angles Between Vectors

### 07 Finding perpendicular vectors {data-difficulty="1"}


Given the vector $\vec{v} = (2, 3)$:

1. Find a non-zero vector $\vec{w} = (x, y)$ such that $\vec{v}$ and $\vec{w}$ are perpendicular.
2. Verify that your chosen vector $\vec{w}$ satisfies $\vec{v} \cdot \vec{w} = 0$.
3. Find a unit vector in the direction of $\vec{w}$ by computing $\frac{\vec{w}}{||\vec{w}||}$.
4. Explain why there are infinitely many vectors perpendicular to $\vec{v}$ and describe the general form of all such vectors.


### 08: Word embeddings similarity {data-difficulty="3"}
::: {.callout-important collapse="true" appearance="minimal"}
#### Context
Computers understand numbers, not words. To make sense of text, we convert words into vectors in a high-dimensional space, called **word embeddings**. In this space, words with similar meanings are located close to each other. For example, "king" and "queen" are closer than "king" and "car".
:::

```{python}
#| eval: false

# install gensim if you haven't already (you can also do it without `uv`, but why would you? uv is fantastic)
# !uv pip install gensim

import gensim.downloader as api

# this might take a few minutes first time
model_name = "glove-twitter-25" # smaller model 

# If you're willing to wait a bit longer for a better model, uncomment line below:
# model_name = "word2vec-google-news-300"  # 1.5 gb model

model = api.load(model_name)  # 300-dimensional vectors

print("Model loaded successfully!")

# to get vector for a word we just get it like from a dictionary
word = "cheese"
if word in model:
    vector = model[word]
    print(f"Vector for '{word}' has shape: {vector.shape}")
    print(f"First 10 dimensions: {vector[:10]}")
else:
    print(f"'{word}' not found in vocabulary")
```

Familiarize yourself with the code above. It loads a pre-trained word embedding model and retrieves the vector for the word "cheese".

Your task is to calculate how similar the word "cheese" (or any other word you choose) is to a list of other words (given below) using cosine similarity. 

```{python}
#| eval: false

potential_words = ["elephant", "cheese", "butter", "bread", "watermelon", "potato", "iron", "clock", "computer", "chicken", "fries"]
```

Return a sorted dictionary where keys are words and values are their cosine similarity to "cheese".

You can **AFTERWARDS** also use the built-in method to find most similar words:
```{python}
#| eval: false
model.most_similar("cheese", topn=10)
```

**Bonus:** Play around with vector arithmetic to explore relationships - you can add and subtract word vectors to see how meanings combine. For example, try "king" - "man" + "woman" and see what word is closest to the resulting vector! Or try "Paris" - "France" + "Armenia" to see if you get "Yerevan".

::: {.callout-note collapse="true" appearance="minimal"}
 Check out this fantastic [3Blue1Brown video on word vectors (embeddings)](https://youtu.be/wjZofJX0v4M?t=751) for more insights! Also, [this](https://huggingface.co/blog/embeddinggemma#demo) is a cool tool to play with
:::


## Vector Spaces and Subspaces
### 09: Identifying vector spaces and non-vector spaces {data-difficulty="3"}

For each of the following sets, determine whether it is a vector space or not. If it is a vector space, prove it by verifying all the required axioms. If it is not a vector space, identify which axiom(s) fail and provide counterexamples. 

a) $A = \left\{\begin{pmatrix} a \\ 0 \end{pmatrix} \mid a \in \mathbb{R}\right\}$ (vectors with second component zero)

b) $B = \left\{\begin{pmatrix} a \\ -a \end{pmatrix} \mid a \in \mathbb{R}\right\}$ (vectors where second component is negative of first)

c) $C = \mathbb{N}$ (the set of natural numbers)

d) $D = \left\{\begin{pmatrix} a \\ 1 \end{pmatrix} \mid a \in \mathbb{R}\right\}$ (vectors with second component always 1)

Note: By default, if we don't mention the operation, we mean the standard vector addition and scalar multiplication (e.g. our good old + and * we learning at school).

**Hint:** For the non-vector spaces, show that there are some "bad" elements such that if we add them or multiply with some number (not necessarily positive), the result would not belong to the set.

### 10 vector space or not? {data-difficulty="2"}

Check if the following set is a vector space:

a) $A=\mathbb{Z}$, with the usual operations $+$ and $\cdot$.

b) $B=\left\{ \begin{bmatrix} 0 \\ 0 \\ a \end{bmatrix} \,\middle|\, a\in\mathbb{R} \right\}$ with the usual operations $+$ and $\cdot$.

c) $C=\mathbb{R}^2=\left\{ \begin{bmatrix} a \\ b \end{bmatrix} \,\middle|\, a,b\in\mathbb{R} \right\}$, with the usual operation $\cdot$ and the addition defined as
$$
\begin{bmatrix} x_1 \\ x_2 \end{bmatrix}
+
\begin{bmatrix} y_1 \\ y_2 \end{bmatrix}
=
\begin{bmatrix} x_1 + y_1 \\ x_2 + y_2 + 1 \end{bmatrix}.
$$

d) The set of all polynomials of degree $\le 2$, with the usual operations $+$ and $\cdot$. Bonus question - is this maybe "equivalent" to some other vector space we already know?



### 11 Vector subspaces {data-difficulty="2"}
![](assets/subspace_exercise.png)



### 12 Deriving the cosine angle formula {.bonus problem data-difficulty="3"}

Derive the formula for the cosine of the angle between two vectors: $\cos(\theta) = \frac{\vec{a} \cdot \vec{b}}{||\vec{a}|| \cdot ||\vec{b}||}$

::: {.callout-warning collapse="true" appearance="minimal"}
#### Hint
Start with the law of cosines for a triangle: $c^2 = a^2 + b^2 - 2ab\cos(\theta)$. Consider a triangle formed by vectors $\vec{a}$, $\vec{b}$, and $\vec{a} - \vec{b}$. The side lengths are $||\vec{a}||$, $||\vec{b}||$, and $||\vec{a} - \vec{b}||$. Express $||\vec{a} - \vec{b}||^2$ using the dot product and substitute into the law of cosines.
:::

1. Write down the law of cosines for the triangle with sides $||\vec{a}||$, $||\vec{b}||$, and $||\vec{a} - \vec{b}||$
2. Express $||\vec{a} - \vec{b}||^2$ in terms of dot products by expanding $(\vec{a} - \vec{b}) \cdot (\vec{a} - \vec{b})$
3. Substitute your result from part (2) into the law of cosines and solve for $\cos(\theta)$
4. Verify your derived formula using vectors $\vec{u} = (3, 4)$ and $\vec{v} = (1, 0)$


### 13 High-dimensional vector geometry {.bonus-problem data-difficulty="1"}
In high-dimensional spaces (common in ML), our intuition about geometry can be misleading.

Consider the unit sphere in $\mathbb{R}^n$ (all vectors with norm 1):

a) In 2D, what fraction of a unit square $[-1,1] \times [-1,1]$ is occupied by the unit circle?
b) Estimate this fraction for a unit cube in 3D (you can google the formula)
c) Try to guess and then google  What happens to this fraction as the dimension $n$ increases? This is known as the "curse of dimensionality."

[Video](https://www.youtube.com/watch?v=9Tf-_mJhOkU)

# üõ†Ô∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂ ToDo
- [üõ†Ô∏èüì∫ ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ ’ø’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’®]()
- [üõ†Ô∏èüóÇÔ∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ PDF-’®]()
:::

# üé≤ 38 (01) TODO
- ‚ñ∂Ô∏è[ToDo]()
- üîó[Random link]()
- üá¶üá≤üé∂[ToDo]()
- üåêüé∂[ToDo]()
- ü§å[‘ø’°÷Ä’£’´’∂]()
