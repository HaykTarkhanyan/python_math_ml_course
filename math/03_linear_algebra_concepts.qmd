---
title: "03 Linear Independence, Basis, and Systems"
format:
  html:
    css: homework-styles.css
---

<script src="homework-scripts.js"></script>

<!-- 
[image.png](../background_photos/)
[Õ¬Õ¸Ö‚Õ½5. Verify the rank-nullity theorem: $\dim(\text{domain}) = \text{rank}(T) + \dim(\text{kernel}(T))$.



# ğŸ› ï¸ Ô³Õ¸Ö€Õ®Õ¶Õ¡Õ¯Õ¡Õ¶Õ¶Õ¯Õ¡Ö€Õ« Õ°Õ²Õ¸Ö‚Õ´Õ¨](https://unsplash.com/photos/a-tall-building-with-lots-of-windows-and-balconies-AowELlZmpZM), Õ€Õ¥Õ²Õ«Õ¶Õ¡Õ¯Õ [Gor Davtyan](https://unsplash.com/@gor918) -->

![image.png](../background_photos/math_03_gazananoc.jpg)
[Õ¬Õ¸Ö‚Õ½Õ¡Õ¶Õ¯Õ¡Ö€Õ« Õ°Õ²Õ¸Ö‚Õ´Õ¨](https://unsplash.com/photos/a-close-up-of-a-monkey-in-a-cage-mQF2vmyV0Zc), Ô³Õ¡Õ¦Õ¡Õ¶Õ¡Õ¶Õ¸Ö, Õ€Õ¥Õ²Õ«Õ¶Õ¡Õ¯Õ [Elmira Gokoryan](https://unsplash.com/@elmira)

      

# ğŸ“š Õ†ÕµÕ¸Ö‚Õ©Õ¨ ToDo

- [ğŸ“š Ô±Õ´Õ¢Õ¸Õ²Õ»Õ¡Õ¯Õ¡Õ¶ Õ¶ÕµÕ¸Ö‚Õ©Õ¨]()
- [ğŸ“º ÕÕ¥Õ½Õ¡Õ£Ö€Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶Õ¨]()
- [ğŸï¸ ÕÕ¬Õ¡ÕµÕ¤Õ¥Ö€ - ToDo](Lectures/L01_Vectors.pdf)
- [ğŸï¸ ÕÕ¬Õ¡ÕµÕ¤Õ¥Ö€ - Geometry](Lectures/L02_Geometry_of_Vectors__Matrices.pdf)
- [ğŸ› ï¸ğŸ“º Ô³Õ¸Ö€Õ®Õ¶Õ¡Õ¯Õ¡Õ¶Õ« Õ¿Õ¥Õ½Õ¡Õ£Ö€Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶Õ¨](https://youtu.be/vectors_practical)
- [ğŸ› ï¸ğŸ—‚ï¸ Ô³Õ¸Ö€Õ®Õ¶Õ¡Õ¯Õ¡Õ¶Õ« PDF-Õ¨](Homeworks/hw_01_vectors.pdf)
  


# ğŸ¡ ÕÕ¶Õ¡ÕµÕ«Õ¶

::: {.callout-note collapse="false"}
1. â—â—â— DON'T CHECK THE SOLUTIONS BEFORE TRYING TO DO THE HOMEWORK BY YOURSELFâ—â—â—
2. Please don't hesitate to ask questions, never forget about the ğŸŠkaralyokğŸŠ principle!
3. The harder the problem is, the more ğŸ§€cheesesğŸ§€ it has.
4. Problems with ğŸ are just extra bonuses. It would be good to try to solve them, but also it's not the highest priority task.
5. If the problem involve many boring calculations, feel free to skip them - important part is understanding the concepts.
6. Submit your solutions [here](https://forms.gle/CFEvNqFiTSsDLiFc6) (even if it's unfinished)
:::


## Systems of Linear Equations

### 01: GPS positioning - from 2D to 3D {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
GPS systems solve systems of equations to determine location. Understanding this process helps grasp how linear systems work in practice and why we need the right number of equations.
:::

**Part A: 2D Positioning (Easier warm-up)**

Imagine you're lost in a 2D world and receive distance signals from cell towers:
- Tower A at (0, 0): You are 5 units away
- Tower B at (6, 0): You are 3 units away

1. Write the system of equations for your position (x, y).
2. Solve it step by step using substitution or elimination.
3. Plot the circles and find their intersection point(s).

**Part B: 3D GPS Challenge**

Now for real GPS with 4 satellites in 3D space:
- Why do you need exactly 4 satellites for 3D positioning when you only need 2 towers for 2D?
- What happens if you only have 3 satellites? 


### 02: Linear regression with normal equations {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
The normal equation provides a direct way to solve linear regression problems, connecting linear algebra to machine learning prediction tasks.
:::

You have the following data points for house size (x) vs price (y):
- (50, 100), (100, 180), (150, 280)

1. Set up the design matrix X (including the intercept column) and target vector y.
2. Solve the normal equation $\vec{\theta} = (X^T X)^{-1} X^T \vec{y}$ using Gaussian elimination.
3. Find the line equation $y = \theta_0 + \theta_1 x$.
4. Plot the data points and your fitted line.
5. Predict the price for a 120 square meter house.

### 03: The cheese shop multicollinearity problem {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Real datasets often contain redundant features that are linear combinations of each other, causing problems in machine learning models.
:::

A cheese shop tracks the following features for each cheese:
- Price in AMD: $p_1$
- Price in USD: $p_2$ 
- Weight in kilograms: $w_1$
- Weight in pounds: $w_2$

Given the conversion rates: 1 USD = 400 AMD and 1 kg = 2.2 pounds.

1. **Linear dependence detection**: Which features are linearly dependent? Write the exact relationships.

2. **Matrix rank problem**: If you create a data matrix with these 4 features for 100 cheeses, what would be the maximum possible rank? Why?

3. **Feature selection**: Which 2 features should you keep to avoid multicollinearity? Explain your choice.

4. **System solvability**: If you try to fit a model using all 4 features, what problems might arise?


### 04: System consistency analysis {data-difficulty="2"}


For which values of $a$ does the following system have 0, 1, or infinitely many solutions?

$$\begin{cases}
x + 2y + z = 1 \\
2x + 4y + az = 2 \\
-x - y + (a-1)z = 0
\end{cases}$$

### 05: Linear independence in polynomial space {data-difficulty="2"}



Determine if the set $\{1 + t, 1 + 2t, 1 + t + t^2\} \subset P_2$ is linearly independent. If dependent, express one polynomial as a linear combination of the others.

### 06: Matrix subspaces and trace {data-difficulty="3"}


Let $V = \{M \in M_{2 \times 2} : \text{tr}(M) = 0\}$ be the set of $2 \times 2$ matrices with zero trace.

1. Prove that $V$ is a subspace of $M_{2 \times 2}$.
2. Find a basis and determine the dimension of $V$.

### 07: Change of basis and coordinates {data-difficulty="3"}


Let $B = \{(3,1), (2,1)\}$ and $C = \{(1,1), (1,0)\}$ be bases of $\mathbb{R}^2$.

1. Find the change-of-basis matrix $P_{C \leftarrow B}$.
2. If $[v]_B = (1, 2)^T$, compute $[v]_C$.

::: {.callout-tip collapse="true"}
#### Solution

**Part 1: Find the change-of-basis matrix $P_{C \leftarrow B}$**

The columns of the change-of-basis matrix $P_{C \leftarrow B}$ are the coordinate vectors of the basis vectors in $B$ relative to the basis $C$. Let $\vec{b_1} = (3,1)$, $\vec{b_2} = (2,1)$, and $\vec{c_1} = (1,1)$, $\vec{c_2} = (1,0)$.

We need to express $\vec{b_1}$ and $\vec{b_2}$ as linear combinations of $\vec{c_1}$ and $\vec{c_2}$.

For $\vec{b_1} = (3,1)$:
We need to find scalars $x_1, y_1$ such that $(3,1) = x_1(1,1) + y_1(1,0)$. This gives the system:
$$ \begin{cases} x_1 + y_1 = 3 \\ x_1 = 1 \end{cases} $$
Solving this system yields $x_1 = 1$ and $y_1 = 2$. So, $[\vec{b_1}]_C = \begin{pmatrix} 1 \\ 2 \end{pmatrix}$.

For $\vec{b_2} = (2,1)$:
We need to find scalars $x_2, y_2$ such that $(2,1) = x_2(1,1) + y_2(1,0)$. This gives the system:
$$ \begin{cases} x_2 + y_2 = 2 \\ x_2 = 1 \end{cases} $$
Solving this system yields $x_2 = 1$ and $y_2 = 1$. So, $[\vec{b_2}]_C = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$.

Combining these column vectors gives the change-of-basis matrix:
$$ P_{C \leftarrow B} = \begin{pmatrix} 1 & 1 \\ 2 & 1 \end{pmatrix} $$

**Part 2: Compute $[v]_C$**

To find the coordinates of $v$ with respect to basis $C$, we use the formula:
$$ [v]_C = P_{C \leftarrow B} [v]_B $$

Given $[v]_B = \begin{pmatrix} 1 \\ 2 \end{pmatrix}$, we can compute:
$$ [v]_C = \begin{pmatrix} 1 & 1 \\ 2 & 1 \end{pmatrix} \begin{pmatrix} 1 \\ 2 \end{pmatrix} = \begin{pmatrix} 1 \cdot 1 + 1 \cdot 2 \\ 2 \cdot 1 + 1 \cdot 2 \end{pmatrix} = \begin{pmatrix} 1 + 2 \\ 2 + 2 \end{pmatrix} = \begin{pmatrix} 3 \\ 4 \end{pmatrix} $$

So, the coordinates of the vector $v$ in the basis $C$ are $(3, 4)^T$.
:::

### 08: Basis verification in different vector spaces {data-difficulty="2"}

In each case, determine whether $S$ is a basis for $V$:

**a)** $V = P_3$, $S = \{0, x, x^2, x^3\}$

**b)** $V = M_{2 \times 2}$, $S = \left\{I_2, \begin{pmatrix} 1 & 3 \\ 2 & 4 \end{pmatrix}, \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}\right\}$

**c)** $V = \mathbb{R}^3$, $S = \{e_1, e_2, e_1 + 3e_2\}$

**d)** $V = P_1$, $S = \{1 - 2x, 2 - x\}$

### 09: Linear transformation from coordinates {data-difficulty="3"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Linear transformations can be uniquely determined by their values on a basis. This principle is fundamental in neural networks where transformations are defined by weight matrices.
:::

Suppose $T$ is a linear transformation from $\mathbb{R}^2$ to $P_2$ such that:
$$T\begin{pmatrix} -1 \\ 1 \end{pmatrix} = x + 3, \quad T\begin{pmatrix} 2 \\ 3 \end{pmatrix} = 2x^2 - x$$

Find $T\begin{pmatrix} a \\ b \end{pmatrix}$ for arbitrary $a, b \in \mathbb{R}$.


# ğŸ› ï¸ Ô³Õ¸Ö€Õ®Õ¶Õ¡Õ¯Õ¡Õ¶
- [ğŸ› ï¸ğŸ“º Ô³Õ¸Ö€Õ®Õ¶Õ¡Õ¯Õ¡Õ¶Õ« Õ¿Õ¥Õ½Õ¡Õ£Ö€Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶Õ¨]()
- [ğŸ› ï¸ğŸ—‚ï¸ Ô³Õ¸Ö€Õ®Õ¶Õ¡Õ¯Õ¡Õ¶Õ« PDF-Õ¨]()

# ğŸ² 40 (03)
- â–¶ï¸[]()
- ğŸ”—[Random link](https://www.youtube.com/watch?v=qTkpyUHBGDA)
- ğŸ‡¦ğŸ‡²ğŸ¶[]()
- ğŸŒğŸ¶[Chet Baker (Almost blue)](https://www.youtube.com/watch?v=z4PKzz81m5c)
- ğŸ¤Œ[Ô¿Õ¡Ö€Õ£Õ«Õ¶ ToDo]()