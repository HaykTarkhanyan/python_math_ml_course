---
title: "02 Linear Algebra - Matrices"
format:
  html:
    css: homework-styles.css
---

<script src="homework-scripts.js"></script>

![image.png](../background_photos/math_02_old_yerevan.jpg)
Õ€Õ¡Õ¶Ö€Õ¡ÕºÕ¥Õ¿Õ¸Ö‚Õ©ÕµÕ¡Õ¶ Õ€Ö€Õ¡ÕºÕ¡Ö€Õ¡Õ¯, [Õ¬Õ¸Ö‚Õ½Õ¡Õ¶Õ¯Õ¡Ö€Õ« Õ°Õ²Õ¸Ö‚Õ´Õ¨](https://www.facebook.com/photo/?fbid=2458079004566211&set=g.576489656651693), Õ†Õ¯Õ¡Ö€Õ¨ facebook-Õ¸Ö‚Õ´ Õ°Ö€Õ¡ÕºÕ¡Ö€Õ¡Õ¯Õ¸Õ²Õ [Marine Tovmasyan](https://www.facebook.com/marine.tovmasyan.921?__cft__[0]=AZUiFtxhpNRX8mqfh4IPPyKdAeWHXpH_HYctA5scvjH2Q9TF8CbeXA-UH2n0-yIfhqK_JXZwUEyuv6H-g7QlmpobqU9-frIHuQvw_KBsJXGBF_AE0F3_r5pbpa5U6xnb8_Mt13vXCY8AmWgoUD-PP_aE&__tn__=-UC%2CP-R)
      

# ğŸ“š Õ†ÕµÕ¸Ö‚Õ©Õ¨

- [ğŸ“š Ô±Õ´Õ¢Õ¸Õ²Õ»Õ¡Õ¯Õ¡Õ¶ Õ¶ÕµÕ¸Ö‚Õ©Õ¨](02_linear_algebra_matrices.qmd)
- [ğŸ“º Õ„Õ¡Õ¿Ö€Õ«ÖÕ« Õ¥Ö€Õ¯Ö€Õ¡Õ¹Õ¡ÖƒÕ¡Õ¯Õ¡Õ¶ Õ«Õ´Õ¡Õ½Õ¿Õ¨, Õ£Õ®Õ¡ÕµÕ«Õ¶ Õ±Ö‡Õ¡ÖƒÕ¸Õ­Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶Õ¶Õ¥Ö€](https://youtu.be/AjPnQIe-BSo), [ğŸï¸ ÕÕ¬Õ¡ÕµÕ¤Õ¥Ö€](Lectures/L02_Angles__Vector_Spaces__Matrices.pdf)
- [ğŸ“º Lecture 2, ToDo](), [ğŸï¸ ÕÕ¬Õ¡ÕµÕ¤Õ¥Ö€]()
- [ğŸ› ï¸ğŸ“º Ô³Õ¸Ö€Õ®Õ¶Õ¡Õ¯Õ¡Õ¶Õ« Õ¿Õ¥Õ½Õ¡Õ£Ö€Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶Õ¨ ToDo]()
- [ğŸ› ï¸ğŸ—‚ï¸ Ô³Õ¸Ö€Õ®Õ¶Õ¡Õ¯Õ¡Õ¶Õ« PDF-Õ¨ ToDo](Homeworks/)
  
ğŸ“š ÕÕ¡Õ¶Õ¨ Õ¯Õ¡Ö€Õ¤Õ¸Ö‚Õ´ Õ¥Õ¶Ö„Õ
**Õ„Õ¡Õ¿Ö€Õ«ÖÕ« Õ¥Ö€Õ¯Ö€Õ¡Õ¹Õ¡ÖƒÕ¡Õ¯Õ¡Õ¶ Õ«Õ´Õ¡Õ½Õ¿Õ¨, Õ£Õ®Õ¡ÕµÕ«Õ¶ Õ±Ö‡Õ¡ÖƒÕ¸Õ­Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶Õ¶Õ¥Ö€**

- [Johnston](bibliography/Poole - Linear Algebra-1-400.pdf), 20-25 (Õ´Õ¡Õ¿Ö€Õ«ÖÕ¶Õ¥Ö€), 35-38 (Õ£Õ®Õ¡ÕµÕ«Õ¶ Õ±Ö‡Õ¡ÖƒÕ¸Õ­Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶) Õ§Õ»Õ¥Ö€Õ¨
- [Poole](bibliography/Nathaniel Johnston - Introduction to Linear and Matrix Algebra-Springer (2021).pdf), 219-221 Õ§Õ»Õ¥Ö€Õ¨ (Õ´Õ¡Õ¿Ö€Õ«ÖÕ¶Õ¥Ö€Õ« Õ¡Ö€Õ¿Õ¡Õ¤Ö€ÕµÕ¡Õ¬/Õ°Õ¡Õ´Õ¡Õ¤Ö€Õ¸Ö‚ÕµÕ©)

Ö‡ Õ¤Õ«Õ¿Õ¸Ö‚Õ´ 3b1b-Õ« 3-Ö€Õ¤ Õ¿Õ¥Õ½Õ¡Õ¤Õ¡Õ½Õ¨ Õ£Õ®Õ¡ÕµÕ«Õ¶ Õ°Õ¡Õ¶Ö€Õ¡Õ°Õ¡Õ·Õ¾Õ«ÖÕ
https://youtu.be/kYB8IZa5AuE

## ğŸ§® Õ€Õ¡Ö€Õ´Õ¡Ö€ Õ£Õ¸Ö€Õ®Õ«Ö„Õ¶Õ¥Ö€
Õ´Õ¡Õ¿Ö€Õ«ÖÕ¶Õ¥Ö€Õ¨ Õ¾Õ«Õ¦Õ¸Ö‚Õ¡Õ¬ ÕºÕ¡Õ¿Õ¯Õ¥Ö€Õ¡ÖÕ¶Õ¥Õ¬Õ¸Ö‚ Õ°Õ¡Õ´Õ¡Ö€ Ö…Õ£Õ¿Õ¡Õ¯Õ¡Ö€ Õ£Õ¸Ö€Õ®Õ«Ö„Õ¶Õ¥Ö€Õ

- 2x2 Õ¹Õ¡ÖƒÕ¡Õ¶Õ« [Õ¥Õ¼Õ¡Õ¶Õ¯ÕµÕ¸Ö‚Õ¶Õ¸Õ¾Õ¨](https://melbapplets.ms.unimelb.edu.au/2024/03/25/visualising-linear-transformations-in-r2/)
- 2x2 Õ¹Õ¡ÖƒÕ¡Õ¶Õ« [Õ¤Õ¥Õ²Õ«Õ¶ Õ½Õ¬Õ¡Ö„Õ¸Õ¾Õ¨](https://visualize-it.github.io/linear_transformations/simulation.html)
- 2x2 Õ¹Õ¡ÖƒÕ¡Õ¶Õ« [Õ±Õ¥Õ¼Ö„Õ¸Õ¾ Õ¯Õ¡Ö€Õ£Õ¡Õ¾Õ¸Ö€Õ¾Õ¸Õ²](https://shad.io/MatVis/)
- 3x3 Õ¹Õ¡ÖƒÕ¡Õ¶Õ« [Õ¥Ö€Õ¥Ö„Õ¡Õ¹Õ¡Öƒ](https://melbapplets.ms.unimelb.edu.au/2024/03/25/visualising-linear-transformations-in-r3/)



# ğŸ¡ ÕÕ¶Õ¡ÕµÕ«Õ¶

::: {.callout-note collapse="false"}
1. â—â—â— DON'T CHECK THE SOLUTIONS BEFORE TRYING TO DO THE HOMEWORK BY YOURSELFâ—â—â—
2. Please don't hesitate to ask questions, never forget about the ğŸŠkaralyokğŸŠ principle!
3. The harder the problem is, the more ğŸ§€cheesesğŸ§€ it has.
4. Problems with ğŸ are just extra bonuses. It would be good to try to solve them, but also it's not the highest priority task.
5. If the problem involve many boring calculations, feel free to skip them - important part is understanding the concepts.
6. Submit your solutions [here](https://forms.gle/CFEvNqFiTSsDLiFc6) (even if it's unfinished)
:::


### 01: Matrix transformations {data-difficulty="2"}

What vectors do you get by applying the matrix $A = \begin{pmatrix} 3 & -3 \\ 3 & 3 \end{pmatrix}$ on the vectors:

1. $\vec{a} = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$
2. $\vec{b} = \begin{pmatrix} 0 \\ 1 \end{pmatrix}$
3. $\vec{c} = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$
4. Draw the vectors before and after multiplying with $A$. What can you say visually about the matrix? Can you guess how it will act on the vector $\begin{pmatrix} 2 \\ -2 \end{pmatrix}$?


### 02: Matrix products {data-difficulty="2"}
Compute the following products:

1. $(A - B)(A + B)$, where $A = \begin{pmatrix} 2 & 3 \\ -1 & 2 \end{pmatrix}$, $B = \begin{pmatrix} 1 & 2 \\ 2 & -1 \end{pmatrix}$
2. $A^2 - B^2$, with the same $A$ and $B$ as in part (b).
3. Any comments on the results?

### 03: Shear matrix transformations {data-difficulty="2"}
::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Shear transformations are commonly used in computer graphics for creating italic text effects, perspective corrections, and geometric distortions. They preserve area but change angles and shapes.
:::
Consider the following matrix (it is called the shear matrix): $S = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}$

1. What would you get if you apply $S$ on the vector $\begin{pmatrix} 0 \\ 1 \end{pmatrix}$?
2. What would you get if you apply $S$ again on the result of the previous point?
3. What if you apply $S$ one more time?
4. What do you think happens when we apply $S$ 100 times on that vector?
5. Can you compute $S^{100}$?

### 04: Diagonal matrix powers {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Diagonal matrices are particularly useful in linear algebra because their powers are easy to compute. This property is extensively used in eigenvalue decomposition and diagonalization of matrices (more on this later).
:::

Consider the diagonal matrix $A = \begin{pmatrix} 2 & 0 \\ 0 & -1 \end{pmatrix}$.

1. Compute $A^2$, $A^3$, and $A^4$.
2. Find a general formula for $A^n$ where $n$ is any positive integer.
3. What does this transformation represent geometrically? How does it affect the unit circle when applied repeatedly?
4. What happens when you apply this transformation to the vector $\begin{pmatrix} 1 \\ 1 \end{pmatrix}$ multiple times?

### 05: Determinant properties {data-difficulty="1"}

1. Prove that $\det(B^{-1}AB) = \det(A)$ if $B$ is invertible.

2. Suppose $Q$ is a $3 \times 3$ real matrix such that $Q^T Q = I$. What values can $\det(Q)$ take?

### 06: Normal equation for linear regression {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
The normal equation is a closed-form solution to linear regression problems. It directly computes the optimal parameters using matrix operations, avoiding the need for iterative optimization algorithms like gradient descent.
:::

Consider a simple linear regression problem where you want to fit a line $y = \theta_0 + \theta_1 x$ to the following data points:

| $x$ | $y$ |
|-----|-----|
| 1   | 2   |
| 2   | 4   |

1. Set up the design matrix $X$ (including the intercept column) and the target vector $\vec{y}$.

2. Use the normal equation $\vec{\theta} = (X^T X)^{-1} X^T \vec{y}$ to find the optimal parameters $\theta_0$ and $\theta_1$.

3. What line equation did you get? Does it make sense given the data?

4. Verify your result by checking that this line passes through the given data points.

::: {.content-visible when-profile="solution"}

### Solution {.solution-header}

**Part 1:** Setting up the matrices

Design matrix $X$ (with intercept column):
$$X = \begin{pmatrix} 1 & 1 \\ 1 & 2 \end{pmatrix}$$

Target vector:
$$\vec{y} = \begin{pmatrix} 2 \\ 4 \end{pmatrix}$$

**Part 2:** Computing the normal equation

First, compute $X^T$:
$$X^T = \begin{pmatrix} 1 & 1 \\ 1 & 2 \end{pmatrix}$$

Next, compute $X^T X$:
$$X^T X = \begin{pmatrix} 1 & 1 \\ 1 & 2 \end{pmatrix} \begin{pmatrix} 1 & 1 \\ 1 & 2 \end{pmatrix} = \begin{pmatrix} 2 & 3 \\ 3 & 5 \end{pmatrix}$$

Find $(X^T X)^{-1}$:
$$\det(X^T X) = 2 \cdot 5 - 3 \cdot 3 = 10 - 9 = 1$$
$$(X^T X)^{-1} = \frac{1}{1} \begin{pmatrix} 5 & -3 \\ -3 & 2 \end{pmatrix} = \begin{pmatrix} 5 & -3 \\ -3 & 2 \end{pmatrix}$$

Compute $X^T \vec{y}$:
$$X^T \vec{y} = \begin{pmatrix} 1 & 1 \\ 1 & 2 \end{pmatrix} \begin{pmatrix} 2 \\ 4 \end{pmatrix} = \begin{pmatrix} 6 \\ 10 \end{pmatrix}$$

Finally, compute $\vec{\theta}$:
$$\vec{\theta} = (X^T X)^{-1} X^T \vec{y} = \begin{pmatrix} 5 & -3 \\ -3 & 2 \end{pmatrix} \begin{pmatrix} 6 \\ 10 \end{pmatrix} = \begin{pmatrix} 0 \\ 2 \end{pmatrix}$$

**Part 3:** The line equation is $y = 0 + 2x = 2x$. This makes perfect sense since both data points lie exactly on this line.

**Part 4:** Verification:
- For $x = 1$: $y = 2(1) = 2$ âœ“
- For $x = 2$: $y = 2(2) = 4$ âœ“

:::




# ğŸ² 39 (02)
- â–¶ï¸[Õ„Õ¥Õ¶Ö„ Õ¥Õ¶Ö„ Õ´Õ¥Ö€ Õ½Õ¡Ö€Õ¥Ö€Õ¨](https://www.youtube.com/watch?v=9CbMaPXC8SI)
- ğŸ”—[Random link](https://www.youtube.com/watch?v=-33IXM8gC4g)
- ğŸ‡¦ğŸ‡²ğŸ¶[Ô±Õ¬Õ¥Ö„Õ½Õ¡Õ¶Õ¤Ö€ Ô±Õ³Õ¥Õ´ÕµÕ¡Õ¶ (ÕÕ«Ö€Õ¸ Õ°Õ¡Õ½Õ¡Õ¯)](https://www.youtube.com/watch?v=2lt23ofSku0)
- ğŸŒğŸ¶[Armin KÃ¼pper(Pipelinefunk)](https://youtu.be/p8GcHoSIPDg?si=kR7B9WfHmQ_-HRqi)
- ğŸ¤Œ[Ô¿Õ¡Ö€Õ£Õ«Õ¶](https://www.youtube.com/watch?v=HCTKe3SwUHU)

<a href="http://s01.flagcounter.com/more/1oO"><img src="https://s01.flagcounter.com/count2/1oO/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_0/pageviews_1/flags_0/percent_0/" alt="Flag Counter"></a>