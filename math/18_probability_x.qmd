---
title: "Probability II â€” LOTUS, Covariance, Joint/Conditional, Convergence"
format:
  html:
    css: homework-styles.css
---

<script src="homework-scripts.js"></script>

# ðŸ“š Topics

- Law of the Unconscious Statistician (LOTUS)
- Covariance and correlation
- Joint, marginal, and conditional distributions
- Convergence modes: in probability, almost sure, in distribution
- Python exercises for LLN and CLT

---

## 1) Law of the Unconscious Statistician (LOTUS)

### 01 Expectation Without the CDF {data-difficulty="2"}
Let $X\sim\mathrm{Uniform}(0,1)$. Define $Y=\log(1+X)$.

- a) Compute $\mathbb{E}[Y]$ using LOTUS directly.
- b) Compute $\operatorname{Var}(Y)$ (you may leave integrals in closed form).

### 03 Piecewise Payoff {data-difficulty="2"}
Let $X\sim\mathrm{Exp}(\lambda)$. A â€œrefund policyâ€ pays $g(X)=\min(X,c)$ for fixed $c>0$.

- a) Compute $\mathbb{E}[g(X)]$ using LOTUS.
- b) Compute $\mathbb{P}(g(X)=c)$.
- c) Find $\dfrac{d}{dc}\,\mathbb{E}[g(X)]$ and interpret.

---

## 2) Covariance and Correlation

### 05 Portfolio Risk With Correlation {data-difficulty="2"}
Two assets have returns $X,Y$ with
$\mathbb{E}[X]=\mu_X,\; \mathbb{E}[Y]=\mu_Y,\; \operatorname{Var}(X)=\sigma_X^2,\; \operatorname{Var}(Y)=\sigma_Y^2,\; \operatorname{Corr}(X,Y)=\rho$.
For $R=wX+(1-w)Y$:

- a) Compute $\operatorname{Var}(R)$ in terms of $w,\sigma_X,\sigma_Y,\rho$.
- b) Find the minimizing $w^*$.
- c) Give conditions on $\rho$ under which diversification â€œreally helps.â€

### 06 Correlation vs. Independence Trap {data-difficulty="2"}
Let $X\sim\mathcal{N}(0,1)$ and define $Y=X^2$.

- a) Compute $\operatorname{Cov}(X,Y)$ and $\operatorname{Corr}(X,Y)$.
- b) Are $X$ and $Y$ independent? Explain briefly.
- c) What does this teach you about using correlation in feature selection?

---

## 3) Joint, Marginal, and Conditional Distributions

### 09 Discrete Joint Table {data-difficulty="2"}
A joint PMF on $(X,Y)\in\{0,1,2\}\times\{0,1\}$ is given by
$\;p(x,y)\propto(x+1)(y+1)$ for $x\in\{0,1,2\},\; y\in\{0,1\}$.

- a) Find the normalizing constant.
- b) Compute marginals $p_X(x)$, $p_Y(y)$.
- c) Compute $p_{X\mid Y}(x\mid y)$.
- d) Are $X$ and $Y$ independent?

---

## 4) Convergence Modes (in probability, in distribution, almost sure)

### 13 Classify the Convergence {data-difficulty="3"}
Define random variables $X_n$ by:

- (i) $X_n=\mathbf{1}\{\text{the $n$-th coin toss is Heads}\}$.
- (ii) $X_n=\mathbf{1}\{\text{at least one Head occurs in tosses }1,\dots,n\}$.
- (iii) $X_n=\mathbf{1}\{U\le 1/n\}$ where $U\sim\mathrm{Uniform}(0,1)$.

For each sequence, determine whether $X_n$ converges
- a) in probability,
- b) almost surely,
- c) in distribution,

and identify the limit when it exists.

---

## 5) Python Exercises: LLN and CLT

### LLN: Sample-Mean Convergence {data-difficulty="2"}
Simulate the Law of Large Numbers (LLN) for a non-degenerate distribution (e.g., Exponential with rate 1 or Bernoulli with $p=0.3$).

- a) For $n\in\{10,50,100,500,1000,5000\}$, draw $M$ independent trials of size $n$ and compute $\bar X_n$ in each trial. Plot $\lvert \bar X_n-\mu\rvert$ vs $n$ (median and 90th percentile bands over trials).
- b) For a fixed $\varepsilon>0$, estimate $\mathbb{P}(|\bar X_n-\mu|>\varepsilon)$ and compare to Chebyshevâ€™s bound.
- c) Optional: On a logâ€“log plot, verify the $O(n^{-1/2})$ decay of the typical error.

```python
# LLN skeleton (fill in as an exercise)
import numpy as np
import matplotlib.pyplot as plt

rng = np.random.default_rng(0)
M = 2000
ns = np.array([10, 50, 100, 500, 1000, 5000])
# Choose a distribution: Exponential(1)
mu = 1.0

median_err = []
p90_err = []

for n in ns:
    samples = rng.exponential(scale=1.0, size=(M, n))
    xbar = samples.mean(axis=1)
    err = np.abs(xbar - mu)
    median_err.append(np.median(err))
    p90_err.append(np.quantile(err, 0.9))

plt.plot(ns, median_err, label="median |XÌ„ - Î¼|")
plt.plot(ns, p90_err, label="p90 |XÌ„ - Î¼|")
plt.xscale('log'); plt.yscale('log')
plt.legend(); plt.title('LLN error decay (Exponential(1))')
plt.xlabel('n'); plt.ylabel('error')
plt.show()
```

### CLT: Standardized Sums Approach Normal {data-difficulty="2"}
Demonstrate the Central Limit Theorem (CLT) with a non-Gaussian parent distribution (e.g., Exponential or Bernoulli).

- a) For fixed $n$ (e.g., $n\in\{2,5,10,30,50\}$), draw many sums $S_n=\sum_{i=1}^n X_i$, then standardize $Z_n=(S_n-n\mu)/(\sqrt{n}\,\sigma)$.
- b) Plot histograms of $Z_n$ overlaid with the standard normal density. Comment on convergence as $n$ grows.
- c) Optional: Use a normality test (e.g., Kolmogorovâ€“Smirnov) to quantify the fit.

```python
# CLT skeleton (fill in as an exercise)
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm, kstest

rng = np.random.default_rng(1)
ns = [2, 5, 10, 30, 50]
M = 20000

# Parent: Bernoulli(p)
p = 0.3
mu = p
sigma = np.sqrt(p*(1-p))

for n in ns:
    X = rng.binomial(1, p, size=(M, n)).astype(float)
    Sn = X.sum(axis=1)
    Zn = (Sn - n*mu) / (np.sqrt(n)*sigma)

    # Plot
    xs = np.linspace(-4, 4, 400)
    plt.figure()
    plt.hist(Zn, bins=60, density=True, alpha=0.6, label=f'n={n}')
    plt.plot(xs, norm.pdf(xs), 'r-', lw=2, label='N(0,1) pdf')

    # KS test vs standard normal
    ks_stat, ks_p = kstest(Zn, 'norm')
    plt.title(f'CLT demo (Bernoulli p={p}) â€” KS p-value: {ks_p:.3g}')
    plt.legend()
    plt.show()
```
