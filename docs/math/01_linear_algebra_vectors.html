<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>01 Vectors and Linear Algebra Fundamentals ‚Äì Python Math &amp; ML Course</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../misc/google_colab.html" rel="next">
<link href="../math/00_sets_comb_funcs.html" rel="prev">
<link href="../assets/metric.webp" rel="icon">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-ba1dc69c819719ff92c66bffe944bb46.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-45c9daaf6acd34b0420988892e8b8644.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-7fe27d669c2c9815f007ef6e49b33cf3.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-7dae178cec1b9eccbcf055202f48df61.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    var sidebar = document.getElementById('quarto-sidebar');
    var logo = sidebar.getElementsByTagName('img')[0];
    
    sidebar.insertBefore(logo, sidebar.firstChild);
  });
</script>
<style>
div.callout-ml.callout {
  border-left-color: #001489;
}
div.callout-ml.callout-style-default > .callout-header {
  background-color: rgba(0, 20, 137, 0.13);
}
div.callout-ml .callout-toggle::before {  background-image: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(33, 37, 41)" class="bi bi-chevron-down" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M1.646 4.646a.5.5 0 0 1 .708 0L8 10.293l5.646-5.647a.5.5 0 0 1 .708.708l-6 6a.5.5 0 0 1-.708 0l-6-6a.5.5 0 0 1 0-.708z"/></svg>');}
div.callout-ml.callout-style-default .callout-icon::before, div.callout-ml.callout-titled .callout-icon::before {
  content: 'ü§ñ';
  background-image: none;
}
div.callout-math.callout {
  border-left-color: #D90012;
}
div.callout-math.callout-style-default > .callout-header {
  background-color: rgba(217, 0, 18, 0.13);
}
div.callout-math .callout-toggle::before {  background-image: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(33, 37, 41)" class="bi bi-chevron-down" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M1.646 4.646a.5.5 0 0 1 .708 0L8 10.293l5.646-5.647a.5.5 0 0 1 .708.708l-6 6a.5.5 0 0 1-.708 0l-6-6a.5.5 0 0 1 0-.708z"/></svg>');}
div.callout-math.callout-style-default .callout-icon::before, div.callout-math.callout-titled .callout-icon::before {
  content: 'üìà';
  background-image: none;
}
div.callout-misc.callout {
  border-left-color: #FF9E1B;
}
div.callout-misc.callout-style-default > .callout-header {
  background-color: rgba(255, 158, 27, 0.13);
}
div.callout-misc .callout-toggle::before {  background-image: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(33, 37, 41)" class="bi bi-chevron-down" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M1.646 4.646a.5.5 0 0 1 .708 0L8 10.293l5.646-5.647a.5.5 0 0 1 .708.708l-6 6a.5.5 0 0 1-.708 0l-6-6a.5.5 0 0 1 0-.708z"/></svg>');}
div.callout-misc.callout-style-default .callout-icon::before, div.callout-misc.callout-titled .callout-icon::before {
  content: 'üßÄ';
  background-image: none;
}
div.callout-links.callout {
  border-left-color: #D90012;
}
div.callout-links.callout-style-default > .callout-header {
  background-color: rgba(217, 0, 18, 0.13);
}
div.callout-links .callout-toggle::before {  background-image: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(33, 37, 41)" class="bi bi-chevron-down" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M1.646 4.646a.5.5 0 0 1 .708 0L8 10.293l5.646-5.647a.5.5 0 0 1 .708.708l-6 6a.5.5 0 0 1-.708 0l-6-6a.5.5 0 0 1 0-.708z"/></svg>');}
div.callout-links.callout-style-default .callout-icon::before, div.callout-links.callout-titled .callout-icon::before {
  content: 'üîó';
  background-image: none;
}
div.callout-python.callout {
  border-left-color: #001489;
}
div.callout-python.callout-style-default > .callout-header {
  background-color: rgba(0, 20, 137, 0.13);
}
div.callout-python .callout-toggle::before {  background-image: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(33, 37, 41)" class="bi bi-chevron-down" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M1.646 4.646a.5.5 0 0 1 .708 0L8 10.293l5.646-5.647a.5.5 0 0 1 .708.708l-6 6a.5.5 0 0 1-.708 0l-6-6a.5.5 0 0 1 0-.708z"/></svg>');}
div.callout-python.callout-style-default .callout-icon::before, div.callout-python.callout-titled .callout-icon::before {
  content: 'üêç';
  background-image: none;
}
div.callout-libs.callout {
  border-left-color: #FF9E1B;
}
div.callout-libs.callout-style-default > .callout-header {
  background-color: rgba(255, 158, 27, 0.13);
}
div.callout-libs .callout-toggle::before {  background-image: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(33, 37, 41)" class="bi bi-chevron-down" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M1.646 4.646a.5.5 0 0 1 .708 0L8 10.293l5.646-5.647a.5.5 0 0 1 .708.708l-6 6a.5.5 0 0 1-.708 0l-6-6a.5.5 0 0 1 0-.708z"/></svg>');}
div.callout-libs.callout-style-default .callout-icon::before, div.callout-libs.callout-titled .callout-icon::before {
  content: 'üì¶';
  background-image: none;
}
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../custom-theme.css">
<link rel="stylesheet" href="homework-styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../math/00_sets_comb_funcs.html">üìà Math</a></li><li class="breadcrumb-item"><a href="../math/01_linear_algebra_vectors.html"><span class="chapter-title">01 Vectors and Linear Algebra Fundamentals</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../assets/metric.webp" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Python Math &amp; ML Course</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/HaykTarkhanyan/python_math_ml_course" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Python, Math, Machine Learning Course</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">üêç Python</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python/01_intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">01 Intro: Print, Comments, Variables, Numbers, Input</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python/02_conditions.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">02 Conditionals: if, elif, else, match, nested if</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python/03_str_range_list_some_funcs.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">03 String, Range, List, Some functions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python/04_loops.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">04 Loops: For, While, Break, Continue, Else</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python/05_lst_str_methods_one_line_if_for.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">05 List/String Methods; Ternary Operators; List Comprehensions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python/06_tuple_set_dictionary.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">06 Tuple, Set, Dictionary</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python/07_functions_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">07 Functions 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python/08_functions_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">08 Functions 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python/09_files_packages_terminal.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">09 Terminal, Packages, File I/O</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python/10_git_conda_pep8.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">10 Git, Anaconda (venvs), PEP8</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python/11_exception_handling.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">11 Exception handling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python/12_streamlit_recursion.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">12 Streamlit, Recursion, Misc</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python/13_decorators.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">13 Decorators</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python/14_classes.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">14 OOP 1: Classes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python/15_inheritance_polymorphism.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">15 OOP 2: Inheritance, Polymorphism</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python/16_encapsulation_abstraction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">16 OOP 3: Encapsulation, Abstraction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python/17_dataclass_iterator_generator_context_manager.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">17 OOP 4: Data Class, Iterators, Generators, Context Managers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python/18_youtube_translator.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">18 Final Project: YouTube Transcript Translator</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">üì¶ Libraries</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python_libs/01_openai_api_timestamp_generator.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">01 OpenAI + Timestamp</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python_libs/02_numpy.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">02 NumPy</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python_libs/03_pandas_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">03 Pandas 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python_libs/04_pandas_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">04 Pandas 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python_libs/05_noble_people_analysis.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">05 Noble People Analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python_libs/06_data_viz.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">06 Data Visualization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python_libs/07_kargin_project.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">07 Kargin Project</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python_libs/08_logging__clis.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">08: Logging, CLIs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python_libs/09_testing__debugging.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">09 Testing + Debugging</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python_libs/10_scraping__parallelization.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">10 Scraping; Parallel Processing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python_libs/11_ysu_scraping.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">11: YSU Scraping</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python_libs/12_sql.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">12 SQL (with Python)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python_libs/13_pydantic.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">13 FastAPI, Pydantic</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python_libs/14_misc_libraries.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">14 Misc: dotenv, pathlib, numba, collections, icecream</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python_libs/15_fast_api.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">15: FastAPI, async/await</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python_libs/16_dbs_supabase.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">16 DBs (Supabase)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python_libs/17_vibe_coding.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">17 Vibe Coding</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../python_libs/18_clean_code_architecture.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">18 Best practices for coding in ML/DS</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">üìà Math</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../math/00_sets_comb_funcs.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">00 Preliminaries: Sets, Combinatorics, Functions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../math/01_linear_algebra_vectors.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">01 Vectors and Linear Algebra Fundamentals</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">üßÄ Misc</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../misc/google_colab.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Google Colab-’´÷Å ÷Ö’£’ø’æ’•’¨</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#’∂’µ’∏÷Ç’©’®" id="toc-’∂’µ’∏÷Ç’©’®" class="nav-link active" data-scroll-target="#’∂’µ’∏÷Ç’©’®">üìö ’Ü’µ’∏÷Ç’©’®</a></li>
  <li><a href="#’ø’∂’°’µ’´’∂" id="toc-’ø’∂’°’µ’´’∂" class="nav-link" data-scroll-target="#’ø’∂’°’µ’´’∂">üè° ’è’∂’°’µ’´’∂</a>
  <ul class="collapse">
  <li><a href="#vector-operations" id="toc-vector-operations" class="nav-link" data-scroll-target="#vector-operations">Vector Operations</a>
  <ul class="collapse">
  <li><a href="#rgb-color-mixing-with-vectors" id="toc-rgb-color-mixing-with-vectors" class="nav-link" data-scroll-target="#rgb-color-mixing-with-vectors">01: RGB color mixing with vectors</a></li>
  <li><a href="#feature-vector-normalization" id="toc-feature-vector-normalization" class="nav-link" data-scroll-target="#feature-vector-normalization">02: Feature vector normalization</a></li>
  <li><a href="#distance-between-data-points" id="toc-distance-between-data-points" class="nav-link" data-scroll-target="#distance-between-data-points">03: Distance between data points</a></li>
  </ul></li>
  <li><a href="#dot-products-and-angles" id="toc-dot-products-and-angles" class="nav-link" data-scroll-target="#dot-products-and-angles">Dot Products and Angles</a>
  <ul class="collapse">
  <li><a href="#similarity-measurement" id="toc-similarity-measurement" class="nav-link" data-scroll-target="#similarity-measurement">04: Similarity measurement</a></li>
  <li><a href="#word-embeddings-similarity" id="toc-word-embeddings-similarity" class="nav-link" data-scroll-target="#word-embeddings-similarity">04.5: Word embeddings similarity</a></li>
  <li><a href="#matrix-transformations" id="toc-matrix-transformations" class="nav-link" data-scroll-target="#matrix-transformations">05: Matrix transformations</a></li>
  <li><a href="#finding-perpendicular-vectors" id="toc-finding-perpendicular-vectors" class="nav-link" data-scroll-target="#finding-perpendicular-vectors">06: Finding perpendicular vectors</a></li>
  <li><a href="#orthogonality-check" id="toc-orthogonality-check" class="nav-link" data-scroll-target="#orthogonality-check">06.5: Orthogonality check</a></li>
  <li><a href="#orthogonality-check-1" id="toc-orthogonality-check-1" class="nav-link" data-scroll-target="#orthogonality-check-1">06: Orthogonality check</a></li>
  <li><a href="#matrix-products" id="toc-matrix-products" class="nav-link" data-scroll-target="#matrix-products">07: Matrix products</a></li>
  <li><a href="#deriving-the-cosine-angle-formula" id="toc-deriving-the-cosine-angle-formula" class="nav-link" data-scroll-target="#deriving-the-cosine-angle-formula">08: Deriving the cosine angle formula</a></li>
  <li><a href="#deriving-the-cosine-angle-formula-1" id="toc-deriving-the-cosine-angle-formula-1" class="nav-link" data-scroll-target="#deriving-the-cosine-angle-formula-1">08: Deriving the cosine angle formula</a></li>
  <li><a href="#shear-matrix-transformations" id="toc-shear-matrix-transformations" class="nav-link" data-scroll-target="#shear-matrix-transformations">09: Shear matrix transformations</a></li>
  <li><a href="#projection-and-components" id="toc-projection-and-components" class="nav-link" data-scroll-target="#projection-and-components">10: Projection and components</a></li>
  </ul></li>
  <li><a href="#geometric-interpretation" id="toc-geometric-interpretation" class="nav-link" data-scroll-target="#geometric-interpretation">Geometric Interpretation</a>
  <ul class="collapse">
  <li><a href="#triangle-inequality" id="toc-triangle-inequality" class="nav-link" data-scroll-target="#triangle-inequality">08: Triangle inequality</a></li>
  <li><a href="#cross-product-applications" id="toc-cross-product-applications" class="nav-link" data-scroll-target="#cross-product-applications">09: Cross product applications</a></li>
  </ul></li>
  <li><a href="#linear-combinations-and-spans" id="toc-linear-combinations-and-spans" class="nav-link" data-scroll-target="#linear-combinations-and-spans">Linear Combinations and Spans</a>
  <ul class="collapse">
  <li><a href="#linear-combinations" id="toc-linear-combinations" class="nav-link" data-scroll-target="#linear-combinations">10: Linear combinations</a></li>
  <li><a href="#basis-vectors" id="toc-basis-vectors" class="nav-link" data-scroll-target="#basis-vectors">11: Basis vectors</a></li>
  <li><a href="#span-and-linear-independence" id="toc-span-and-linear-independence" class="nav-link" data-scroll-target="#span-and-linear-independence">12: Span and linear independence</a></li>
  <li><a href="#principal-component-analysis-pca-intuition" id="toc-principal-component-analysis-pca-intuition" class="nav-link" data-scroll-target="#principal-component-analysis-pca-intuition">13: Principal Component Analysis (PCA) intuition</a></li>
  <li><a href="#vector-in-machine-learning-cost-functions" id="toc-vector-in-machine-learning-cost-functions" class="nav-link" data-scroll-target="#vector-in-machine-learning-cost-functions">14: Vector in machine learning cost functions</a></li>
  <li><a href="#model-selection-with-regularization" id="toc-model-selection-with-regularization" class="nav-link" data-scroll-target="#model-selection-with-regularization">15: Model selection with regularization</a></li>
  <li><a href="#vector-spaces" id="toc-vector-spaces" class="nav-link" data-scroll-target="#vector-spaces">16: Vector spaces</a></li>
  <li><a href="#vector-subspaces" id="toc-vector-subspaces" class="nav-link" data-scroll-target="#vector-subspaces">17: Vector subspaces</a></li>
  <li><a href="#high-dimensional-vector-geometry" id="toc-high-dimensional-vector-geometry" class="nav-link" data-scroll-target="#high-dimensional-vector-geometry">18: High-dimensional vector geometry</a></li>
  </ul></li>
  <li><a href="#matrix-transformations-1" id="toc-matrix-transformations-1" class="nav-link" data-scroll-target="#matrix-transformations-1">Matrix Transformations</a>
  <ul class="collapse">
  <li><a href="#matrix-transformations-and-geometric-interpretation" id="toc-matrix-transformations-and-geometric-interpretation" class="nav-link" data-scroll-target="#matrix-transformations-and-geometric-interpretation">19: Matrix transformations and geometric interpretation</a></li>
  <li><a href="#matrix-operations" id="toc-matrix-operations" class="nav-link" data-scroll-target="#matrix-operations">20: Matrix operations</a></li>
  <li><a href="#shear-matrix-analysis" id="toc-shear-matrix-analysis" class="nav-link" data-scroll-target="#shear-matrix-analysis">21: Shear matrix analysis</a></li>
  <li><a href="#diagonal-matrix-powers" id="toc-diagonal-matrix-powers" class="nav-link" data-scroll-target="#diagonal-matrix-powers">21.5: Diagonal matrix powers</a></li>
  </ul></li>
  <li><a href="#vector-spaces-and-subspaces" id="toc-vector-spaces-and-subspaces" class="nav-link" data-scroll-target="#vector-spaces-and-subspaces">Vector Spaces and Subspaces</a>
  <ul class="collapse">
  <li><a href="#identifying-vector-spaces-and-non-vector-spaces" id="toc-identifying-vector-spaces-and-non-vector-spaces" class="nav-link" data-scroll-target="#identifying-vector-spaces-and-non-vector-spaces">22: Identifying vector spaces and non-vector spaces</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#’£’∏÷Ä’Æ’∂’°’Ø’°’∂" id="toc-’£’∏÷Ä’Æ’∂’°’Ø’°’∂" class="nav-link" data-scroll-target="#’£’∏÷Ä’Æ’∂’°’Ø’°’∂">üõ†Ô∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂</a></li>
  <li><a href="#section" id="toc-section" class="nav-link" data-scroll-target="#section">üé≤ 38 (01)</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/HaykTarkhanyan/python_math_ml_course/edit/main/math/01_linear_algebra_vectors.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../math/00_sets_comb_funcs.html">üìà Math</a></li><li class="breadcrumb-item"><a href="../math/01_linear_algebra_vectors.html"><span class="chapter-title">01 Vectors and Linear Algebra Fundamentals</span></a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-title">01 Vectors and Linear Algebra Fundamentals</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<script src="homework-scripts.js"></script>
<p><img src="../background_photos/math_01_vectors.jpg" class="img-fluid" alt="image.png"> <a href="https://unsplash.com/">’¨’∏÷Ç’Ω’°’∂’Ø’°÷Ä’´ ’∞’≤’∏÷Ç’¥’®</a>, ’Ä’•’≤’´’∂’°’Ø’ù <a href="https://unsplash.com/">Artist Name</a></p>
<section id="’∂’µ’∏÷Ç’©’®" class="level1">
<h1>üìö ’Ü’µ’∏÷Ç’©’®</h1>
<ul>
<li><a href="01_vectors_linear_algebra.qmd">üìö ‘±’¥’¢’∏’≤’ª’°’Ø’°’∂ ’∂’µ’∏÷Ç’©’®</a></li>
<li><a href="https://youtu.be/vectors_lecture">üì∫ ’è’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’®</a></li>
<li><a href="Lectures/L01_Vectors.pdf">üéûÔ∏è ’ç’¨’°’µ’§’•÷Ä - Vectors</a></li>
<li><a href="Lectures/L02_Geometry_of_Vectors__Matrices.pdf">üéûÔ∏è ’ç’¨’°’µ’§’•÷Ä - Geometry</a></li>
<li><a href="https://youtu.be/vectors_practical">üõ†Ô∏èüì∫ ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ ’ø’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’®</a></li>
<li><a href="Homeworks/hw_01_vectors.pdf">üõ†Ô∏èüóÇÔ∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ PDF-’®</a></li>
</ul>
<p>‘±’µ’Ω ’§’°’Ω’´’∂ ’Ø’Æ’°’∂’∏’©’°’∂’°’∂÷Ñ ’æ’•’Ø’ø’∏÷Ä’∂’•÷Ä’´ ÷á ’¥’°’ø÷Ä’´÷Å’∂’•÷Ä’´ ’∞’´’¥’∏÷Ç’∂÷Ñ’∂’•÷Ä’´’∂’ù</p>
<ol type="1">
<li>’é’•’Ø’ø’∏÷Ä’∂’•÷Ä (’£’∏÷Ä’Æ’∏’≤’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä, ’Ω’Ø’°’¨’µ’°÷Ä ’°÷Ä’ø’°’§÷Ä’µ’°’¨, ’∂’∏÷Ä’¥’°)</li>
<li>’é’•’Ø’ø’∏÷Ä’∂’•÷Ä’´ ’•÷Ä’Ø÷Ä’°’π’°÷É’∏÷Ç’©’µ’∏÷Ç’∂ (’°’∂’Ø’µ’∏÷Ç’∂’∂’•÷Ä, ’∏÷Ç’≤’≤’°’∞’°’µ’°÷Å’∏÷Ç’©’µ’∏÷Ç’∂)</li>
<li>’Ñ’°’ø÷Ä’´÷Å’∂’•÷Ä ÷á ’£’Æ’°’µ’´’∂ ÷É’∏’≠’°’Ø’•÷Ä’∫’∏÷Ç’¥’∂’•÷Ä</li>
<li>‘≥’Æ’°’µ’´’∂ ’∞’°’¥’°’Ø÷Å’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä ÷á ’ø’°÷Ä’°’Æ’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä</li>
</ol>
</section>
<section id="’ø’∂’°’µ’´’∂" class="level1">
<h1>üè° ’è’∂’°’µ’´’∂</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ol type="1">
<li>‚ùó‚ùó‚ùó DON‚ÄôT CHECK THE SOLUTIONS BEFORE TRYING TO DO THE HOMEWORK BY YOURSELF‚ùó‚ùó‚ùó</li>
<li>Please don‚Äôt hesitate to ask questions, never forget about the üçäkaralyoküçä principle!</li>
<li>The harder the problem is, the more üßÄcheesesüßÄ it has.</li>
<li>Problems with üéÅ are just extra bonuses. It would be good to try to solve them, but also it‚Äôs not the highest priority task.</li>
<li>If the problem involve many boring calculations, feel free to skip them - important part is understanding the concepts.</li>
<li>Submit your solutions <a href="https://forms.gle/CFEvNqFiTSsDLiFc6">here</a> (even if it‚Äôs unfinished)</li>
</ol>
</div>
</div>
</div>
<section id="vector-operations" class="level2">
<h2 class="anchored" data-anchor-id="vector-operations">Vector Operations</h2>
<section id="rgb-color-mixing-with-vectors" class="level3" data-difficulty="1">
<h3 data-difficulty="1" class="anchored" data-anchor-id="rgb-color-mixing-with-vectors">01: RGB color mixing with vectors</h3>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Context
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In computer graphics and image processing, colors are represented as RGB vectors where each component (Red, Green, Blue) ranges from 0 to 255. Vector operations on these RGB values correspond to color mixing and transformations.</p>
</div>
</div>
</div>
<p>Consider these RGB color vectors: - Red: <span class="math inline">\(\vec{r} = (255, 0, 0)\)</span> - Yellow: <span class="math inline">\(\vec{y} = (255, 255, 0)\)</span> - A custom color: <span class="math inline">\(\vec{c} = (128, 64, 192)\)</span></p>
<ol type="a">
<li><p>Calculate what color you get by adding red and yellow: <span class="math inline">\(\vec{r} + \vec{y}\)</span>. What happens when RGB values exceed 255?</p></li>
<li><p>Find the ‚Äúaverage‚Äù color between red and yellow: <span class="math inline">\(\frac{1}{2}(\vec{r} + \vec{y})\)</span></p></li>
<li><p>Use a color picker (like <a href="https://g.co/kgs/color-picker">Google‚Äôs color picker</a> or any online tool) to verify your answers from parts (a) and (b). What colors do you actually see?</p></li>
<li><p>Calculate <span class="math inline">\(\vec{r} - \frac{1}{2}\vec{y}\)</span>. What does this operation represent in terms of color mixing?</p></li>
</ol>
</section>
<section id="feature-vector-normalization" class="level3" data-difficulty="2">
<h3 data-difficulty="2" class="anchored" data-anchor-id="feature-vector-normalization">02: Feature vector normalization</h3>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Context
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In ML preprocessing, we often normalize feature vectors to have unit length. This helps algorithms that are sensitive to the scale of input features, like k-nearest neighbors or neural networks.</p>
</div>
</div>
</div>
<p>A customer profile is represented by the vector <span class="math inline">\(\vec{v} = (25, 50000, 3)\)</span> where components represent [age, income in $, number of purchases].</p>
<ol type="a">
<li>Calculate the Euclidean norm (magnitude) <span class="math inline">\(||\vec{v}||_2\)</span></li>
<li>Find the unit vector <span class="math inline">\(\hat{v} = \frac{\vec{v}}{||\vec{v}||_2}\)</span></li>
<li>Verify that <span class="math inline">\(||\hat{v}||_2 = 1\)</span></li>
</ol>
</section>
<section id="distance-between-data-points" class="level3" data-difficulty="2">
<h3 data-difficulty="2" class="anchored" data-anchor-id="distance-between-data-points">03: Distance between data points</h3>
<p>Two data points in a dataset are represented as <span class="math inline">\(\vec{p_1} = (1, 3, -2)\)</span> and <span class="math inline">\(\vec{p_2} = (4, -1, 1)\)</span>.</p>
<ol type="a">
<li>Calculate the Euclidean distance between these points</li>
<li>Calculate the Manhattan distance (L1 norm of the difference)</li>
<li>Which distance metric would be more robust to outliers? Explain briefly.</li>
</ol>
</section>
</section>
<section id="dot-products-and-angles" class="level2">
<h2 class="anchored" data-anchor-id="dot-products-and-angles">Dot Products and Angles</h2>
<section id="similarity-measurement" class="level3" data-difficulty="2">
<h3 data-difficulty="2" class="anchored" data-anchor-id="similarity-measurement">04: Similarity measurement</h3>
<div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Context
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The dot product is fundamental in measuring similarity between vectors. In recommendation systems, we often use cosine similarity (based on dot products) to find similar users or items.</p>
</div>
</div>
</div>
<p>Two user preference vectors are <span class="math inline">\(\vec{u_1} = (5, 3, 1, 4)\)</span> and <span class="math inline">\(\vec{u_2} = (3, 5, 2, 2)\)</span> where each component represents rating for different movie genres.</p>
<ol type="a">
<li>Calculate the dot product <span class="math inline">\(\vec{u_1} \cdot \vec{u_2}\)</span></li>
<li>Calculate the cosine similarity: <span class="math inline">\(\cos(\theta) = \frac{\vec{u_1} \cdot \vec{u_2}}{||\vec{u_1}|| \cdot ||\vec{u_2}||}\)</span></li>
<li>What does a cosine similarity close to 1 indicate about user preferences?</li>
</ol>
</section>
<section id="word-embeddings-similarity" class="level3" data-difficulty="2">
<h3 data-difficulty="2" class="anchored" data-anchor-id="word-embeddings-similarity">04.5: Word embeddings similarity</h3>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Context
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In NLP, words can be represented as vectors called embeddings. By comparing these vectors using different distance metrics, we can determine semantic similarity between words. This is the foundation of modern language models and search engines.</p>
</div>
</div>
</div>
<p>Given the following 2D word embeddings: - <strong>cheese:</strong> <span class="math inline">\((1, 2)\)</span> - <strong>mushroom:</strong> <span class="math inline">\((3, 1)\)</span><br>
- <strong>tasty:</strong> <span class="math inline">\((2, 2)\)</span></p>
<ol type="a">
<li><strong>Euclidean Distance Analysis:</strong>
<ul>
<li>Compute the Euclidean distance between <strong>tasty</strong> and <strong>cheese</strong></li>
<li>Compute the Euclidean distance between <strong>tasty</strong> and <strong>mushroom</strong></li>
<li>Which word is closer to <strong>tasty</strong> based on Euclidean distance?</li>
</ul></li>
<li><strong>Cosine Similarity Analysis:</strong>
<ul>
<li>Compute the cosine similarity between <strong>tasty</strong> and <strong>cheese</strong></li>
<li>Compute the cosine similarity between <strong>tasty</strong> and <strong>mushroom</strong></li>
<li>Which word is closer to <strong>tasty</strong> based on cosine similarity?</li>
</ul></li>
<li><strong>Discussion:</strong> Compare the outcomes from parts (a) and (b). Why might one metric be preferred over the other in different NLP applications?</li>
</ol>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Fun fact:</strong> Check out this <a href="https://youtu.be/wjZofJX0v4M?t=751">3Blue1Brown video on word vectors</a> for more insights!</p>
</div>
</div>
</div>
</section>
<section id="matrix-transformations" class="level3" data-difficulty="2">
<h3 data-difficulty="2" class="anchored" data-anchor-id="matrix-transformations">05: Matrix transformations</h3>
<p>What vectors do you get by applying the matrix <span class="math inline">\(A = \begin{pmatrix} 3 &amp; -3 \\ 3 &amp; 3 \end{pmatrix}\)</span> on the vectors:</p>
<ol type="a">
<li><p><span class="math inline">\(\vec{a} = \begin{pmatrix} 1 \\ 0 \end{pmatrix}\)</span></p></li>
<li><p><span class="math inline">\(\vec{b} = \begin{pmatrix} 0 \\ 1 \end{pmatrix}\)</span></p></li>
<li><p><span class="math inline">\(\vec{c} = \begin{pmatrix} 1 \\ 1 \end{pmatrix}\)</span></p></li>
<li><p>(Additional) Draw the vectors before and after multiplying with <span class="math inline">\(A\)</span>. What can you say visually about the matrix? Can you guess how it will act on the vector <span class="math inline">\(\begin{pmatrix} 2 \\ -2 \end{pmatrix}\)</span>?</p></li>
</ol>
</section>
<section id="finding-perpendicular-vectors" class="level3" data-difficulty="2">
<h3 data-difficulty="2" class="anchored" data-anchor-id="finding-perpendicular-vectors">06: Finding perpendicular vectors</h3>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Context
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Finding perpendicular vectors is fundamental in many applications, from computer graphics (surface normals) to optimization (gradient descent directions) and data analysis (principal component analysis).</p>
</div>
</div>
</div>
<p>Given the vector <span class="math inline">\(\vec{v} = (2, 3)\)</span>:</p>
<ol type="a">
<li><p>Find a non-zero vector <span class="math inline">\(\vec{w} = (x, y)\)</span> such that <span class="math inline">\(\vec{v}\)</span> and <span class="math inline">\(\vec{w}\)</span> are perpendicular.</p></li>
<li><p>Verify that your chosen vector <span class="math inline">\(\vec{w}\)</span> satisfies <span class="math inline">\(\vec{v} \cdot \vec{w} = 0\)</span>.</p></li>
<li><p>Find a unit vector in the direction of <span class="math inline">\(\vec{w}\)</span> by computing <span class="math inline">\(\frac{\vec{w}}{||\vec{w}||}\)</span>.</p></li>
<li><p>Explain why there are infinitely many vectors perpendicular to <span class="math inline">\(\vec{v}\)</span> and describe the general form of all such vectors.</p></li>
</ol>
</section>
<section id="orthogonality-check" class="level3" data-difficulty="1">
<h3 data-difficulty="1" class="anchored" data-anchor-id="orthogonality-check">06.5: Orthogonality check</h3>
<p>Determine which pairs of vectors are orthogonal (perpendicular):</p>
<ol type="a">
<li><span class="math inline">\(\vec{a} = (1, 2, -1)\)</span> and <span class="math inline">\(\vec{b} = (2, -1, 0)\)</span></li>
<li><span class="math inline">\(\vec{c} = (3, 4)\)</span> and <span class="math inline">\(\vec{d} = (-4, 3)\)</span></li>
<li><span class="math inline">\(\vec{e} = (1, 1, 1)\)</span> and <span class="math inline">\(\vec{f} = (1, -2, 1)\)</span></li>
</ol>
</section>
<section id="orthogonality-check-1" class="level3" data-difficulty="1">
<h3 data-difficulty="1" class="anchored" data-anchor-id="orthogonality-check-1">06: Orthogonality check</h3>
<p>Determine which pairs of vectors are orthogonal (perpendicular):</p>
<ol type="a">
<li><span class="math inline">\(\vec{a} = (1, 2, -1)\)</span> and <span class="math inline">\(\vec{b} = (2, -1, 0)\)</span></li>
<li><span class="math inline">\(\vec{c} = (3, 4)\)</span> and <span class="math inline">\(\vec{d} = (-4, 3)\)</span></li>
<li><span class="math inline">\(\vec{e} = (1, 1, 1)\)</span> and <span class="math inline">\(\vec{f} = (1, -2, 1)\)</span></li>
</ol>
</section>
<section id="matrix-products" class="level3" data-difficulty="2">
<h3 data-difficulty="2" class="anchored" data-anchor-id="matrix-products">07: Matrix products</h3>
<p>Compute the following products:</p>
<ol type="a">
<li><p><span class="math inline">\(AB\)</span>, where <span class="math inline">\(A = \begin{pmatrix} 6 &amp; 5 \\ -2 &amp; 7 \end{pmatrix}\)</span>, <span class="math inline">\(B = \begin{pmatrix} -5 &amp; 3 \\ 1 &amp; 4 \end{pmatrix}\)</span></p></li>
<li><p><span class="math inline">\((A - B)(A + B)\)</span>, where <span class="math inline">\(A = \begin{pmatrix} 2 &amp; 2 &amp; 4 \\ -3 &amp; -2 &amp; 4 \\ -2 &amp; 0 &amp; 2 \end{pmatrix}\)</span>, <span class="math inline">\(B = \begin{pmatrix} 2 &amp; 1 &amp; 3 \\ -1 &amp; 2 &amp; 2 \\ 1 &amp; 4 &amp; -1 \end{pmatrix}\)</span></p></li>
<li><p><span class="math inline">\(A^2 - B^2\)</span>, with the same <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> as in part (b).</p></li>
</ol>
</section>
<section id="deriving-the-cosine-angle-formula" class="level3" data-difficulty="3">
<h3 data-difficulty="3" class="anchored" data-anchor-id="deriving-the-cosine-angle-formula">08: Deriving the cosine angle formula</h3>
<p>Derive the formula for the cosine of the angle between two vectors: <span class="math inline">\(\cos(\theta) = \frac{\vec{a} \cdot \vec{b}}{||\vec{a}|| \cdot ||\vec{b}||}\)</span></p>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Hint
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Start with the law of cosines for a triangle: <span class="math inline">\(c^2 = a^2 + b^2 - 2ab\cos(\theta)\)</span>. Consider a triangle formed by vectors <span class="math inline">\(\vec{a}\)</span>, <span class="math inline">\(\vec{b}\)</span>, and <span class="math inline">\(\vec{a} - \vec{b}\)</span>. The side lengths are <span class="math inline">\(||\vec{a}||\)</span>, <span class="math inline">\(||\vec{b}||\)</span>, and <span class="math inline">\(||\vec{a} - \vec{b}||\)</span>. Express <span class="math inline">\(||\vec{a} - \vec{b}||^2\)</span> using the dot product and substitute into the law of cosines.</p>
</div>
</div>
</div>
<ol type="a">
<li><p>Write down the law of cosines for the triangle with sides <span class="math inline">\(||\vec{a}||\)</span>, <span class="math inline">\(||\vec{b}||\)</span>, and <span class="math inline">\(||\vec{a} - \vec{b}||\)</span></p></li>
<li><p>Express <span class="math inline">\(||\vec{a} - \vec{b}||^2\)</span> in terms of dot products by expanding <span class="math inline">\((\vec{a} - \vec{b}) \cdot (\vec{a} - \vec{b})\)</span></p></li>
<li><p>Substitute your result from part (b) into the law of cosines and solve for <span class="math inline">\(\cos(\theta)\)</span></p></li>
<li><p>Verify your derived formula using vectors <span class="math inline">\(\vec{u} = (3, 4)\)</span> and <span class="math inline">\(\vec{v} = (1, 0)\)</span></p></li>
</ol>
</section>
<section id="deriving-the-cosine-angle-formula-1" class="level3" data-difficulty="3">
<h3 data-difficulty="3" class="anchored" data-anchor-id="deriving-the-cosine-angle-formula-1">08: Deriving the cosine angle formula</h3>
<p>Derive the formula for the cosine of the angle between two vectors: <span class="math inline">\(\cos(\theta) = \frac{\vec{a} \cdot \vec{b}}{||\vec{a}|| \cdot ||\vec{b}||}\)</span></p>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Hint
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Start with the law of cosines for a triangle: <span class="math inline">\(c^2 = a^2 + b^2 - 2ab\cos(\theta)\)</span>. Consider a triangle formed by vectors <span class="math inline">\(\vec{a}\)</span>, <span class="math inline">\(\vec{b}\)</span>, and <span class="math inline">\(\vec{a} - \vec{b}\)</span>. The side lengths are <span class="math inline">\(||\vec{a}||\)</span>, <span class="math inline">\(||\vec{b}||\)</span>, and <span class="math inline">\(||\vec{a} - \vec{b}||\)</span>. Express <span class="math inline">\(||\vec{a} - \vec{b}||^2\)</span> using the dot product and substitute into the law of cosines.</p>
</div>
</div>
</div>
<ol type="a">
<li><p>Write down the law of cosines for the triangle with sides <span class="math inline">\(||\vec{a}||\)</span>, <span class="math inline">\(||\vec{b}||\)</span>, and <span class="math inline">\(||\vec{a} - \vec{b}||\)</span></p></li>
<li><p>Express <span class="math inline">\(||\vec{a} - \vec{b}||^2\)</span> in terms of dot products by expanding <span class="math inline">\((\vec{a} - \vec{b}) \cdot (\vec{a} - \vec{b})\)</span></p></li>
<li><p>Substitute your result from part (b) into the law of cosines and solve for <span class="math inline">\(\cos(\theta)\)</span></p></li>
<li><p>Verify your derived formula using vectors <span class="math inline">\(\vec{u} = (3, 4)\)</span> and <span class="math inline">\(\vec{v} = (1, 0)\)</span></p></li>
</ol>
</section>
<section id="shear-matrix-transformations" class="level3" data-difficulty="2">
<h3 data-difficulty="2" class="anchored" data-anchor-id="shear-matrix-transformations">09: Shear matrix transformations</h3>
<p>Consider the following matrix (it is called the shear matrix): <span class="math inline">\(S = \begin{pmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{pmatrix}\)</span></p>
<ol type="a">
<li><p>What would you get if you apply <span class="math inline">\(S\)</span> on the vector <span class="math inline">\(\begin{pmatrix} 0 \\ 1 \end{pmatrix}\)</span>?</p></li>
<li><p>What would you get if you apply <span class="math inline">\(S\)</span> again on the result of the previous point?</p></li>
<li><p>What if you apply <span class="math inline">\(S\)</span> one more time?</p></li>
<li><p>What do you think happens when we apply <span class="math inline">\(S\)</span> 100 times on that vector?</p></li>
<li><p>Can you compute <span class="math inline">\(S^{100}\)</span>?</p></li>
</ol>
</section>
<section id="projection-and-components" class="level3" data-difficulty="3">
<h3 data-difficulty="3" class="anchored" data-anchor-id="projection-and-components">10: Projection and components</h3>
<p>Given vectors <span class="math inline">\(\vec{a} = (4, 3)\)</span> and <span class="math inline">\(\vec{b} = (1, 2)\)</span>:</p>
<ol type="a">
<li>Find the projection of <span class="math inline">\(\vec{a}\)</span> onto <span class="math inline">\(\vec{b}\)</span>: <span class="math inline">\(\text{proj}_{\vec{b}}\vec{a} = \frac{\vec{a} \cdot \vec{b}}{||\vec{b}||^2}\vec{b}\)</span></li>
<li>Find the component of <span class="math inline">\(\vec{a}\)</span> perpendicular to <span class="math inline">\(\vec{b}\)</span></li>
<li>Verify that <span class="math inline">\(\vec{a} = \text{proj}_{\vec{b}}\vec{a} + \vec{a}_{\perp}\)</span></li>
</ol>
</section>
</section>
<section id="geometric-interpretation" class="level2">
<h2 class="anchored" data-anchor-id="geometric-interpretation">Geometric Interpretation</h2>
<section id="triangle-inequality" class="level3" data-difficulty="2">
<h3 data-difficulty="2" class="anchored" data-anchor-id="triangle-inequality">08: Triangle inequality</h3>
<p>For vectors <span class="math inline">\(\vec{u} = (3, 4)\)</span> and <span class="math inline">\(\vec{v} = (5, -12)\)</span>:</p>
<ol type="a">
<li>Calculate <span class="math inline">\(||\vec{u}||\)</span>, <span class="math inline">\(||\vec{v}||\)</span>, and <span class="math inline">\(||\vec{u} + \vec{v}||\)</span></li>
<li>Verify the triangle inequality: <span class="math inline">\(||\vec{u} + \vec{v}|| \leq ||\vec{u}|| + ||\vec{v}||\)</span></li>
<li>When does equality hold in the triangle inequality?</li>
</ol>
</section>
<section id="cross-product-applications" class="level3" data-difficulty="3">
<h3 data-difficulty="3" class="anchored" data-anchor-id="cross-product-applications">09: Cross product applications</h3>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Context
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Cross products are used in computer graphics for calculating surface normals, determining the orientation of objects, and computing areas of parallelograms.</p>
</div>
</div>
</div>
<p>Given vectors <span class="math inline">\(\vec{a} = (2, 1, -1)\)</span> and <span class="math inline">\(\vec{b} = (1, 3, 2)\)</span>:</p>
<ol type="a">
<li>Calculate the cross product <span class="math inline">\(\vec{a} \times \vec{b}\)</span></li>
<li>Verify that <span class="math inline">\(\vec{a} \times \vec{b}\)</span> is orthogonal to both <span class="math inline">\(\vec{a}\)</span> and <span class="math inline">\(\vec{b}\)</span></li>
<li>Find the area of the parallelogram spanned by <span class="math inline">\(\vec{a}\)</span> and <span class="math inline">\(\vec{b}\)</span></li>
</ol>
</section>
</section>
<section id="linear-combinations-and-spans" class="level2">
<h2 class="anchored" data-anchor-id="linear-combinations-and-spans">Linear Combinations and Spans</h2>
<section id="linear-combinations" class="level3" data-difficulty="2">
<h3 data-difficulty="2" class="anchored" data-anchor-id="linear-combinations">10: Linear combinations</h3>
<p>Given vectors <span class="math inline">\(\vec{v_1} = (1, 2, 1)\)</span>, <span class="math inline">\(\vec{v_2} = (2, -1, 3)\)</span>, and <span class="math inline">\(\vec{v_3} = (1, -4, 1)\)</span>:</p>
<ol type="a">
<li>Express <span class="math inline">\(\vec{w} = (5, 0, 7)\)</span> as a linear combination of <span class="math inline">\(\vec{v_1}\)</span> and <span class="math inline">\(\vec{v_2}\)</span> if possible</li>
<li>Can <span class="math inline">\(\vec{v_3}\)</span> be written as a linear combination of <span class="math inline">\(\vec{v_1}\)</span> and <span class="math inline">\(\vec{v_2}\)</span>?</li>
<li>What does it mean geometrically if three vectors are linearly dependent?</li>
</ol>
</section>
<section id="basis-vectors" class="level3" data-difficulty="2">
<h3 data-difficulty="2" class="anchored" data-anchor-id="basis-vectors">11: Basis vectors</h3>
<p>Consider the vectors <span class="math inline">\(\vec{e_1} = (1, 0, 0)\)</span>, <span class="math inline">\(\vec{e_2} = (0, 1, 0)\)</span>, and <span class="math inline">\(\vec{e_3} = (0, 0, 1)\)</span>:</p>
<ol type="a">
<li>Express the vector <span class="math inline">\(\vec{v} = (7, -3, 5)\)</span> in terms of <span class="math inline">\(\vec{e_1}\)</span>, <span class="math inline">\(\vec{e_2}\)</span>, and <span class="math inline">\(\vec{e_3}\)</span></li>
<li>Why are <span class="math inline">\(\vec{e_1}\)</span>, <span class="math inline">\(\vec{e_2}\)</span>, and <span class="math inline">\(\vec{e_3}\)</span> called the standard basis vectors for <span class="math inline">\(\mathbb{R}^3\)</span>?</li>
<li>Can any vector in <span class="math inline">\(\mathbb{R}^3\)</span> be expressed uniquely as a linear combination of these basis vectors?</li>
</ol>
</section>
<section id="span-and-linear-independence" class="level3" data-difficulty="3">
<h3 data-difficulty="3" class="anchored" data-anchor-id="span-and-linear-independence">12: Span and linear independence</h3>
<p>Determine whether the following sets of vectors span <span class="math inline">\(\mathbb{R}^3\)</span> and whether they are linearly independent:</p>
<ol type="a">
<li><span class="math inline">\(\{(1, 0, 1), (0, 1, 1), (1, 1, 0)\}\)</span></li>
<li><span class="math inline">\(\{(1, 2, 3), (2, 4, 6), (0, 1, 2)\}\)</span></li>
<li><span class="math inline">\(\{(1, 0, 0), (1, 1, 0), (1, 1, 1), (0, 1, 1)\}\)</span></li>
</ol>
</section>
<section id="principal-component-analysis-pca-intuition" class="level3" data-difficulty="3">
<h3 data-difficulty="3" class="anchored" data-anchor-id="principal-component-analysis-pca-intuition">13: Principal Component Analysis (PCA) intuition</h3>
<div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Context
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>PCA is a dimensionality reduction technique that finds the directions (principal components) along which data varies the most. These directions are eigenvectors of the covariance matrix.</p>
</div>
</div>
</div>
<p>Given a 2D dataset with points: <span class="math inline">\((1, 1)\)</span>, <span class="math inline">\((2, 2)\)</span>, <span class="math inline">\((3, 3)\)</span>, <span class="math inline">\((1, 3)\)</span>, <span class="math inline">\((3, 1)\)</span>:</p>
<ol type="a">
<li>Calculate the mean vector <span class="math inline">\(\vec{\mu}\)</span> of the dataset</li>
<li>Center the data by subtracting the mean from each point</li>
<li>Which direction would you expect the first principal component to point? Explain intuitively.</li>
</ol>
</section>
<section id="vector-in-machine-learning-cost-functions" class="level3" data-difficulty="3">
<h3 data-difficulty="3" class="anchored" data-anchor-id="vector-in-machine-learning-cost-functions">14: Vector in machine learning cost functions</h3>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Context
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In linear regression, we minimize the sum of squared errors. This can be expressed elegantly using vector notation, where the error vector‚Äôs magnitude represents the total cost.</p>
</div>
</div>
</div>
<p>In a simple linear regression with predictions <span class="math inline">\(\vec{y}_{pred} = (2.1, 4.8, 6.9, 9.2)\)</span> and actual values <span class="math inline">\(\vec{y}_{true} = (2, 5, 7, 9)\)</span>:</p>
<ol type="a">
<li>Calculate the error vector <span class="math inline">\(\vec{e} = \vec{y}_{true} - \vec{y}_{pred}\)</span></li>
<li>Calculate the mean squared error: <span class="math inline">\(MSE = \frac{1}{n}||\vec{e}||^2\)</span></li>
<li>How does minimizing <span class="math inline">\(||\vec{e}||^2\)</span> relate to finding the best-fit line?</li>
</ol>
</section>
<section id="model-selection-with-regularization" class="level3" data-difficulty="3">
<h3 data-difficulty="3" class="anchored" data-anchor-id="model-selection-with-regularization">15: Model selection with regularization</h3>
<div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Context
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In machine learning, complex models with large weights can ‚Äúmemorize‚Äù the training data instead of learning generalizable patterns. This leads to overfitting - the model performs well on training data but poorly on new data. Regularization helps by penalizing large weight values, encouraging simpler models that generalize better. L1 regularization (Lasso) uses the Manhattan norm and can drive some weights to zero, while L2 regularization (Ridge) uses the Euclidean norm and keeps all weights small. The total loss = prediction error + Œª √ó regularization penalty.</p>
</div>
</div>
</div>
<p>Two models have learned different weight vectors: - Model A: <span class="math inline">\(\vec{w_A} = (2.5, -1.8, 0.3, 4.1)\)</span> - Model B: <span class="math inline">\(\vec{w_B} = (1.2, -0.9, 0.7, 2.3)\)</span></p>
<p>Both models achieve the same prediction error of <span class="math inline">\(E = 12.5\)</span> on the validation set.</p>
<ol type="a">
<li><p>Calculate the L1 regularization term for each model: <span class="math inline">\(R_{L1} = ||\vec{w}||_1 = \sum_i |w_i|\)</span></p></li>
<li><p>Calculate the L2 regularization term for each model: <span class="math inline">\(R_{L2} = ||\vec{w}||_2^2 = \sum_i w_i^2\)</span></p></li>
<li><p>For regularization parameter <span class="math inline">\(\lambda = 0.1\)</span>, calculate the total regularized loss for each model using both L1 and L2 regularization:</p>
<ul>
<li><span class="math inline">\(Loss_{L1} = E + \lambda \cdot R_{L1}\)</span><br>
</li>
<li><span class="math inline">\(Loss_{L2} = E + \lambda \cdot R_{L2}\)</span></li>
</ul></li>
<li><p>Which model would you choose under L1 regularization? Which under L2 regularization? Explain the difference.</p></li>
</ol>
</section>
<section id="vector-spaces" class="level3" data-difficulty="2">
<h3 data-difficulty="2" class="anchored" data-anchor-id="vector-spaces">16: Vector spaces</h3>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-13-contents" aria-controls="callout-13" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Context
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-13" class="callout-13-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A vector space is a set of vectors that is closed under vector addition and scalar multiplication, and satisfies certain axioms. Understanding vector spaces is crucial for linear algebra and provides the foundation for many ML concepts like feature spaces and function spaces.</p>
</div>
</div>
</div>
<p>Consider the set <span class="math inline">\(S = \{(a, 2a, 3a) : a \in \mathbb{R}\}\)</span> (all vectors of the form <span class="math inline">\((a, 2a, 3a)\)</span> where <span class="math inline">\(a\)</span> is any real number).</p>
<ol type="a">
<li><p>Show that <span class="math inline">\(S\)</span> is closed under vector addition by taking two arbitrary vectors from <span class="math inline">\(S\)</span> and showing their sum is also in <span class="math inline">\(S\)</span>.</p></li>
<li><p>Show that <span class="math inline">\(S\)</span> is closed under scalar multiplication by taking an arbitrary vector from <span class="math inline">\(S\)</span> and an arbitrary scalar.</p></li>
<li><p>Does <span class="math inline">\(S\)</span> form a vector space? What about the zero vector requirement?</p></li>
<li><p>Give a geometric interpretation of what the set <span class="math inline">\(S\)</span> represents in <span class="math inline">\(\mathbb{R}^3\)</span>.</p></li>
</ol>
</section>
<section id="vector-subspaces" class="level3" data-difficulty="3">
<h3 data-difficulty="3" class="anchored" data-anchor-id="vector-subspaces">17: Vector subspaces</h3>
<div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-14-contents" aria-controls="callout-14" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Context
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-14" class="callout-14-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A vector subspace is a subset of a vector space that is itself a vector space. In machine learning, feature subspaces are often used for dimensionality reduction, and understanding subspaces helps with concepts like the null space and column space of matrices.</p>
</div>
</div>
</div>
<p>Determine which of the following sets are vector subspaces of <span class="math inline">\(\mathbb{R}^3\)</span>:</p>
<ol type="a">
<li><p><span class="math inline">\(V_1 = \{(x, y, z) : x + y + z = 0\}\)</span> (vectors whose components sum to zero)</p></li>
<li><p><span class="math inline">\(V_2 = \{(x, y, z) : x + y + z = 1\}\)</span> (vectors whose components sum to one)</p></li>
<li><p><span class="math inline">\(V_3 = \{(x, 0, z) : x, z \in \mathbb{R}\}\)</span> (vectors with zero as the middle component)</p></li>
<li><p><span class="math inline">\(V_4 = \{(x, y, z) : x^2 + y^2 + z^2 \leq 1\}\)</span> (vectors inside or on the unit sphere)</p></li>
</ol>
<p>For each set, check the three subspace requirements: (1) contains zero vector, (2) closed under addition, (3) closed under scalar multiplication.</p>
</section>
<section id="high-dimensional-vector-geometry" class="level3 bonus-problem" data-difficulty="3">
<h3 class="bonus-problem anchored" data-difficulty="3" data-anchor-id="high-dimensional-vector-geometry">18: High-dimensional vector geometry</h3>
<p>In high-dimensional spaces (common in ML), our intuition about geometry can be misleading.</p>
<p>Consider the unit sphere in <span class="math inline">\(\mathbb{R}^n\)</span> (all vectors with norm 1):</p>
<ol type="a">
<li>In 2D, what fraction of a unit square <span class="math inline">\([-1,1] \times [-1,1]\)</span> is occupied by the unit circle?</li>
<li>Estimate this fraction for a unit cube in 3D</li>
<li>Research: What happens to this fraction as the dimension <span class="math inline">\(n\)</span> increases? This is known as the ‚Äúcurse of dimensionality.‚Äù</li>
</ol>
</section>
</section>
<section id="matrix-transformations-1" class="level2">
<h2 class="anchored" data-anchor-id="matrix-transformations-1">Matrix Transformations</h2>
<section id="matrix-transformations-and-geometric-interpretation" class="level3" data-difficulty="2">
<h3 data-difficulty="2" class="anchored" data-anchor-id="matrix-transformations-and-geometric-interpretation">19: Matrix transformations and geometric interpretation</h3>
<p>What vectors do you get by applying the matrix <span class="math inline">\(A = \begin{pmatrix} 3 &amp; -3 \\ 3 &amp; 3 \end{pmatrix}\)</span> on the vectors:</p>
<ol type="a">
<li><p><span class="math inline">\(\vec{a} = \begin{pmatrix} 1 \\ 0 \end{pmatrix}\)</span></p></li>
<li><p><span class="math inline">\(\vec{b} = \begin{pmatrix} 0 \\ 1 \end{pmatrix}\)</span></p></li>
<li><p><span class="math inline">\(\vec{c} = \begin{pmatrix} 1 \\ 1 \end{pmatrix}\)</span></p></li>
</ol>
<p><strong>Additional:</strong> Draw the vectors before and after multiplying with <span class="math inline">\(A\)</span>. What can you say visually about the matrix? Can you guess how it will act on the vector <span class="math inline">\(\begin{pmatrix} 2 \\ -2 \end{pmatrix}\)</span>?</p>
</section>
<section id="matrix-operations" class="level3" data-difficulty="2">
<h3 data-difficulty="2" class="anchored" data-anchor-id="matrix-operations">20: Matrix operations</h3>
<p>Compute the following products:</p>
<ol type="a">
<li><p><span class="math inline">\(AB\)</span>, where <span class="math inline">\(A = \begin{pmatrix} 6 &amp; 5 \\ -2 &amp; 7 \end{pmatrix}\)</span>, <span class="math inline">\(B = \begin{pmatrix} -5 &amp; 3 \\ 1 &amp; 4 \end{pmatrix}\)</span></p></li>
<li><p><span class="math inline">\((A - B)(A + B)\)</span>, where <span class="math inline">\(A = \begin{pmatrix} 2 &amp; 2 &amp; 4 \\ -3 &amp; -2 &amp; 4 \\ -2 &amp; 0 &amp; 2 \end{pmatrix}\)</span>, <span class="math inline">\(B = \begin{pmatrix} 2 &amp; 1 &amp; 3 \\ -1 &amp; 2 &amp; 2 \\ 1 &amp; 4 &amp; -1 \end{pmatrix}\)</span></p></li>
<li><p><span class="math inline">\(A^2 - B^2\)</span>, with the same <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> as in part (b).</p></li>
</ol>
</section>
<section id="shear-matrix-analysis" class="level3 bonus-problem" data-difficulty="3">
<h3 class="bonus-problem anchored" data-difficulty="3" data-anchor-id="shear-matrix-analysis">21: Shear matrix analysis</h3>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-15-contents" aria-controls="callout-15" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Context
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-15" class="callout-15-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Shear transformations are commonly used in computer graphics for creating italic text effects, perspective corrections, and geometric distortions. They preserve area but change angles and shapes.</p>
</div>
</div>
</div>
<p>Consider the shear matrix <span class="math inline">\(S = \begin{pmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{pmatrix}\)</span>:</p>
<ol type="a">
<li><p>What would you get if you apply <span class="math inline">\(S\)</span> on the vector <span class="math inline">\(\begin{pmatrix} 0 \\ 1 \end{pmatrix}\)</span>?</p></li>
<li><p>What would you get if you apply <span class="math inline">\(S\)</span> again on the result of the previous point?</p></li>
<li><p>What if you apply <span class="math inline">\(S\)</span> one more time?</p></li>
<li><p>What do you think happens when we apply <span class="math inline">\(S\)</span> 100 times on that vector?</p></li>
<li><p>Can you compute <span class="math inline">\(S^{100}\)</span>?</p></li>
</ol>
</section>
<section id="diagonal-matrix-powers" class="level3" data-difficulty="2">
<h3 data-difficulty="2" class="anchored" data-anchor-id="diagonal-matrix-powers">21.5: Diagonal matrix powers</h3>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-16-contents" aria-controls="callout-16" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Context
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-16" class="callout-16-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Diagonal matrices are particularly useful in linear algebra because their powers are easy to compute. This property is extensively used in eigenvalue decomposition and diagonalization of matrices.</p>
</div>
</div>
</div>
<p>Consider the diagonal matrix <span class="math inline">\(A = \begin{pmatrix} 2 &amp; 0 \\ 0 &amp; -1 \end{pmatrix}\)</span>.</p>
<ol type="a">
<li><p>Compute <span class="math inline">\(A^2\)</span>, <span class="math inline">\(A^3\)</span>, and <span class="math inline">\(A^4\)</span>.</p></li>
<li><p>Find a general formula for <span class="math inline">\(A^n\)</span> where <span class="math inline">\(n\)</span> is any positive integer.</p></li>
<li><p>What does this transformation represent geometrically? How does it affect the unit circle when applied repeatedly?</p></li>
<li><p><strong>Bonus:</strong> What happens when you apply this transformation to the vector <span class="math inline">\(\begin{pmatrix} 1 \\ 1 \end{pmatrix}\)</span> multiple times?</p></li>
</ol>
</section>
</section>
<section id="vector-spaces-and-subspaces" class="level2">
<h2 class="anchored" data-anchor-id="vector-spaces-and-subspaces">Vector Spaces and Subspaces</h2>
<section id="identifying-vector-spaces-and-non-vector-spaces" class="level3" data-difficulty="3">
<h3 data-difficulty="3" class="anchored" data-anchor-id="identifying-vector-spaces-and-non-vector-spaces">22: Identifying vector spaces and non-vector spaces</h3>
<div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-17-contents" aria-controls="callout-17" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Context
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-17" class="callout-17-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Understanding what constitutes a vector space is fundamental in linear algebra. A set must satisfy specific axioms to be considered a vector space: closure under addition and scalar multiplication, existence of zero element, existence of additive inverses, and several other properties.</p>
</div>
</div>
</div>
<p>For each of the following sets, determine whether it is a vector space or not. If it is a vector space, prove it by verifying all the required axioms. If it is not a vector space, identify which axiom(s) fail and provide counterexamples.</p>
<ol type="a">
<li><p><span class="math inline">\(A = \left\{\begin{pmatrix} a \\ 0 \end{pmatrix} \mid a \in \mathbb{R}\right\}\)</span> (vectors with second component zero)</p></li>
<li><p><span class="math inline">\(B = \left\{\begin{pmatrix} a \\ -a \end{pmatrix} \mid a \in \mathbb{R}\right\}\)</span> (vectors where second component is negative of first)</p></li>
<li><p><span class="math inline">\(C = \mathbb{N}\)</span> (the set of natural numbers)</p></li>
<li><p><span class="math inline">\(D = \left\{\begin{pmatrix} a \\ 1 \end{pmatrix} \mid a \in \mathbb{R}\right\}\)</span> (vectors with second component always 1)</p></li>
</ol>
<p><strong>Hint:</strong> For the non-vector spaces, show that there are some ‚Äúbad‚Äù elements such that if we add them or multiply with some number (not necessarily positive), the result would not belong to the set.</p>
</section>
</section>
</section>
<section id="’£’∏÷Ä’Æ’∂’°’Ø’°’∂" class="level1">
<h1>üõ†Ô∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂</h1>
<ul>
<li><a href="https://youtu.be/vectors_practical">üõ†Ô∏èüì∫ ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ ’ø’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’®</a></li>
<li><a href="Homeworks/hw_01_vectors_practical.pdf">üõ†Ô∏èüóÇÔ∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ PDF-’®</a></li>
</ul>
<p>Practice problems include: - Implementing vector operations in Python/NumPy - Visualizing vectors and transformations - Computing similarity matrices for recommendation systems - PCA implementation on real datasets</p>
</section>
<section id="section" class="level1">
<h1>üé≤ 38 (01)</h1>
<ul>
<li>‚ñ∂Ô∏è<a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">3Blue1Brown - Essence of Linear Algebra</a></li>
<li>üîó<a href="https://www.3blue1brown.com/topics/linear-algebra">Linear Algebra Visualizations</a></li>
<li>üá¶üá≤üé∂<a href="https://www.youtube.com/watch?v=white_trees_forever">’ç’∫’´’ø’°’Ø ‘æ’°’º’•÷Ä (’Ä’°’æ’•’ø)</a></li>
<li>üåêüé∂<a href="https://www.youtube.com/watch?v=onRk0sjSgFU">Radiohead (Everything In Its Right Place)</a></li>
<li>ü§å<a href="https://www.youtube.com/watch?v=vector_garden">’é’•’Ø’ø’∏÷Ä’°’Ø’°’∂ ’∫’°÷Ä’ø’•’¶</a></li>
</ul>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../math/00_sets_comb_funcs.html" class="pagination-link" aria-label="00 Preliminaries: Sets, Combinatorics, Functions">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">00 Preliminaries: Sets, Combinatorics, Functions</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../misc/google_colab.html" class="pagination-link" aria-label="Google Colab-’´÷Å ÷Ö’£’ø’æ’•’¨">
        <span class="nav-page-text"><span class="chapter-title">Google Colab-’´÷Å ÷Ö’£’ø’æ’•’¨</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "01 Vectors and Linear Algebra Fundamentals"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">    css: homework-styles.css</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>&lt;script src="homework-scripts.js"&gt;&lt;/script&gt;</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="al">![image.png](../background_photos/math_01_vectors.jpg)</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">’¨’∏÷Ç’Ω’°’∂’Ø’°÷Ä’´ ’∞’≤’∏÷Ç’¥’®</span><span class="co">](https://unsplash.com/)</span>, ’Ä’•’≤’´’∂’°’Ø’ù <span class="co">[</span><span class="ot">Artist Name</span><span class="co">](https://unsplash.com/)</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="fu"># üìö ’Ü’µ’∏÷Ç’©’®</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">üìö ‘±’¥’¢’∏’≤’ª’°’Ø’°’∂ ’∂’µ’∏÷Ç’©’®</span><span class="co">](01_vectors_linear_algebra.qmd)</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">üì∫ ’è’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’®</span><span class="co">](https://youtu.be/vectors_lecture)</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">üéûÔ∏è ’ç’¨’°’µ’§’•÷Ä - Vectors</span><span class="co">](Lectures/L01_Vectors.pdf)</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">üéûÔ∏è ’ç’¨’°’µ’§’•÷Ä - Geometry</span><span class="co">](Lectures/L02_Geometry_of_Vectors__Matrices.pdf)</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">üõ†Ô∏èüì∫ ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ ’ø’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’®</span><span class="co">](https://youtu.be/vectors_practical)</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">üõ†Ô∏èüóÇÔ∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ PDF-’®</span><span class="co">](Homeworks/hw_01_vectors.pdf)</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>‘±’µ’Ω ’§’°’Ω’´’∂ ’Ø’Æ’°’∂’∏’©’°’∂’°’∂÷Ñ ’æ’•’Ø’ø’∏÷Ä’∂’•÷Ä’´ ÷á ’¥’°’ø÷Ä’´÷Å’∂’•÷Ä’´ ’∞’´’¥’∏÷Ç’∂÷Ñ’∂’•÷Ä’´’∂’ù</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>’é’•’Ø’ø’∏÷Ä’∂’•÷Ä (’£’∏÷Ä’Æ’∏’≤’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä, ’Ω’Ø’°’¨’µ’°÷Ä ’°÷Ä’ø’°’§÷Ä’µ’°’¨, ’∂’∏÷Ä’¥’°)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>’é’•’Ø’ø’∏÷Ä’∂’•÷Ä’´ ’•÷Ä’Ø÷Ä’°’π’°÷É’∏÷Ç’©’µ’∏÷Ç’∂ (’°’∂’Ø’µ’∏÷Ç’∂’∂’•÷Ä, ’∏÷Ç’≤’≤’°’∞’°’µ’°÷Å’∏÷Ç’©’µ’∏÷Ç’∂)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>’Ñ’°’ø÷Ä’´÷Å’∂’•÷Ä ÷á ’£’Æ’°’µ’´’∂ ÷É’∏’≠’°’Ø’•÷Ä’∫’∏÷Ç’¥’∂’•÷Ä</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>‘≥’Æ’°’µ’´’∂ ’∞’°’¥’°’Ø÷Å’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä ÷á ’ø’°÷Ä’°’Æ’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="fu"># üè° ’è’∂’°’µ’´’∂</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="false"}</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>‚ùó‚ùó‚ùó DON'T CHECK THE SOLUTIONS BEFORE TRYING TO DO THE HOMEWORK BY YOURSELF‚ùó‚ùó‚ùó</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Please don't hesitate to ask questions, never forget about the üçäkaralyoküçä principle!</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>The harder the problem is, the more üßÄcheesesüßÄ it has.</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Problems with üéÅ are just extra bonuses. It would be good to try to solve them, but also it's not the highest priority task.</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>If the problem involve many boring calculations, feel free to skip them - important part is understanding the concepts.</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>Submit your solutions <span class="co">[</span><span class="ot">here</span><span class="co">](https://forms.gle/CFEvNqFiTSsDLiFc6)</span> (even if it's unfinished)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="fu">## Vector Operations</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="fu">### 01: RGB color mixing with vectors {data-difficulty="1"}</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true" appearance="minimal"}</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Context</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>In computer graphics and image processing, colors are represented as RGB vectors where each component (Red, Green, Blue) ranges from 0 to 255. Vector operations on these RGB values correspond to color mixing and transformations.</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>Consider these RGB color vectors:</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Red: $\vec{r} = (255, 0, 0)$</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Yellow: $\vec{y} = (255, 255, 0)$</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A custom color: $\vec{c} = (128, 64, 192)$</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>a) Calculate what color you get by adding red and yellow: $\vec{r} + \vec{y}$. What happens when RGB values exceed 255?</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>b) Find the "average" color between red and yellow: $\frac{1}{2}(\vec{r} + \vec{y})$</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>c) Use a color picker (like <span class="co">[</span><span class="ot">Google's color picker</span><span class="co">](https://g.co/kgs/color-picker)</span> or any online tool) to verify your answers from parts (a) and (b). What colors do you actually see?</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>d) Calculate $\vec{r} - \frac{1}{2}\vec{y}$. What does this operation represent in terms of color mixing?</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a><span class="fu">### 02: Feature vector normalization {data-difficulty="2"}</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true" appearance="minimal"}</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Context</span></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>In ML preprocessing, we often normalize feature vectors to have unit length. This helps algorithms that are sensitive to the scale of input features, like k-nearest neighbors or neural networks.</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>A customer profile is represented by the vector $\vec{v} = (25, 50000, 3)$ where components represent <span class="co">[</span><span class="ot">age, income in $, number of purchases</span><span class="co">]</span>.</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>a) Calculate the Euclidean norm (magnitude) $||\vec{v}||_2$</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>b) Find the unit vector $\hat{v} = \frac{\vec{v}}{||\vec{v}||_2}$</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>c) Verify that $||\hat{v}||_2 = 1$</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a><span class="fu">### 03: Distance between data points {data-difficulty="2"}</span></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>Two data points in a dataset are represented as $\vec{p_1} = (1, 3, -2)$ and $\vec{p_2} = (4, -1, 1)$.</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>a) Calculate the Euclidean distance between these points</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>b) Calculate the Manhattan distance (L1 norm of the difference)</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>c) Which distance metric would be more robust to outliers? Explain briefly.</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a><span class="fu">## Dot Products and Angles</span></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a><span class="fu">### 04: Similarity measurement {data-difficulty="2"}</span></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>::: {.callout-important collapse="true" appearance="minimal"}</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Context</span></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>The dot product is fundamental in measuring similarity between vectors. In recommendation systems, we often use cosine similarity (based on dot products) to find similar users or items.</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>Two user preference vectors are $\vec{u_1} = (5, 3, 1, 4)$ and $\vec{u_2} = (3, 5, 2, 2)$ where each component represents rating for different movie genres.</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>a) Calculate the dot product $\vec{u_1} \cdot \vec{u_2}$</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>b) Calculate the cosine similarity: $\cos(\theta) = \frac{\vec{u_1} \cdot \vec{u_2}}{||\vec{u_1}|| \cdot ||\vec{u_2}||}$</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>c) What does a cosine similarity close to 1 indicate about user preferences?</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a><span class="fu">### 04.5: Word embeddings similarity {data-difficulty="2"}</span></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true" appearance="minimal"}</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Context</span></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>In NLP, words can be represented as vectors called embeddings. By comparing these vectors using different distance metrics, we can determine semantic similarity between words. This is the foundation of modern language models and search engines.</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>Given the following 2D word embeddings:</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**cheese:** $(1, 2)$</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**mushroom:** $(3, 1)$  </span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**tasty:** $(2, 2)$</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>a) **Euclidean Distance Analysis:**</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Compute the Euclidean distance between **tasty** and **cheese**</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Compute the Euclidean distance between **tasty** and **mushroom**</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Which word is closer to **tasty** based on Euclidean distance?</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>b) **Cosine Similarity Analysis:**</span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Compute the cosine similarity between **tasty** and **cheese**</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Compute the cosine similarity between **tasty** and **mushroom**</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Which word is closer to **tasty** based on cosine similarity?</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>c) **Discussion:** Compare the outcomes from parts (a) and (b). Why might one metric be preferred over the other in different NLP applications?</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true" appearance="minimal"}</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>**Fun fact:** Check out this <span class="co">[</span><span class="ot">3Blue1Brown video on word vectors</span><span class="co">](https://youtu.be/wjZofJX0v4M?t=751)</span> for more insights!</span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a><span class="fu">### 05: Matrix transformations {data-difficulty="2"}</span></span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>What vectors do you get by applying the matrix $A = \begin{pmatrix} 3 &amp; -3 <span class="sc">\\</span> 3 &amp; 3 \end{pmatrix}$ on the vectors:</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>a) $\vec{a} = \begin{pmatrix} 1 <span class="sc">\\</span> 0 \end{pmatrix}$</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>b) $\vec{b} = \begin{pmatrix} 0 <span class="sc">\\</span> 1 \end{pmatrix}$</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>c) $\vec{c} = \begin{pmatrix} 1 <span class="sc">\\</span> 1 \end{pmatrix}$</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>d) (Additional) Draw the vectors before and after multiplying with $A$. What can you say visually about the matrix? Can you guess how it will act on the vector $\begin{pmatrix} 2 <span class="sc">\\</span> -2 \end{pmatrix}$?</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a><span class="fu">### 06: Finding perpendicular vectors {data-difficulty="2"}</span></span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true" appearance="minimal"}</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Context</span></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>Finding perpendicular vectors is fundamental in many applications, from computer graphics (surface normals) to optimization (gradient descent directions) and data analysis (principal component analysis).</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a>Given the vector $\vec{v} = (2, 3)$:</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>a) Find a non-zero vector $\vec{w} = (x, y)$ such that $\vec{v}$ and $\vec{w}$ are perpendicular.</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>b) Verify that your chosen vector $\vec{w}$ satisfies $\vec{v} \cdot \vec{w} = 0$.</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a>c) Find a unit vector in the direction of $\vec{w}$ by computing $\frac{\vec{w}}{||\vec{w}||}$.</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>d) Explain why there are infinitely many vectors perpendicular to $\vec{v}$ and describe the general form of all such vectors.</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a><span class="fu">### 06.5: Orthogonality check {data-difficulty="1"}</span></span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>Determine which pairs of vectors are orthogonal (perpendicular):</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a>a) $\vec{a} = (1, 2, -1)$ and $\vec{b} = (2, -1, 0)$</span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>b) $\vec{c} = (3, 4)$ and $\vec{d} = (-4, 3)$</span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a>c) $\vec{e} = (1, 1, 1)$ and $\vec{f} = (1, -2, 1)$</span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a><span class="fu">### 06: Orthogonality check {data-difficulty="1"}</span></span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>Determine which pairs of vectors are orthogonal (perpendicular):</span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>a) $\vec{a} = (1, 2, -1)$ and $\vec{b} = (2, -1, 0)$</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>b) $\vec{c} = (3, 4)$ and $\vec{d} = (-4, 3)$</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>c) $\vec{e} = (1, 1, 1)$ and $\vec{f} = (1, -2, 1)$</span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a><span class="fu">### 07: Matrix products {data-difficulty="2"}</span></span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a>Compute the following products:</span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a>a) $AB$, where $A = \begin{pmatrix} 6 &amp; 5 <span class="sc">\\</span> -2 &amp; 7 \end{pmatrix}$, $B = \begin{pmatrix} -5 &amp; 3 <span class="sc">\\</span> 1 &amp; 4 \end{pmatrix}$</span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a>b) $(A - B)(A + B)$, where $A = \begin{pmatrix} 2 &amp; 2 &amp; 4 <span class="sc">\\</span> -3 &amp; -2 &amp; 4 <span class="sc">\\</span> -2 &amp; 0 &amp; 2 \end{pmatrix}$, $B = \begin{pmatrix} 2 &amp; 1 &amp; 3 <span class="sc">\\</span> -1 &amp; 2 &amp; 2 <span class="sc">\\</span> 1 &amp; 4 &amp; -1 \end{pmatrix}$</span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a>c) $A^2 - B^2$, with the same $A$ and $B$ as in part (b).</span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a><span class="fu">### 08: Deriving the cosine angle formula {data-difficulty="3"}</span></span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a>Derive the formula for the cosine of the angle between two vectors: $\cos(\theta) = \frac{\vec{a} \cdot \vec{b}}{||\vec{a}|| \cdot ||\vec{b}||}$</span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning collapse="true" appearance="minimal"}</span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Hint</span></span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a>Start with the law of cosines for a triangle: $c^2 = a^2 + b^2 - 2ab\cos(\theta)$. Consider a triangle formed by vectors $\vec{a}$, $\vec{b}$, and $\vec{a} - \vec{b}$. The side lengths are $||\vec{a}||$, $||\vec{b}||$, and $||\vec{a} - \vec{b}||$. Express $||\vec{a} - \vec{b}||^2$ using the dot product and substitute into the law of cosines.</span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>a) Write down the law of cosines for the triangle with sides $||\vec{a}||$, $||\vec{b}||$, and $||\vec{a} - \vec{b}||$</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a>b) Express $||\vec{a} - \vec{b}||^2$ in terms of dot products by expanding $(\vec{a} - \vec{b}) \cdot (\vec{a} - \vec{b})$</span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>c) Substitute your result from part (b) into the law of cosines and solve for $\cos(\theta)$</span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>d) Verify your derived formula using vectors $\vec{u} = (3, 4)$ and $\vec{v} = (1, 0)$</span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a><span class="fu">### 08: Deriving the cosine angle formula {data-difficulty="3"}</span></span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a>Derive the formula for the cosine of the angle between two vectors: $\cos(\theta) = \frac{\vec{a} \cdot \vec{b}}{||\vec{a}|| \cdot ||\vec{b}||}$</span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning collapse="true" appearance="minimal"}</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Hint</span></span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a>Start with the law of cosines for a triangle: $c^2 = a^2 + b^2 - 2ab\cos(\theta)$. Consider a triangle formed by vectors $\vec{a}$, $\vec{b}$, and $\vec{a} - \vec{b}$. The side lengths are $||\vec{a}||$, $||\vec{b}||$, and $||\vec{a} - \vec{b}||$. Express $||\vec{a} - \vec{b}||^2$ using the dot product and substitute into the law of cosines.</span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a>a) Write down the law of cosines for the triangle with sides $||\vec{a}||$, $||\vec{b}||$, and $||\vec{a} - \vec{b}||$</span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a>b) Express $||\vec{a} - \vec{b}||^2$ in terms of dot products by expanding $(\vec{a} - \vec{b}) \cdot (\vec{a} - \vec{b})$</span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a>c) Substitute your result from part (b) into the law of cosines and solve for $\cos(\theta)$</span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a>d) Verify your derived formula using vectors $\vec{u} = (3, 4)$ and $\vec{v} = (1, 0)$</span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a><span class="fu">### 09: Shear matrix transformations {data-difficulty="2"}</span></span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a>Consider the following matrix (it is called the shear matrix): $S = \begin{pmatrix} 1 &amp; 1 <span class="sc">\\</span> 0 &amp; 1 \end{pmatrix}$</span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a>a) What would you get if you apply $S$ on the vector $\begin{pmatrix} 0 <span class="sc">\\</span> 1 \end{pmatrix}$?</span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a>b) What would you get if you apply $S$ again on the result of the previous point?</span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a>c) What if you apply $S$ one more time?</span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a>d) What do you think happens when we apply $S$ 100 times on that vector?</span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a>e) Can you compute $S^{100}$?</span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a><span class="fu">### 10: Projection and components {data-difficulty="3"}</span></span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a>Given vectors $\vec{a} = (4, 3)$ and $\vec{b} = (1, 2)$:</span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a>a) Find the projection of $\vec{a}$ onto $\vec{b}$: $\text{proj}_{\vec{b}}\vec{a} = \frac{\vec{a} \cdot \vec{b}}{||\vec{b}||^2}\vec{b}$</span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>b) Find the component of $\vec{a}$ perpendicular to $\vec{b}$</span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a>c) Verify that $\vec{a} = \text{proj}_{\vec{b}}\vec{a} + \vec{a}_{\perp}$</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a><span class="fu">## Geometric Interpretation</span></span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a><span class="fu">### 08: Triangle inequality {data-difficulty="2"}</span></span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a>For vectors $\vec{u} = (3, 4)$ and $\vec{v} = (5, -12)$:</span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a>a) Calculate $||\vec{u}||$, $||\vec{v}||$, and $||\vec{u} + \vec{v}||$</span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a>b) Verify the triangle inequality: $||\vec{u} + \vec{v}|| \leq ||\vec{u}|| + ||\vec{v}||$</span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a>c) When does equality hold in the triangle inequality?</span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a><span class="fu">### 09: Cross product applications {data-difficulty="3"}</span></span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true" appearance="minimal"}</span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Context</span></span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a>Cross products are used in computer graphics for calculating surface normals, determining the orientation of objects, and computing areas of parallelograms.</span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a>Given vectors $\vec{a} = (2, 1, -1)$ and $\vec{b} = (1, 3, 2)$:</span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a>a) Calculate the cross product $\vec{a} \times \vec{b}$</span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a>b) Verify that $\vec{a} \times \vec{b}$ is orthogonal to both $\vec{a}$ and $\vec{b}$</span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a>c) Find the area of the parallelogram spanned by $\vec{a}$ and $\vec{b}$</span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a><span class="fu">## Linear Combinations and Spans</span></span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a><span class="fu">### 10: Linear combinations {data-difficulty="2"}</span></span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a>Given vectors $\vec{v_1} = (1, 2, 1)$, $\vec{v_2} = (2, -1, 3)$, and $\vec{v_3} = (1, -4, 1)$:</span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a>a) Express $\vec{w} = (5, 0, 7)$ as a linear combination of $\vec{v_1}$ and $\vec{v_2}$ if possible</span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a>b) Can $\vec{v_3}$ be written as a linear combination of $\vec{v_1}$ and $\vec{v_2}$?</span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a>c) What does it mean geometrically if three vectors are linearly dependent?</span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a><span class="fu">### 11: Basis vectors {data-difficulty="2"}</span></span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a>Consider the vectors $\vec{e_1} = (1, 0, 0)$, $\vec{e_2} = (0, 1, 0)$, and $\vec{e_3} = (0, 0, 1)$:</span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a>a) Express the vector $\vec{v} = (7, -3, 5)$ in terms of $\vec{e_1}$, $\vec{e_2}$, and $\vec{e_3}$</span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a>b) Why are $\vec{e_1}$, $\vec{e_2}$, and $\vec{e_3}$ called the standard basis vectors for $\mathbb{R}^3$?</span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a>c) Can any vector in $\mathbb{R}^3$ be expressed uniquely as a linear combination of these basis vectors?</span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a><span class="fu">### 12: Span and linear independence {data-difficulty="3"}</span></span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a>Determine whether the following sets of vectors span $\mathbb{R}^3$ and whether they are linearly independent:</span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a>a) $<span class="sc">\{</span>(1, 0, 1), (0, 1, 1), (1, 1, 0)<span class="sc">\}</span>$</span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a>b) $<span class="sc">\{</span>(1, 2, 3), (2, 4, 6), (0, 1, 2)<span class="sc">\}</span>$</span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a>c) $<span class="sc">\{</span>(1, 0, 0), (1, 1, 0), (1, 1, 1), (0, 1, 1)<span class="sc">\}</span>$</span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a><span class="fu">### 13: Principal Component Analysis (PCA) intuition {data-difficulty="3"}</span></span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a>::: {.callout-important collapse="true" appearance="minimal"}</span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Context</span></span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a>PCA is a dimensionality reduction technique that finds the directions (principal components) along which data varies the most. These directions are eigenvectors of the covariance matrix.</span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a>Given a 2D dataset with points: $(1, 1)$, $(2, 2)$, $(3, 3)$, $(1, 3)$, $(3, 1)$:</span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a>a) Calculate the mean vector $\vec{\mu}$ of the dataset</span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a>b) Center the data by subtracting the mean from each point</span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a>c) Which direction would you expect the first principal component to point? Explain intuitively.</span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a><span class="fu">### 14: Vector in machine learning cost functions {data-difficulty="3"}</span></span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true" appearance="minimal"}</span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Context</span></span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a>In linear regression, we minimize the sum of squared errors. This can be expressed elegantly using vector notation, where the error vector's magnitude represents the total cost.</span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a>In a simple linear regression with predictions $\vec{y}_{pred} = (2.1, 4.8, 6.9, 9.2)$ and actual values $\vec{y}_{true} = (2, 5, 7, 9)$:</span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a>a) Calculate the error vector $\vec{e} = \vec{y}_{true} - \vec{y}_{pred}$</span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a>b) Calculate the mean squared error: $MSE = \frac{1}{n}||\vec{e}||^2$</span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a>c) How does minimizing $||\vec{e}||^2$ relate to finding the best-fit line?</span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a><span class="fu">### 15: Model selection with regularization {data-difficulty="3"}</span></span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a>::: {.callout-important collapse="true" appearance="minimal"}</span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Context</span></span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a>In machine learning, complex models with large weights can "memorize" the training data instead of learning generalizable patterns. This leads to overfitting - the model performs well on training data but poorly on new data. Regularization helps by penalizing large weight values, encouraging simpler models that generalize better. L1 regularization (Lasso) uses the Manhattan norm and can drive some weights to zero, while L2 regularization (Ridge) uses the Euclidean norm and keeps all weights small. The total loss = prediction error + Œª √ó regularization penalty.</span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a>Two models have learned different weight vectors:</span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Model A: $\vec{w_A} = (2.5, -1.8, 0.3, 4.1)$</span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Model B: $\vec{w_B} = (1.2, -0.9, 0.7, 2.3)$</span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a>Both models achieve the same prediction error of $E = 12.5$ on the validation set.</span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a>a) Calculate the L1 regularization term for each model: $R_{L1} = ||\vec{w}||_1 = \sum_i |w_i|$</span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a>b) Calculate the L2 regularization term for each model: $R_{L2} = ||\vec{w}||_2^2 = \sum_i w_i^2$</span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a>c) For regularization parameter $\lambda = 0.1$, calculate the total regularized loss for each model using both L1 and L2 regularization:</span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$Loss_{L1} = E + \lambda \cdot R_{L1}$  </span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$Loss_{L2} = E + \lambda \cdot R_{L2}$</span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a>d) Which model would you choose under L1 regularization? Which under L2 regularization? Explain the difference.</span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a><span class="fu">### 16: Vector spaces {data-difficulty="2"}</span></span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true" appearance="minimal"}</span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Context</span></span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a>A vector space is a set of vectors that is closed under vector addition and scalar multiplication, and satisfies certain axioms. Understanding vector spaces is crucial for linear algebra and provides the foundation for many ML concepts like feature spaces and function spaces.</span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-335"><a href="#cb1-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-336"><a href="#cb1-336" aria-hidden="true" tabindex="-1"></a>Consider the set $S = <span class="sc">\{</span>(a, 2a, 3a) : a \in \mathbb{R}<span class="sc">\}</span>$ (all vectors of the form $(a, 2a, 3a)$ where $a$ is any real number).</span>
<span id="cb1-337"><a href="#cb1-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-338"><a href="#cb1-338" aria-hidden="true" tabindex="-1"></a>a) Show that $S$ is closed under vector addition by taking two arbitrary vectors from $S$ and showing their sum is also in $S$.</span>
<span id="cb1-339"><a href="#cb1-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-340"><a href="#cb1-340" aria-hidden="true" tabindex="-1"></a>b) Show that $S$ is closed under scalar multiplication by taking an arbitrary vector from $S$ and an arbitrary scalar.</span>
<span id="cb1-341"><a href="#cb1-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-342"><a href="#cb1-342" aria-hidden="true" tabindex="-1"></a>c) Does $S$ form a vector space? What about the zero vector requirement?</span>
<span id="cb1-343"><a href="#cb1-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-344"><a href="#cb1-344" aria-hidden="true" tabindex="-1"></a>d) Give a geometric interpretation of what the set $S$ represents in $\mathbb{R}^3$.</span>
<span id="cb1-345"><a href="#cb1-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-346"><a href="#cb1-346" aria-hidden="true" tabindex="-1"></a><span class="fu">### 17: Vector subspaces {data-difficulty="3"}</span></span>
<span id="cb1-347"><a href="#cb1-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-348"><a href="#cb1-348" aria-hidden="true" tabindex="-1"></a>::: {.callout-important collapse="true" appearance="minimal"}</span>
<span id="cb1-349"><a href="#cb1-349" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Context</span></span>
<span id="cb1-350"><a href="#cb1-350" aria-hidden="true" tabindex="-1"></a>A vector subspace is a subset of a vector space that is itself a vector space. In machine learning, feature subspaces are often used for dimensionality reduction, and understanding subspaces helps with concepts like the null space and column space of matrices.</span>
<span id="cb1-351"><a href="#cb1-351" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-352"><a href="#cb1-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-353"><a href="#cb1-353" aria-hidden="true" tabindex="-1"></a>Determine which of the following sets are vector subspaces of $\mathbb{R}^3$:</span>
<span id="cb1-354"><a href="#cb1-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-355"><a href="#cb1-355" aria-hidden="true" tabindex="-1"></a>a) $V_1 = <span class="sc">\{</span>(x, y, z) : x + y + z = 0<span class="sc">\}</span>$ (vectors whose components sum to zero)</span>
<span id="cb1-356"><a href="#cb1-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-357"><a href="#cb1-357" aria-hidden="true" tabindex="-1"></a>b) $V_2 = <span class="sc">\{</span>(x, y, z) : x + y + z = 1<span class="sc">\}</span>$ (vectors whose components sum to one)</span>
<span id="cb1-358"><a href="#cb1-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-359"><a href="#cb1-359" aria-hidden="true" tabindex="-1"></a>c) $V_3 = <span class="sc">\{</span>(x, 0, z) : x, z \in \mathbb{R}<span class="sc">\}</span>$ (vectors with zero as the middle component)</span>
<span id="cb1-360"><a href="#cb1-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-361"><a href="#cb1-361" aria-hidden="true" tabindex="-1"></a>d) $V_4 = <span class="sc">\{</span>(x, y, z) : x^2 + y^2 + z^2 \leq 1<span class="sc">\}</span>$ (vectors inside or on the unit sphere)</span>
<span id="cb1-362"><a href="#cb1-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-363"><a href="#cb1-363" aria-hidden="true" tabindex="-1"></a>For each set, check the three subspace requirements: (1) contains zero vector, (2) closed under addition, (3) closed under scalar multiplication.</span>
<span id="cb1-364"><a href="#cb1-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-365"><a href="#cb1-365" aria-hidden="true" tabindex="-1"></a><span class="fu">### 18: High-dimensional vector geometry {.bonus-problem data-difficulty="3"}</span></span>
<span id="cb1-366"><a href="#cb1-366" aria-hidden="true" tabindex="-1"></a>In high-dimensional spaces (common in ML), our intuition about geometry can be misleading.</span>
<span id="cb1-367"><a href="#cb1-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-368"><a href="#cb1-368" aria-hidden="true" tabindex="-1"></a>Consider the unit sphere in $\mathbb{R}^n$ (all vectors with norm 1):</span>
<span id="cb1-369"><a href="#cb1-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-370"><a href="#cb1-370" aria-hidden="true" tabindex="-1"></a>a) In 2D, what fraction of a unit square $<span class="co">[</span><span class="ot">-1,1</span><span class="co">]</span> \times <span class="co">[</span><span class="ot">-1,1</span><span class="co">]</span>$ is occupied by the unit circle?</span>
<span id="cb1-371"><a href="#cb1-371" aria-hidden="true" tabindex="-1"></a>b) Estimate this fraction for a unit cube in 3D</span>
<span id="cb1-372"><a href="#cb1-372" aria-hidden="true" tabindex="-1"></a>c) Research: What happens to this fraction as the dimension $n$ increases? This is known as the "curse of dimensionality."</span>
<span id="cb1-373"><a href="#cb1-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-374"><a href="#cb1-374" aria-hidden="true" tabindex="-1"></a><span class="fu">## Matrix Transformations</span></span>
<span id="cb1-375"><a href="#cb1-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-376"><a href="#cb1-376" aria-hidden="true" tabindex="-1"></a><span class="fu">### 19: Matrix transformations and geometric interpretation {data-difficulty="2"}</span></span>
<span id="cb1-377"><a href="#cb1-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-378"><a href="#cb1-378" aria-hidden="true" tabindex="-1"></a>What vectors do you get by applying the matrix $A = \begin{pmatrix} 3 &amp; -3 <span class="sc">\\</span> 3 &amp; 3 \end{pmatrix}$ on the vectors:</span>
<span id="cb1-379"><a href="#cb1-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-380"><a href="#cb1-380" aria-hidden="true" tabindex="-1"></a>a) $\vec{a} = \begin{pmatrix} 1 <span class="sc">\\</span> 0 \end{pmatrix}$</span>
<span id="cb1-381"><a href="#cb1-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-382"><a href="#cb1-382" aria-hidden="true" tabindex="-1"></a>b) $\vec{b} = \begin{pmatrix} 0 <span class="sc">\\</span> 1 \end{pmatrix}$</span>
<span id="cb1-383"><a href="#cb1-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-384"><a href="#cb1-384" aria-hidden="true" tabindex="-1"></a>c) $\vec{c} = \begin{pmatrix} 1 <span class="sc">\\</span> 1 \end{pmatrix}$</span>
<span id="cb1-385"><a href="#cb1-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-386"><a href="#cb1-386" aria-hidden="true" tabindex="-1"></a>**Additional:** Draw the vectors before and after multiplying with $A$. What can you say visually about the matrix? Can you guess how it will act on the vector $\begin{pmatrix} 2 <span class="sc">\\</span> -2 \end{pmatrix}$?</span>
<span id="cb1-387"><a href="#cb1-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-388"><a href="#cb1-388" aria-hidden="true" tabindex="-1"></a><span class="fu">### 20: Matrix operations {data-difficulty="2"}</span></span>
<span id="cb1-389"><a href="#cb1-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-390"><a href="#cb1-390" aria-hidden="true" tabindex="-1"></a>Compute the following products:</span>
<span id="cb1-391"><a href="#cb1-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-392"><a href="#cb1-392" aria-hidden="true" tabindex="-1"></a>a) $AB$, where $A = \begin{pmatrix} 6 &amp; 5 <span class="sc">\\</span> -2 &amp; 7 \end{pmatrix}$, $B = \begin{pmatrix} -5 &amp; 3 <span class="sc">\\</span> 1 &amp; 4 \end{pmatrix}$</span>
<span id="cb1-393"><a href="#cb1-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-394"><a href="#cb1-394" aria-hidden="true" tabindex="-1"></a>b) $(A - B)(A + B)$, where $A = \begin{pmatrix} 2 &amp; 2 &amp; 4 <span class="sc">\\</span> -3 &amp; -2 &amp; 4 <span class="sc">\\</span> -2 &amp; 0 &amp; 2 \end{pmatrix}$, $B = \begin{pmatrix} 2 &amp; 1 &amp; 3 <span class="sc">\\</span> -1 &amp; 2 &amp; 2 <span class="sc">\\</span> 1 &amp; 4 &amp; -1 \end{pmatrix}$</span>
<span id="cb1-395"><a href="#cb1-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-396"><a href="#cb1-396" aria-hidden="true" tabindex="-1"></a>c) $A^2 - B^2$, with the same $A$ and $B$ as in part (b).</span>
<span id="cb1-397"><a href="#cb1-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-398"><a href="#cb1-398" aria-hidden="true" tabindex="-1"></a><span class="fu">### 21: Shear matrix analysis {.bonus-problem data-difficulty="3"}</span></span>
<span id="cb1-399"><a href="#cb1-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-400"><a href="#cb1-400" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true" appearance="minimal"}</span>
<span id="cb1-401"><a href="#cb1-401" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Context</span></span>
<span id="cb1-402"><a href="#cb1-402" aria-hidden="true" tabindex="-1"></a>Shear transformations are commonly used in computer graphics for creating italic text effects, perspective corrections, and geometric distortions. They preserve area but change angles and shapes.</span>
<span id="cb1-403"><a href="#cb1-403" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-404"><a href="#cb1-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-405"><a href="#cb1-405" aria-hidden="true" tabindex="-1"></a>Consider the shear matrix $S = \begin{pmatrix} 1 &amp; 1 <span class="sc">\\</span> 0 &amp; 1 \end{pmatrix}$:</span>
<span id="cb1-406"><a href="#cb1-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-407"><a href="#cb1-407" aria-hidden="true" tabindex="-1"></a>a) What would you get if you apply $S$ on the vector $\begin{pmatrix} 0 <span class="sc">\\</span> 1 \end{pmatrix}$?</span>
<span id="cb1-408"><a href="#cb1-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-409"><a href="#cb1-409" aria-hidden="true" tabindex="-1"></a>b) What would you get if you apply $S$ again on the result of the previous point?</span>
<span id="cb1-410"><a href="#cb1-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-411"><a href="#cb1-411" aria-hidden="true" tabindex="-1"></a>c) What if you apply $S$ one more time?</span>
<span id="cb1-412"><a href="#cb1-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-413"><a href="#cb1-413" aria-hidden="true" tabindex="-1"></a>d) What do you think happens when we apply $S$ 100 times on that vector?</span>
<span id="cb1-414"><a href="#cb1-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-415"><a href="#cb1-415" aria-hidden="true" tabindex="-1"></a>e) Can you compute $S^{100}$?</span>
<span id="cb1-416"><a href="#cb1-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-417"><a href="#cb1-417" aria-hidden="true" tabindex="-1"></a><span class="fu">### 21.5: Diagonal matrix powers {data-difficulty="2"}</span></span>
<span id="cb1-418"><a href="#cb1-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-419"><a href="#cb1-419" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true" appearance="minimal"}</span>
<span id="cb1-420"><a href="#cb1-420" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Context</span></span>
<span id="cb1-421"><a href="#cb1-421" aria-hidden="true" tabindex="-1"></a>Diagonal matrices are particularly useful in linear algebra because their powers are easy to compute. This property is extensively used in eigenvalue decomposition and diagonalization of matrices.</span>
<span id="cb1-422"><a href="#cb1-422" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-423"><a href="#cb1-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-424"><a href="#cb1-424" aria-hidden="true" tabindex="-1"></a>Consider the diagonal matrix $A = \begin{pmatrix} 2 &amp; 0 <span class="sc">\\</span> 0 &amp; -1 \end{pmatrix}$.</span>
<span id="cb1-425"><a href="#cb1-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-426"><a href="#cb1-426" aria-hidden="true" tabindex="-1"></a>a) Compute $A^2$, $A^3$, and $A^4$.</span>
<span id="cb1-427"><a href="#cb1-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-428"><a href="#cb1-428" aria-hidden="true" tabindex="-1"></a>b) Find a general formula for $A^n$ where $n$ is any positive integer.</span>
<span id="cb1-429"><a href="#cb1-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-430"><a href="#cb1-430" aria-hidden="true" tabindex="-1"></a>c) What does this transformation represent geometrically? How does it affect the unit circle when applied repeatedly?</span>
<span id="cb1-431"><a href="#cb1-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-432"><a href="#cb1-432" aria-hidden="true" tabindex="-1"></a>d) **Bonus:** What happens when you apply this transformation to the vector $\begin{pmatrix} 1 <span class="sc">\\</span> 1 \end{pmatrix}$ multiple times?</span>
<span id="cb1-433"><a href="#cb1-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-434"><a href="#cb1-434" aria-hidden="true" tabindex="-1"></a><span class="fu">## Vector Spaces and Subspaces</span></span>
<span id="cb1-435"><a href="#cb1-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-436"><a href="#cb1-436" aria-hidden="true" tabindex="-1"></a><span class="fu">### 22: Identifying vector spaces and non-vector spaces {data-difficulty="3"}</span></span>
<span id="cb1-437"><a href="#cb1-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-438"><a href="#cb1-438" aria-hidden="true" tabindex="-1"></a>::: {.callout-important collapse="true" appearance="minimal"}</span>
<span id="cb1-439"><a href="#cb1-439" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Context</span></span>
<span id="cb1-440"><a href="#cb1-440" aria-hidden="true" tabindex="-1"></a>Understanding what constitutes a vector space is fundamental in linear algebra. A set must satisfy specific axioms to be considered a vector space: closure under addition and scalar multiplication, existence of zero element, existence of additive inverses, and several other properties.</span>
<span id="cb1-441"><a href="#cb1-441" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-442"><a href="#cb1-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-443"><a href="#cb1-443" aria-hidden="true" tabindex="-1"></a>For each of the following sets, determine whether it is a vector space or not. If it is a vector space, prove it by verifying all the required axioms. If it is not a vector space, identify which axiom(s) fail and provide counterexamples.</span>
<span id="cb1-444"><a href="#cb1-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-445"><a href="#cb1-445" aria-hidden="true" tabindex="-1"></a>a) $A = \left<span class="sc">\{</span>\begin{pmatrix} a <span class="sc">\\</span> 0 \end{pmatrix} \mid a \in \mathbb{R}\right<span class="sc">\}</span>$ (vectors with second component zero)</span>
<span id="cb1-446"><a href="#cb1-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-447"><a href="#cb1-447" aria-hidden="true" tabindex="-1"></a>b) $B = \left<span class="sc">\{</span>\begin{pmatrix} a <span class="sc">\\</span> -a \end{pmatrix} \mid a \in \mathbb{R}\right<span class="sc">\}</span>$ (vectors where second component is negative of first)</span>
<span id="cb1-448"><a href="#cb1-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-449"><a href="#cb1-449" aria-hidden="true" tabindex="-1"></a>c) $C = \mathbb{N}$ (the set of natural numbers)</span>
<span id="cb1-450"><a href="#cb1-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-451"><a href="#cb1-451" aria-hidden="true" tabindex="-1"></a>d) $D = \left<span class="sc">\{</span>\begin{pmatrix} a <span class="sc">\\</span> 1 \end{pmatrix} \mid a \in \mathbb{R}\right<span class="sc">\}</span>$ (vectors with second component always 1)</span>
<span id="cb1-452"><a href="#cb1-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-453"><a href="#cb1-453" aria-hidden="true" tabindex="-1"></a>**Hint:** For the non-vector spaces, show that there are some "bad" elements such that if we add them or multiply with some number (not necessarily positive), the result would not belong to the set.</span>
<span id="cb1-454"><a href="#cb1-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-455"><a href="#cb1-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-456"><a href="#cb1-456" aria-hidden="true" tabindex="-1"></a><span class="fu"># üõ†Ô∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂</span></span>
<span id="cb1-457"><a href="#cb1-457" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">üõ†Ô∏èüì∫ ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ ’ø’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’®</span><span class="co">](https://youtu.be/vectors_practical)</span></span>
<span id="cb1-458"><a href="#cb1-458" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">üõ†Ô∏èüóÇÔ∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ PDF-’®</span><span class="co">](Homeworks/hw_01_vectors_practical.pdf)</span></span>
<span id="cb1-459"><a href="#cb1-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-460"><a href="#cb1-460" aria-hidden="true" tabindex="-1"></a>Practice problems include:</span>
<span id="cb1-461"><a href="#cb1-461" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Implementing vector operations in Python/NumPy</span>
<span id="cb1-462"><a href="#cb1-462" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Visualizing vectors and transformations</span>
<span id="cb1-463"><a href="#cb1-463" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Computing similarity matrices for recommendation systems</span>
<span id="cb1-464"><a href="#cb1-464" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>PCA implementation on real datasets</span>
<span id="cb1-465"><a href="#cb1-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-466"><a href="#cb1-466" aria-hidden="true" tabindex="-1"></a><span class="fu"># üé≤ 38 (01)</span></span>
<span id="cb1-467"><a href="#cb1-467" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>‚ñ∂Ô∏è<span class="co">[</span><span class="ot">3Blue1Brown - Essence of Linear Algebra</span><span class="co">](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)</span></span>
<span id="cb1-468"><a href="#cb1-468" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>üîó<span class="co">[</span><span class="ot">Linear Algebra Visualizations</span><span class="co">](https://www.3blue1brown.com/topics/linear-algebra)</span></span>
<span id="cb1-469"><a href="#cb1-469" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>üá¶üá≤üé∂<span class="co">[</span><span class="ot">’ç’∫’´’ø’°’Ø ‘æ’°’º’•÷Ä (’Ä’°’æ’•’ø)</span><span class="co">](https://www.youtube.com/watch?v=white_trees_forever)</span></span>
<span id="cb1-470"><a href="#cb1-470" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>üåêüé∂<span class="co">[</span><span class="ot">Radiohead (Everything In Its Right Place)</span><span class="co">](https://www.youtube.com/watch?v=onRk0sjSgFU)</span></span>
<span id="cb1-471"><a href="#cb1-471" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>ü§å<span class="co">[</span><span class="ot">’é’•’Ø’ø’∏÷Ä’°’Ø’°’∂ ’∫’°÷Ä’ø’•’¶</span><span class="co">](https://www.youtube.com/watch?v=vector_garden)</span></span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/HaykTarkhanyan/python_math_ml_course/edit/main/math/01_linear_algebra_vectors.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div></div></footer></body></html>