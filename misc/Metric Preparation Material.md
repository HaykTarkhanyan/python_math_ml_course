# Preparation materials

## Free online courses by Metric in Armenian

- ğŸ“½ï¸ Courses on Python, Math, ML \[[url](https://hayktarkhanyan.github.io/python_math_ml_course/)\]


## Video based resources
In general, anything by [3Blue1Brown](https://www.youtube.com/@3blue1brown), [Andrej Karpathy](https://www.youtube.com/@AndrejKarpathy), and [Welch Labs](https://www.youtube.com/@WelchLabsVideo) is fantastic â€” highly recommended for building deep intuition.


## Machine Learning

- ğŸ“˜ Dive into Deep Learning book \[[url](https://d2l.ai/)\]
- ğŸ“˜ Quite hard book by Bishop \[[url](https://g.co/kgs/1p69Lur)\]
- ğŸ“½ï¸ Relatively easy ML specialization by deeplearning.ai \[[url](https://www.coursera.org/specializations/machine-learning-introduction)\]
- ğŸ“½ï¸ A number of courses by deeplearning.ai \[[url](https://www.deeplearning.ai/)\]
- ğŸ“½ï¸ Animated videos on Neural Networks by 3Blue1Brown \[[url](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\]

## Generative AI / Deep Learning for NLP

- ğŸ“š Hugging Face Learn â€” free courses on NLP, LLMs, Transformers, and more \[[url](https://huggingface.co/learn/)\]

#### Transformers & Attention

- ğŸ“„ "The Illustrated Transformer" by Jay Alammar \[[url](https://jalammar.github.io/illustrated-transformer/)\]
- ğŸ“„ "Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal Self-Attention in LLMs" by Sebastian Raschka \[[url](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention)\]
- ğŸ“½ï¸ "Attention in transformers, step-by-step" by 3Blue1Brown \[[url](https://www.youtube.com/watch?v=eMlx5fFNoYc)\]
- ğŸ“„ "Transformer Architecture: The Positional Encoding" by Amirhossein Kazemnejad \[[url](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)\]

#### Notable Models (BERT, GPT, T5)

- ğŸ“„ "The Illustrated BERT, ELMo, and co." by Jay Alammar \[[url](https://jalammar.github.io/illustrated-bert/)\]
- ğŸ“„ "The Illustrated GPT-2" by Jay Alammar \[[url](https://jalammar.github.io/illustrated-gpt2/)\]
- ğŸ“„ "Exploring Transfer Learning with T5" by Google Research \[[url](https://research.google/blog/exploring-transfer-learning-with-t5-the-text-to-text-transfer-transformer/)\]
- ğŸ“½ï¸ "Let's build GPT: from scratch, in code, spelled out" by Andrej Karpathy \[[url](https://www.youtube.com/watch?v=kCc8FmEb1nY)\]
- ğŸ“½ï¸ "Deep Dive into LLMs like ChatGPT" by Andrej Karpathy \[[url](https://www.youtube.com/watch?v=7xTGNNLPyMI)\]

#### Tokenization & Embeddings

- ğŸ“„ "The Illustrated Word2vec" by Jay Alammar \[[url](https://jalammar.github.io/illustrated-word2vec/)\]
- ğŸ“„ "Byte-Pair Encoding tokenization" â€” Hugging Face NLP Course, Chapter 6 \[[url](https://huggingface.co/learn/llm-course/en/chapter6/5)\]
- ğŸ“½ï¸ "Let's build the GPT Tokenizer" by Andrej Karpathy \[[url](https://www.youtube.com/watch?v=zduSFxRajkE)\]
- ğŸ“½ï¸ "Word Embedding and Word2Vec, Clearly Explained!!!" by StatQuest \[[url](https://youtu.be/viZrOnJclY0)\]

#### Fine-tuning & RLHF

- ğŸ“„ "Finetuning Large Language Models" by Sebastian Raschka \[[url](https://magazine.sebastianraschka.com/p/finetuning-large-language-models)\]
- ğŸ“„ "Illustrating Reinforcement Learning from Human Feedback (RLHF)" â€” Hugging Face Blog \[[url](https://huggingface.co/blog/rlhf)\]
- ğŸ“„ "Fine-tuning a model with the Trainer API" â€” Hugging Face Course, Chapter 3 \[[url](https://huggingface.co/learn/llm-course/en/chapter3/3)\]

#### Decoding Strategies

- ğŸ“„ "How to generate text: using different decoding methods" â€” Hugging Face Blog \[[url](https://huggingface.co/blog/how-to-generate)\]
- ğŸ“„ "Decoding Strategies in Large Language Models" by Maxime Labonne \[[url](https://huggingface.co/blog/mlabonne/decoding-strategies)\]

## Mathematics

ğŸ“š A good book about Mathematics for Machine Learning by Mark Peter et al \[[url](https://mml-book.github.io/)\]

#### Linear Algebra

- ğŸ“½ï¸ Animated videos providing visual intuition for the essential topics by 3blue1brown \[[url](https://www.3blue1brown.com/topics/linear-algebra)\]
- ğŸ“½ï¸ Video lectures on Algebra by Technion University \[[url](https://www.youtube.com/playlist?list=PLW3u28VuDAHJNrf3JCgT0GG_rjFVz0-j9)\]
- ğŸ“½ï¸ Video lectures on Algebra by MIT University (Gilbert Strang) \[[url](https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/)\]
- ğŸ“½ï¸ Short, a bit more theoretical lecture series by TBSOM \[[url](https://www.youtube.com/playlist?list=PLBh2i93oe2quLc5zaxD0WHzQTGrXMwAI6)\]
- ğŸ“˜ Relatively easy textbook by David Poole \[[url](https://vk.com/wall-117872446_2759)\]
- ğŸ“˜ A more theoretical textbook by Sheldon Axler \[[url](https://linear.axler.net/)\]

#### Calculus

- ğŸ“½ï¸ Animated videos providing visual intuition for the essential topics by 3blue1brown \[[url](https://www.3blue1brown.com/topics/calculus)\]
- ğŸ“½ï¸ Videos on Multivariate Calculus by Khan Academy \[[url](https://www.youtube.com/playlist?list=PLSQl0a2vh4HC5feHa6Rc5c0wbRTx56nF7)\]
- ğŸ“– Lecture notes by Paul Dawkins from Lamar University \[[url](http://tutorial.math.lamar.edu)\]
- ğŸ“˜ Theoretical textbook in Armenian by V. Musoyan \[[url](http://publishing.ysu.am/hy/1522314393)\]
- ğŸ“™ Problem book in Armenian by G. Gevorgyan et al \[[url](http://publishing.ysu.am/hy/1425385266)\]

#### Probability theory

- ğŸ“½ï¸ Video lectures by J. Tsitsiklis from MIT \[[url](https://www.youtube.com/playlist?list=PLUl4u3cNGP60hI9ATjSFgLZpbNJ7myAg6)\]
- ğŸ“½ï¸ Short, a bit more theoretical lecture series by TBSOM \[[url](https://www.youtube.com/playlist?list=PLBh2i93oe2qswFOC98oSFc37-0f4S3D4z)\]
- ğŸ“™ Problem book in Armenian by N. Aharonyan et al \[[url](http://publishing.ysu.am/hy/1467369665)\]

#### Statistics

- ğŸ“½ï¸ Video series by Crash Course \[[url](https://www.youtube.com/playlist?list=PL8dPuuaLjXtNM_Y-bUAhblSAdWRnmBUcr)\]
- ğŸ“½ï¸ Video series by Organic Chemistry tutor \[[url](https://www.youtube.com/playlist?list=PL0o_zxa4K1BVsziIRdfv4Hl4UIqDZhXWV)\]
- ğŸ“½ï¸ Lectures on Mathematical Statistics by PhysTech (in Russian) \[[url](https://www.youtube.com/watch?v=aJokwg6c2KQ&list=PLthfp5exSWErTVWq4cVtRXDw5MqBqavJ1)\]

## Python

- ğŸ“– Intro to Python course by Profound Academy \[[url](https://profound.academy/python-introduction)\]
- ğŸ“– Intermediate Python course by Profound Academy \[[url](https://profound.academy/hy/python-mid)\]
- ğŸ“– Pandas, NumPy, Matplotlib/Plotly courses
  - [https://www.kaggle.com/learn](https://www.kaggle.com/learn)
  - [https://www.codecademy.com/catalog/language/python](https://www.codecademy.com/catalog/language/python)
- ğŸ“½ï¸ Good YouTube channels
  - [https://www.youtube.com/@coreyms](https://www.youtube.com/@coreyms)
  - [https://www.youtube.com/@ArjanCodes](https://www.youtube.com/@ArjanCodes)
- ğŸ“– Good websites
  - [https://realpython.com/](https://realpython.com/)
  - [https://www.w3schools.com/python](https://www.w3schools.com/python/)

## Git / GitHub

- ğŸ“½ï¸ Videos by Samvel Hayrapetyan \[[url](https://www.youtube.com/playlist?list=PLQLz3vJxwofh8KSaJ7FoA5Bw2Zdvyo4S_)\]
- ğŸ“½ï¸ Video by freeCodeCamp \[[url](https://www.youtube.com/watch?v=RGOj5yH7evk)\]

## Projects

We strongly encourage you to work on projects. Pick a dataset/competition from [Kaggle](https://kaggle.com/), and try to achieve the best result you can. Make sure to also write clean code and publish everything to your GitHub.

Some good datasets to start from are

- [MNIST](https://www.kaggle.com/competitions/digit-recognizer) (handwritten digit recognition)
  - There is also a dataset for Armenian letter recognition called [Mashtots](https://www.kaggle.com/c/mashtots-dataset).
- [Titanic](https://www.kaggle.com/competitions/titanic) (classification)
- [Wine Quality](https://www.kaggle.com/datasets/rajyellow46/wine-quality) (classification and EDA)
- [House Price](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques) (regression)
- [Personality Analysis](https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis) (clustering)

For GenAI / NLP, try these

- Fine-tune a small language model (e.g. GPT-2, DistilBERT) on a custom dataset using HuggingFace \[[guide](https://huggingface.co/learn/llm-course/en/chapter3/3)\]
- Build a simple RAG (Retrieval-Augmented Generation) pipeline
- Sentiment analysis or text classification with a pre-trained transformer

# Expected knowledge

Knowledge of classical ML alone is not sufficient â€” applicants must also have some experience with Generative AI / LLMs.

Topics starting with star(\*) are optional.



## Machine Learning

### Data Preprocessing

1. Feature Scaling
2. Data Cleaning
3. Handling Missing Data

### Model Evaluation and Selection

1. Cross-Validation
2. Hyperparameter Tuning
3. Model Complexity and Bias-Variance Tradeoff
4. Performance Metrics (Accuracy, Precision, Recall, F1 Score, ROC, AUC)

### Supervised Learning Algorithms

1. k-Nearest Neighbors (k-NN)
2. Linear Regression
3. Logistic Regression
4. Decision Trees
5. Random Forests
6. Gradient Boosting
7. \*Support Vector Machines (SVM)
8. Naive Bayes
9. Neural Networks

### Unsupervised Learning Algorithms

1. Clustering (K-means, \*Hierarchical, \*DBSCAN)
2. Principal Component Analysis (PCA)
3. \*Anomaly Detection
4. \*Association Rules

### Neural Networks and Deep Learning

1. Perceptron
2. Backpropagation
3. \*Convolutional Neural Networks (CNNs)
4. \*Recurrent Neural Networks (RNNs)
5. \*Long Short-Term Memory Networks (LSTMs)

### Generative AI / Deep Learning for NLP

We don't expect full technical mastery here â€” rather at least a solid intuition for each topic: what it is, why it matters, and roughly how it works.

1. Word Embeddings (Word2Vec: CBOW, Skip-gram)
2. RNNs and Seq2Seq models
3. Attention Mechanism
4. Transformer Architecture (Self-Attention, Multi-Head Attention, Positional Encoding)
5. Encoder-only models (BERT), Decoder-only models (GPT), Encoder-Decoder models (T5)
6. Tokenization (BPE, WordPiece, SentencePiece)
7. Pre-training objectives (MLM, CLM, NSP)
8. Fine-tuning (SFT, LoRA, Adapters)
9. RLHF and Reward Models
10. Prompting (Zero-shot, Few-shot, Chain-of-Thought)
11. Decoding Strategies (Greedy, Beam Search, Top-k, Top-p, Temperature)
12. RAG (Retrieval-Augmented Generation)
13. \*Hallucinations and Grounding
14. \*Scaling Laws
15. \*Mixture of Experts (MoE)
16. \*Inference Optimization (Quantization, KV-Cache, Distillation)
