{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39560b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "````xml\n",
    "<VSCode.Cell language=\"raw\">\n",
    "---\n",
    "title: \"Text Corpus Analyzer - OOP Project\"\n",
    "format: \n",
    "  html:\n",
    "    code-fold: false\n",
    "---\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "# üìö Text Corpus Analyzer\n",
    "\n",
    "A data-driven OOP project analyzing literary works from great authors:\n",
    "- **Fyodor Dostoevsky** - Russian psychological realism\n",
    "- **Albert Camus** - French absurdism  \n",
    "- **Erich Maria Remarque** - German war literature\n",
    "\n",
    "We'll build classes to analyze writing styles and compare authors.\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## üì¶ Import Libraries\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## üìù Document Class\n",
    "\n",
    "Represents a single text document (excerpt from a book)\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "class Document:\n",
    "    \"\"\"Represents a single text document with analysis methods\"\"\"\n",
    "    \n",
    "    # Common English stop words (small selection)\n",
    "    STOP_WORDS = {\n",
    "        'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'i',\n",
    "        'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you', 'do', 'at',\n",
    "        'this', 'but', 'his', 'by', 'from', 'they', 'we', 'her', 'she',\n",
    "        'or', 'an', 'will', 'my', 'one', 'all', 'would', 'there', 'their',\n",
    "        'what', 'so', 'if', 'who', 'which', 'when', 'can', 'has', 'had',\n",
    "        'were', 'been', 'is', 'was', 'are', 'am'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, author: str, title: str, text: str, theme: str = None, year: int = None):\n",
    "        self.author = author\n",
    "        self.title = title\n",
    "        self.text = text\n",
    "        self.theme = theme\n",
    "        self.year = year\n",
    "        self._words = None  # Cache for processed words\n",
    "        \n",
    "    def get_words(self) -> List[str]:\n",
    "        \"\"\"Get list of words (lowercase, no punctuation)\"\"\"\n",
    "        if self._words is None:\n",
    "            # Remove punctuation and convert to lowercase\n",
    "            text = self.text.lower()\n",
    "            text = re.sub(r'[^a-z\\s]', '', text)\n",
    "            self._words = text.split()\n",
    "        return self._words\n",
    "    \n",
    "    def word_count(self) -> int:\n",
    "        \"\"\"Total number of words\"\"\"\n",
    "        return len(self.get_words())\n",
    "    \n",
    "    def unique_word_count(self) -> int:\n",
    "        \"\"\"Number of unique words\"\"\"\n",
    "        return len(set(self.get_words()))\n",
    "    \n",
    "    def vocabulary_richness(self) -> float:\n",
    "        \"\"\"Ratio of unique words to total words\"\"\"\n",
    "        total = self.word_count()\n",
    "        if total == 0:\n",
    "            return 0\n",
    "        return self.unique_word_count() / total\n",
    "    \n",
    "    def average_word_length(self) -> float:\n",
    "        \"\"\"Average length of words in characters\"\"\"\n",
    "        words = self.get_words()\n",
    "        if not words:\n",
    "            return 0\n",
    "        return sum(len(word) for word in words) / len(words)\n",
    "    \n",
    "    def sentence_count(self) -> int:\n",
    "        \"\"\"Estimate number of sentences\"\"\"\n",
    "        return len(re.findall(r'[.!?]+', self.text))\n",
    "    \n",
    "    def average_sentence_length(self) -> float:\n",
    "        \"\"\"Average words per sentence\"\"\"\n",
    "        sentences = self.sentence_count()\n",
    "        if sentences == 0:\n",
    "            return 0\n",
    "        return self.word_count() / sentences\n",
    "    \n",
    "    def top_words(self, n: int = 10, exclude_stopwords: bool = True) -> List[tuple]:\n",
    "        \"\"\"Most common words\"\"\"\n",
    "        if exclude_stopwords:\n",
    "            words = [w for w in self.get_words() if w not in self.STOP_WORDS]\n",
    "        else:\n",
    "            words = self.get_words()\n",
    "        return Counter(words).most_common(n)\n",
    "    \n",
    "    def contains_word(self, word: str) -> bool:\n",
    "        \"\"\"Check if document contains a specific word\"\"\"\n",
    "        return word.lower() in self.get_words()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Document('{self.title}' by {self.author})\"\n",
    "    \n",
    "    def summary(self) -> str:\n",
    "        \"\"\"Generate a summary of the document\"\"\"\n",
    "        top_5 = ', '.join(w for w, _ in self.top_words(5))\n",
    "        return f\"\"\"\n",
    "üìÑ {self.title}\n",
    "‚úçÔ∏è  Author: {self.author}\n",
    "üè∑Ô∏è  Theme: {self.theme}\n",
    "üìÖ Year: {self.year}\n",
    "\n",
    "üìä Statistics:\n",
    "   ‚Ä¢ Total words: {self.word_count()}\n",
    "   ‚Ä¢ Unique words: {self.unique_word_count()}\n",
    "   ‚Ä¢ Vocabulary richness: {self.vocabulary_richness():.2%}\n",
    "   ‚Ä¢ Avg word length: {self.average_word_length():.2f} chars\n",
    "   ‚Ä¢ Sentences: {self.sentence_count()}\n",
    "   ‚Ä¢ Words per sentence: {self.average_sentence_length():.1f}\n",
    "\n",
    "üîù Top words: {top_5}\n",
    "\"\"\"\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## ‚úçÔ∏è Author Class\n",
    "\n",
    "Represents an author with multiple documents\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "class Author:\n",
    "    \"\"\"Represents an author with multiple documents\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.documents: List[Document] = []\n",
    "    \n",
    "    def add_document(self, document: Document):\n",
    "        \"\"\"Add a document to this author's collection\"\"\"\n",
    "        if document.author == self.name:\n",
    "            self.documents.append(document)\n",
    "        else:\n",
    "            raise ValueError(f\"Document author mismatch: '{document.author}' != '{self.name}'\")\n",
    "    \n",
    "    def document_count(self) -> int:\n",
    "        \"\"\"Number of documents\"\"\"\n",
    "        return len(self.documents)\n",
    "    \n",
    "    def total_words(self) -> int:\n",
    "        \"\"\"Total words across all documents\"\"\"\n",
    "        return sum(doc.word_count() for doc in self.documents)\n",
    "    \n",
    "    def average_document_length(self) -> float:\n",
    "        \"\"\"Average words per document\"\"\"\n",
    "        if not self.documents:\n",
    "            return 0\n",
    "        return self.total_words() / len(self.documents)\n",
    "    \n",
    "    def vocabulary_size(self) -> int:\n",
    "        \"\"\"Total unique words used\"\"\"\n",
    "        all_words = []\n",
    "        for doc in self.documents:\n",
    "            all_words.extend(doc.get_words())\n",
    "        return len(set(all_words))\n",
    "    \n",
    "    def average_word_length(self) -> float:\n",
    "        \"\"\"Average word length across all documents\"\"\"\n",
    "        if not self.documents:\n",
    "            return 0\n",
    "        return sum(doc.average_word_length() for doc in self.documents) / len(self.documents)\n",
    "    \n",
    "    def average_sentence_length(self) -> float:\n",
    "        \"\"\"Average sentence length across all documents\"\"\"\n",
    "        if not self.documents:\n",
    "            return 0\n",
    "        return sum(doc.average_sentence_length() for doc in self.documents) / len(self.documents)\n",
    "    \n",
    "    def favorite_words(self, n: int = 10) -> List[tuple]:\n",
    "        \"\"\"Most frequently used words across all documents\"\"\"\n",
    "        all_words = []\n",
    "        for doc in self.documents:\n",
    "            words = [w for w in doc.get_words() if w not in Document.STOP_WORDS]\n",
    "            all_words.extend(words)\n",
    "        return Counter(all_words).most_common(n)\n",
    "    \n",
    "    def themes(self) -> Dict[str, int]:\n",
    "        \"\"\"Count documents by theme\"\"\"\n",
    "        theme_list = [doc.theme for doc in self.documents if doc.theme]\n",
    "        return dict(Counter(theme_list))\n",
    "    \n",
    "    def find_documents_with_word(self, word: str) -> List[Document]:\n",
    "        \"\"\"Find documents containing a word\"\"\"\n",
    "        return [doc for doc in self.documents if doc.contains_word(word)]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Author('{self.name}', {len(self.documents)} documents)\"\n",
    "    \n",
    "    def summary(self) -> str:\n",
    "        \"\"\"Detailed summary of the author\"\"\"\n",
    "        themes = self.themes()\n",
    "        top_words = ', '.join(w for w, _ in self.favorite_words(5))\n",
    "        \n",
    "        return f\"\"\"\n",
    "‚úçÔ∏è  {self.name}\n",
    "{'=' * (len(self.name) + 3)}\n",
    "\n",
    "üìö Overview:\n",
    "   ‚Ä¢ Documents: {self.document_count()}\n",
    "   ‚Ä¢ Total words: {self.total_words():,}\n",
    "   ‚Ä¢ Unique vocabulary: {self.vocabulary_size():,}\n",
    "   ‚Ä¢ Avg document length: {self.average_document_length():.0f} words\n",
    "\n",
    "üìù Writing Style:\n",
    "   ‚Ä¢ Avg word length: {self.average_word_length():.2f} chars\n",
    "   ‚Ä¢ Avg sentence length: {self.average_sentence_length():.1f} words\n",
    "\n",
    "üí≠ Themes: {', '.join(f'{k} ({v})' for k, v in themes.items())}\n",
    "\n",
    "üîù Favorite words: {top_words}\n",
    "\"\"\"\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## üìö Corpus Class\n",
    "\n",
    "Manages the entire collection of documents from multiple authors\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "class Corpus:\n",
    "    \"\"\"A collection of documents from multiple authors\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str = \"Literary Corpus\"):\n",
    "        self.name = name\n",
    "        self.documents: List[Document] = []\n",
    "        self.authors: Dict[str, Author] = {}\n",
    "    \n",
    "    def add_document(self, document: Document):\n",
    "        \"\"\"Add a document to the corpus\"\"\"\n",
    "        self.documents.append(document)\n",
    "        \n",
    "        # Add to author's collection\n",
    "        if document.author not in self.authors:\n",
    "            self.authors[document.author] = Author(document.author)\n",
    "        self.authors[document.author].add_document(document)\n",
    "    \n",
    "    def load_from_json(self, filepath: str):\n",
    "        \"\"\"Load documents from a JSON file\"\"\"\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        for item in data:\n",
    "            doc = Document(\n",
    "                author=item['author'],\n",
    "                title=item['title'],\n",
    "                text=item['text'],\n",
    "                theme=item.get('theme'),\n",
    "                year=item.get('year')\n",
    "            )\n",
    "            self.add_document(doc)\n",
    "    \n",
    "    def get_author(self, name: str) -> Author:\n",
    "        \"\"\"Get Author object by name\"\"\"\n",
    "        return self.authors.get(name)\n",
    "    \n",
    "    def author_names(self) -> List[str]:\n",
    "        \"\"\"List all author names\"\"\"\n",
    "        return sorted(self.authors.keys())\n",
    "    \n",
    "    def compare_authors(self, metric: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compare authors on a specific metric.\n",
    "        Options: 'avg_word_length', 'avg_sentence_length', 'vocabulary_size'\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        for author_name, author in self.authors.items():\n",
    "            if metric == 'avg_word_length':\n",
    "                result[author_name] = author.average_word_length()\n",
    "            elif metric == 'avg_sentence_length':\n",
    "                result[author_name] = author.average_sentence_length()\n",
    "            elif metric == 'vocabulary_size':\n",
    "                result[author_name] = author.vocabulary_size()\n",
    "            elif metric == 'total_words':\n",
    "                result[author_name] = author.total_words()\n",
    "        \n",
    "        # Sort by value, highest first\n",
    "        return dict(sorted(result.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    def find_by_theme(self, theme: str) -> List[Document]:\n",
    "        \"\"\"Find all documents with a specific theme\"\"\"\n",
    "        return [doc for doc in self.documents if doc.theme == theme]\n",
    "    \n",
    "    def all_themes(self) -> List[str]:\n",
    "        \"\"\"Get all unique themes\"\"\"\n",
    "        themes = set(doc.theme for doc in self.documents if doc.theme)\n",
    "        return sorted(themes)\n",
    "    \n",
    "    def search(self, word: str) -> List[Document]:\n",
    "        \"\"\"Find documents containing a word\"\"\"\n",
    "        return [doc for doc in self.documents if doc.contains_word(word)]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Corpus('{self.name}', {len(self.authors)} authors, {len(self.documents)} docs)\"\n",
    "    \n",
    "    def summary(self) -> str:\n",
    "        \"\"\"Overview of the corpus\"\"\"\n",
    "        total_words = sum(author.total_words() for author in self.authors.values())\n",
    "        \n",
    "        return f\"\"\"\n",
    "üìö {self.name}\n",
    "{'=' * (len(self.name) + 3)}\n",
    "\n",
    "üìä Statistics:\n",
    "   ‚Ä¢ Authors: {len(self.authors)}\n",
    "   ‚Ä¢ Documents: {len(self.documents)}\n",
    "   ‚Ä¢ Total words: {total_words:,}\n",
    "   ‚Ä¢ Themes: {len(self.all_themes())}\n",
    "\n",
    "‚úçÔ∏è  Authors: {', '.join(self.author_names())}\n",
    "\n",
    "üè∑Ô∏è  Themes: {', '.join(self.all_themes())}\n",
    "\"\"\"\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "---\n",
    "\n",
    "# üî¨ Analysis Examples\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 1Ô∏è‚É£ Load the Data\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Create a corpus and load all authors\n",
    "corpus = Corpus(\"Great 20th Century Writers\")\n",
    "corpus.load_from_json('data/authors.json')\n",
    "\n",
    "print(corpus.summary())\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 2Ô∏è‚É£ Analyze a Single Document\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Look at the first document\n",
    "doc = corpus.documents[0]\n",
    "print(doc.summary())\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 3Ô∏è‚É£ Compare All Authors\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Print summary for each author\n",
    "for author_name in corpus.author_names():\n",
    "    author = corpus.get_author(author_name)\n",
    "    print(author.summary())\n",
    "    print(\"=\"*80, \"\\n\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 4Ô∏è‚É£ Compare Authors on Specific Metrics\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "print(\"üìè WHO USES LONGER WORDS?\")\n",
    "for author, length in corpus.compare_authors('avg_word_length').items():\n",
    "    print(f\"   {author:30} {length:.2f} characters\")\n",
    "\n",
    "print(\"\\nüìê WHO WRITES LONGER SENTENCES?\")\n",
    "for author, length in corpus.compare_authors('avg_sentence_length').items():\n",
    "    print(f\"   {author:30} {length:.1f} words\")\n",
    "\n",
    "print(\"\\nüìñ WHO HAS THE RICHEST VOCABULARY?\")\n",
    "for author, vocab in corpus.compare_authors('vocabulary_size').items():\n",
    "    print(f\"   {author:30} {vocab:,} unique words\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 5Ô∏è‚É£ Explore Themes\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "print(\"üè∑Ô∏è  DOCUMENTS BY THEME:\\n\")\n",
    "\n",
    "for theme in corpus.all_themes():\n",
    "    docs = corpus.find_by_theme(theme)\n",
    "    print(f\"{theme} ({len(docs)} docs):\")\n",
    "    for doc in docs:\n",
    "        print(f\"   ‚Ä¢ {doc.title} by {doc.author}\")\n",
    "    print()\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 6Ô∏è‚É£ Search for Specific Words\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Which documents mention \"death\"?\n",
    "print(\"üíÄ Documents containing 'death':\\n\")\n",
    "death_docs = corpus.search('death')\n",
    "for doc in death_docs:\n",
    "    print(f\"   ‚Ä¢ {doc.title} ({doc.author})\")\n",
    "\n",
    "print(f\"\\nüìä {len(death_docs)} out of {len(corpus.documents)} documents mention death\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Which documents mention \"freedom\"?\n",
    "print(\"üïäÔ∏è Documents containing 'freedom':\\n\")\n",
    "freedom_docs = corpus.search('freedom')\n",
    "for doc in freedom_docs:\n",
    "    print(f\"   ‚Ä¢ {doc.title} ({doc.author})\")\n",
    "\n",
    "print(f\"\\nüìä {len(freedom_docs)} out of {len(corpus.documents)} documents mention freedom\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 7Ô∏è‚É£ Analyze Author's Favorite Words\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "print(\"üéØ TOP 10 WORDS FOR EACH AUTHOR:\\n\")\n",
    "\n",
    "for author_name in corpus.author_names():\n",
    "    author = corpus.get_author(author_name)\n",
    "    print(f\"{author_name}:\")\n",
    "    \n",
    "    for word, count in author.favorite_words(10):\n",
    "        print(f\"   {word:15} {count:3} times\")\n",
    "    print()\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 8Ô∏è‚É£ Find Who Writes About Specific Topics\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Check which author talks most about \"war\"\n",
    "print(\"‚öîÔ∏è WHO TALKS ABOUT WAR?\\n\")\n",
    "\n",
    "for author_name in corpus.author_names():\n",
    "    author = corpus.get_author(author_name)\n",
    "    war_docs = author.find_documents_with_word('war')\n",
    "    if war_docs:\n",
    "        print(f\"{author_name}: {len(war_docs)} document(s)\")\n",
    "        for doc in war_docs:\n",
    "            print(f\"   ‚Ä¢ {doc.title}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "---\n",
    "\n",
    "# üéØ Exercises for Students\n",
    "\n",
    "1. **Add a new method**: Create a `longest_words(n)` method in Document class that returns the N longest words\n",
    "\n",
    "2. **Comparison method**: Add a method to compare two authors directly\n",
    "\n",
    "3. **Time analysis**: Add methods to analyze documents by year/decade\n",
    "\n",
    "4. **Word pairs**: Extend Document to find common word pairs (bigrams)\n",
    "\n",
    "5. **Inheritance**: Create specialized classes like `NovelExcerpt` or `PhilosophicalText` that inherit from Document\n",
    "\n",
    "6. **Visualization**: Use matplotlib to create bar charts comparing authors\n",
    "\n",
    "7. **More data**: Add JSON data for other authors (Tolstoy, Kafka, Hemingway)\n",
    "\n",
    "8. **Export**: Add methods to export analysis results to CSV\n",
    "\n",
    "9. **Statistics**: Calculate and compare standard deviation in sentence lengths\n",
    "\n",
    "10. **Themes analysis**: Create a new `Theme` class that analyzes documents by theme\n",
    "</VSCode.Cell>\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36123286",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "title: \"Text Corpus Analyzer - OOP Project\"\n",
    "format: \n",
    "  html:\n",
    "    code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8eb8fa",
   "metadata": {},
   "source": [
    "# üìö Text Corpus Analyzer\n",
    "\n",
    "A data-driven OOP project analyzing literary works from great authors:\n",
    "- **Fyodor Dostoevsky** - Russian psychological realism\n",
    "- **Albert Camus** - French absurdism  \n",
    "- **Erich Maria Remarque** - German war literature\n",
    "\n",
    "We'll build classes to analyze writing styles, themes, and linguistic patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419bc836",
   "metadata": {},
   "source": [
    "## üì¶ Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8e0cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import math\n",
    "from typing import List, Dict, Set\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516662c1",
   "metadata": {},
   "source": [
    "## üìù Document Class\n",
    "\n",
    "Represents a single text document (excerpt from a book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a163f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    \"\"\"Represents a single text document with analysis methods\"\"\"\n",
    "    \n",
    "    # Common English stop words\n",
    "    STOP_WORDS = {\n",
    "        'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'i',\n",
    "        'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you', 'do', 'at',\n",
    "        'this', 'but', 'his', 'by', 'from', 'they', 'we', 'say', 'her', 'she',\n",
    "        'or', 'an', 'will', 'my', 'one', 'all', 'would', 'there', 'their',\n",
    "        'what', 'so', 'up', 'out', 'if', 'about', 'who', 'get', 'which', 'go',\n",
    "        'me', 'when', 'make', 'can', 'like', 'time', 'no', 'just', 'him', 'know',\n",
    "        'take', 'people', 'into', 'year', 'your', 'good', 'some', 'could', 'them',\n",
    "        'see', 'other', 'than', 'then', 'now', 'look', 'only', 'come', 'its', 'over',\n",
    "        'think', 'also', 'back', 'after', 'use', 'two', 'how', 'our', 'work',\n",
    "        'first', 'well', 'way', 'even', 'new', 'want', 'because', 'any', 'these',\n",
    "        'give', 'day', 'most', 'us', 'is', 'was', 'are', 'been', 'has', 'had',\n",
    "        'were', 'said', 'did', 'having', 'may', 'should', 'am'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, author: str, title: str, text: str, theme: str = None, year: int = None):\n",
    "        self.author = author\n",
    "        self.title = title\n",
    "        self.text = text\n",
    "        self.theme = theme\n",
    "        self.year = year\n",
    "        self._tokens = None  # Cache for tokens\n",
    "        self._words = None   # Cache for words only\n",
    "        \n",
    "    def tokenize(self) -> List[str]:\n",
    "        \"\"\"Split text into tokens (words)\"\"\"\n",
    "        if self._tokens is None:\n",
    "            # Remove punctuation and convert to lowercase\n",
    "            text = self.text.lower()\n",
    "            # Keep only letters and spaces\n",
    "            text = re.sub(r'[^a-z\\s]', '', text)\n",
    "            self._tokens = text.split()\n",
    "        return self._tokens\n",
    "    \n",
    "    def get_words(self) -> List[str]:\n",
    "        \"\"\"Get list of words (same as tokens for now, but could filter further)\"\"\"\n",
    "        if self._words is None:\n",
    "            self._words = self.tokenize()\n",
    "        return self._words\n",
    "    \n",
    "    def word_count(self) -> int:\n",
    "        \"\"\"Total number of words in document\"\"\"\n",
    "        return len(self.get_words())\n",
    "    \n",
    "    def unique_word_count(self) -> int:\n",
    "        \"\"\"Number of unique words (vocabulary size)\"\"\"\n",
    "        return len(set(self.get_words()))\n",
    "    \n",
    "    def vocabulary_richness(self) -> float:\n",
    "        \"\"\"Ratio of unique words to total words (type-token ratio)\"\"\"\n",
    "        total = self.word_count()\n",
    "        if total == 0:\n",
    "            return 0\n",
    "        return self.unique_word_count() / total\n",
    "    \n",
    "    def average_word_length(self) -> float:\n",
    "        \"\"\"Average length of words in characters\"\"\"\n",
    "        words = self.get_words()\n",
    "        if not words:\n",
    "            return 0\n",
    "        return sum(len(word) for word in words) / len(words)\n",
    "    \n",
    "    def sentence_count(self) -> int:\n",
    "        \"\"\"Estimate number of sentences\"\"\"\n",
    "        # Count sentence-ending punctuation\n",
    "        return len(re.findall(r'[.!?]+', self.text))\n",
    "    \n",
    "    def average_sentence_length(self) -> float:\n",
    "        \"\"\"Average words per sentence\"\"\"\n",
    "        sentences = self.sentence_count()\n",
    "        if sentences == 0:\n",
    "            return 0\n",
    "        return self.word_count() / sentences\n",
    "    \n",
    "    def word_frequency(self, top_n: int = 10) -> List[tuple]:\n",
    "        \"\"\"Most common words with their frequencies\"\"\"\n",
    "        words = self.get_words()\n",
    "        counter = Counter(words)\n",
    "        return counter.most_common(top_n)\n",
    "    \n",
    "    def non_stopword_frequency(self, top_n: int = 10) -> List[tuple]:\n",
    "        \"\"\"Most common non-stopwords\"\"\"\n",
    "        words = [w for w in self.get_words() if w not in self.STOP_WORDS]\n",
    "        counter = Counter(words)\n",
    "        return counter.most_common(top_n)\n",
    "    \n",
    "    def long_words(self, min_length: int = 7) -> List[str]:\n",
    "        \"\"\"Find words longer than min_length characters\"\"\"\n",
    "        return sorted(set(w for w in self.get_words() if len(w) >= min_length))\n",
    "    \n",
    "    def contains_word(self, word: str) -> bool:\n",
    "        \"\"\"Check if document contains a specific word\"\"\"\n",
    "        return word.lower() in self.get_words()\n",
    "    \n",
    "    def word_positions(self, word: str) -> List[int]:\n",
    "        \"\"\"Get all positions (indices) where a word appears\"\"\"\n",
    "        word = word.lower()\n",
    "        return [i for i, w in enumerate(self.get_words()) if w == word]\n",
    "    \n",
    "    def flesch_reading_ease(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Flesch Reading Ease score.\n",
    "        90-100: Very Easy\n",
    "        60-70: Standard\n",
    "        0-30: Very Difficult\n",
    "        \"\"\"\n",
    "        words = self.word_count()\n",
    "        sentences = self.sentence_count()\n",
    "        \n",
    "        if words == 0 or sentences == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Count syllables (rough approximation)\n",
    "        syllables = sum(self._count_syllables(word) for word in self.get_words())\n",
    "        \n",
    "        # Flesch formula\n",
    "        score = 206.835 - 1.015 * (words / sentences) - 84.6 * (syllables / words)\n",
    "        return round(score, 2)\n",
    "    \n",
    "    def _count_syllables(self, word: str) -> int:\n",
    "        \"\"\"Rough syllable counter\"\"\"\n",
    "        word = word.lower()\n",
    "        count = 0\n",
    "        vowels = 'aeiouy'\n",
    "        previous_was_vowel = False\n",
    "        \n",
    "        for char in word:\n",
    "            is_vowel = char in vowels\n",
    "            if is_vowel and not previous_was_vowel:\n",
    "                count += 1\n",
    "            previous_was_vowel = is_vowel\n",
    "        \n",
    "        # Adjust for silent e\n",
    "        if word.endswith('e'):\n",
    "            count -= 1\n",
    "        \n",
    "        # At least one syllable\n",
    "        if count == 0:\n",
    "            count = 1\n",
    "            \n",
    "        return count\n",
    "    \n",
    "    def sentiment_score(self) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Basic sentiment analysis using word lists.\n",
    "        Returns counts of positive, negative, and death-related words.\n",
    "        \"\"\"\n",
    "        positive_words = {\n",
    "            'love', 'hope', 'joy', 'peace', 'happy', 'beauty', 'beautiful', \n",
    "            'good', 'comfort', 'freedom', 'light', 'life', 'salvation'\n",
    "        }\n",
    "        \n",
    "        negative_words = {\n",
    "            'death', 'fear', 'pain', 'suffering', 'despair', 'shame', 'guilt',\n",
    "            'terrible', 'awful', 'horrible', 'bad', 'evil', 'dark', 'murder'\n",
    "        }\n",
    "        \n",
    "        death_words = {\n",
    "            'death', 'dead', 'die', 'died', 'dying', 'kill', 'killed', 'murder',\n",
    "            'murdered', 'corpse', 'grave', 'funeral', 'coffin'\n",
    "        }\n",
    "        \n",
    "        words = self.get_words()\n",
    "        \n",
    "        return {\n",
    "            'positive': sum(1 for w in words if w in positive_words),\n",
    "            'negative': sum(1 for w in words if w in negative_words),\n",
    "            'death_related': sum(1 for w in words if w in death_words)\n",
    "        }\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Document('{self.title}' by {self.author}, {self.word_count()} words)\"\n",
    "    \n",
    "    def summary(self) -> str:\n",
    "        \"\"\"Generate a summary of the document statistics\"\"\"\n",
    "        return f\"\"\"\n",
    "üìÑ {self.title}\n",
    "‚úçÔ∏è  Author: {self.author}\n",
    "üìÖ Year: {self.year}\n",
    "üè∑Ô∏è  Theme: {self.theme}\n",
    "\n",
    "üìä Statistics:\n",
    "   ‚Ä¢ Words: {self.word_count()}\n",
    "   ‚Ä¢ Unique words: {self.unique_word_count()}\n",
    "   ‚Ä¢ Vocabulary richness: {self.vocabulary_richness():.2%}\n",
    "   ‚Ä¢ Average word length: {self.average_word_length():.2f} characters\n",
    "   ‚Ä¢ Sentences: {self.sentence_count()}\n",
    "   ‚Ä¢ Words per sentence: {self.average_sentence_length():.1f}\n",
    "   ‚Ä¢ Readability (Flesch): {self.flesch_reading_ease():.1f}\n",
    "\n",
    "üîù Top 5 content words: {', '.join(w for w, _ in self.non_stopword_frequency(5))}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6e16df",
   "metadata": {},
   "source": [
    "## ‚úçÔ∏è Author Class\n",
    "\n",
    "Represents an author and analyzes all their documents collectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1003939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Author:\n",
    "    \"\"\"Represents an author with multiple documents\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.documents: List[Document] = []\n",
    "    \n",
    "    def add_document(self, document: Document):\n",
    "        \"\"\"Add a document to this author's collection\"\"\"\n",
    "        if document.author == self.name:\n",
    "            self.documents.append(document)\n",
    "        else:\n",
    "            raise ValueError(f\"Document author '{document.author}' doesn't match Author '{self.name}'\")\n",
    "    \n",
    "    def document_count(self) -> int:\n",
    "        \"\"\"Total number of documents by this author\"\"\"\n",
    "        return len(self.documents)\n",
    "    \n",
    "    def total_words(self) -> int:\n",
    "        \"\"\"Total words across all documents\"\"\"\n",
    "        return sum(doc.word_count() for doc in self.documents)\n",
    "    \n",
    "    def average_words_per_document(self) -> float:\n",
    "        \"\"\"Average document length\"\"\"\n",
    "        if not self.documents:\n",
    "            return 0\n",
    "        return self.total_words() / len(self.documents)\n",
    "    \n",
    "    def vocabulary_size(self) -> int:\n",
    "        \"\"\"Total unique words used by author across all documents\"\"\"\n",
    "        all_words = []\n",
    "        for doc in self.documents:\n",
    "            all_words.extend(doc.get_words())\n",
    "        return len(set(all_words))\n",
    "    \n",
    "    def overall_vocabulary_richness(self) -> float:\n",
    "        \"\"\"Vocabulary richness across all documents\"\"\"\n",
    "        total = self.total_words()\n",
    "        if total == 0:\n",
    "            return 0\n",
    "        return self.vocabulary_size() / total\n",
    "    \n",
    "    def average_word_length(self) -> float:\n",
    "        \"\"\"Average word length across all documents\"\"\"\n",
    "        if not self.documents:\n",
    "            return 0\n",
    "        return sum(doc.average_word_length() for doc in self.documents) / len(self.documents)\n",
    "    \n",
    "    def average_sentence_length(self) -> float:\n",
    "        \"\"\"Average sentence length across all documents\"\"\"\n",
    "        if not self.documents:\n",
    "            return 0\n",
    "        return sum(doc.average_sentence_length() for doc in self.documents) / len(self.documents)\n",
    "    \n",
    "    def average_readability(self) -> float:\n",
    "        \"\"\"Average Flesch reading ease score\"\"\"\n",
    "        if not self.documents:\n",
    "            return 0\n",
    "        return sum(doc.flesch_reading_ease() for doc in self.documents) / len(self.documents)\n",
    "    \n",
    "    def favorite_words(self, top_n: int = 10, exclude_stopwords: bool = True) -> List[tuple]:\n",
    "        \"\"\"Most frequently used words across all documents\"\"\"\n",
    "        all_words = []\n",
    "        for doc in self.documents:\n",
    "            if exclude_stopwords:\n",
    "                all_words.extend([w for w in doc.get_words() if w not in Document.STOP_WORDS])\n",
    "            else:\n",
    "                all_words.extend(doc.get_words())\n",
    "        \n",
    "        counter = Counter(all_words)\n",
    "        return counter.most_common(top_n)\n",
    "    \n",
    "    def themes_distribution(self) -> Dict[str, int]:\n",
    "        \"\"\"Count documents by theme\"\"\"\n",
    "        themes = [doc.theme for doc in self.documents if doc.theme]\n",
    "        return dict(Counter(themes))\n",
    "    \n",
    "    def signature_words(self, min_frequency: int = 3) -> List[str]:\n",
    "        \"\"\"\n",
    "        Words that appear frequently in this author's work.\n",
    "        These could be considered 'signature' words for the author.\n",
    "        \"\"\"\n",
    "        word_freq = self.favorite_words(top_n=50, exclude_stopwords=True)\n",
    "        return [word for word, count in word_freq if count >= min_frequency]\n",
    "    \n",
    "    def sentiment_profile(self) -> Dict[str, float]:\n",
    "        \"\"\"Average sentiment scores across all documents\"\"\"\n",
    "        if not self.documents:\n",
    "            return {'positive': 0, 'negative': 0, 'death_related': 0}\n",
    "        \n",
    "        total_positive = sum(doc.sentiment_score()['positive'] for doc in self.documents)\n",
    "        total_negative = sum(doc.sentiment_score()['negative'] for doc in self.documents)\n",
    "        total_death = sum(doc.sentiment_score()['death_related'] for doc in self.documents)\n",
    "        total_words = self.total_words()\n",
    "        \n",
    "        if total_words == 0:\n",
    "            return {'positive': 0, 'negative': 0, 'death_related': 0}\n",
    "        \n",
    "        return {\n",
    "            'positive': (total_positive / total_words) * 100,\n",
    "            'negative': (total_negative / total_words) * 100,\n",
    "            'death_related': (total_death / total_words) * 100\n",
    "        }\n",
    "    \n",
    "    def stylistic_fingerprint(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Create a 'fingerprint' of the author's writing style\n",
    "        using various metrics\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'avg_word_length': round(self.average_word_length(), 2),\n",
    "            'avg_sentence_length': round(self.average_sentence_length(), 2),\n",
    "            'vocabulary_richness': round(self.overall_vocabulary_richness(), 4),\n",
    "            'readability': round(self.average_readability(), 2),\n",
    "            'sentiment_positive_pct': round(self.sentiment_profile()['positive'], 3),\n",
    "            'sentiment_negative_pct': round(self.sentiment_profile()['negative'], 3),\n",
    "            'death_theme_pct': round(self.sentiment_profile()['death_related'], 3)\n",
    "        }\n",
    "    \n",
    "    def find_documents_with_word(self, word: str) -> List[Document]:\n",
    "        \"\"\"Find all documents containing a specific word\"\"\"\n",
    "        return [doc for doc in self.documents if doc.contains_word(word)]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Author('{self.name}', {len(self.documents)} documents, {self.total_words()} words)\"\n",
    "    \n",
    "    def summary(self) -> str:\n",
    "        \"\"\"Detailed summary of the author's corpus\"\"\"\n",
    "        fp = self.stylistic_fingerprint()\n",
    "        themes = self.themes_distribution()\n",
    "        top_words = self.favorite_words(5)\n",
    "        \n",
    "        return f\"\"\"\n",
    "‚úçÔ∏è  {self.name}\n",
    "{'=' * (len(self.name) + 3)}\n",
    "\n",
    "üìö Corpus Overview:\n",
    "   ‚Ä¢ Documents: {self.document_count()}\n",
    "   ‚Ä¢ Total words: {self.total_words():,}\n",
    "   ‚Ä¢ Vocabulary size: {self.vocabulary_size():,} unique words\n",
    "   ‚Ä¢ Average document length: {self.average_words_per_document():.0f} words\n",
    "\n",
    "üé® Writing Style:\n",
    "   ‚Ä¢ Average word length: {fp['avg_word_length']} characters\n",
    "   ‚Ä¢ Average sentence length: {fp['avg_sentence_length']:.1f} words\n",
    "   ‚Ä¢ Vocabulary richness: {fp['vocabulary_richness']:.4f}\n",
    "   ‚Ä¢ Readability score: {fp['readability']:.1f}\n",
    "\n",
    "üí≠ Themes: {', '.join(f'{k} ({v})' for k, v in themes.items())}\n",
    "\n",
    "üîù Favorite words: {', '.join(w for w, _ in top_words)}\n",
    "\n",
    "üòä Sentiment: {fp['sentiment_positive_pct']:.1f}% positive, {fp['sentiment_negative_pct']:.1f}% negative\n",
    "üíÄ Death references: {fp['death_theme_pct']:.1f}% of words\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18072455",
   "metadata": {},
   "source": [
    "## üìö Corpus Class\n",
    "\n",
    "Manages a collection of documents from multiple authors and provides comparative analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7d92e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    \"\"\"A collection of documents from multiple authors with analysis capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str = \"Literary Corpus\"):\n",
    "        self.name = name\n",
    "        self.documents: List[Document] = []\n",
    "        self.authors: Dict[str, Author] = {}\n",
    "    \n",
    "    def add_document(self, document: Document):\n",
    "        \"\"\"Add a document to the corpus\"\"\"\n",
    "        self.documents.append(document)\n",
    "        \n",
    "        # Add to author's collection\n",
    "        if document.author not in self.authors:\n",
    "            self.authors[document.author] = Author(document.author)\n",
    "        self.authors[document.author].add_document(document)\n",
    "    \n",
    "    def load_from_json(self, filepath: str):\n",
    "        \"\"\"Load documents from a JSON file\"\"\"\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        for item in data:\n",
    "            doc = Document(\n",
    "                author=item['author'],\n",
    "                title=item['title'],\n",
    "                text=item['text'],\n",
    "                theme=item.get('theme'),\n",
    "                year=item.get('year')\n",
    "            )\n",
    "            self.add_document(doc)\n",
    "    \n",
    "    def get_author(self, author_name: str) -> Author:\n",
    "        \"\"\"Get Author object by name\"\"\"\n",
    "        return self.authors.get(author_name)\n",
    "    \n",
    "    def author_names(self) -> List[str]:\n",
    "        \"\"\"List of all author names in corpus\"\"\"\n",
    "        return sorted(self.authors.keys())\n",
    "    \n",
    "    def document_count(self) -> int:\n",
    "        \"\"\"Total number of documents\"\"\"\n",
    "        return len(self.documents)\n",
    "    \n",
    "    def total_words(self) -> int:\n",
    "        \"\"\"Total words in entire corpus\"\"\"\n",
    "        return sum(doc.word_count() for doc in self.documents)\n",
    "    \n",
    "    def compare_authors(self, metric: str = 'avg_word_length') -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compare authors on a specific metric.\n",
    "        Available metrics: avg_word_length, avg_sentence_length, \n",
    "                          vocabulary_richness, readability\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        for author_name, author in self.authors.items():\n",
    "            fp = author.stylistic_fingerprint()\n",
    "            if metric in fp:\n",
    "                result[author_name] = fp[metric]\n",
    "        return dict(sorted(result.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    def find_documents_by_theme(self, theme: str) -> List[Document]:\n",
    "        \"\"\"Find all documents with a specific theme\"\"\"\n",
    "        return [doc for doc in self.documents if doc.theme == theme]\n",
    "    \n",
    "    def themes_in_corpus(self) -> List[str]:\n",
    "        \"\"\"Get all unique themes in the corpus\"\"\"\n",
    "        themes = set(doc.theme for doc in self.documents if doc.theme)\n",
    "        return sorted(themes)\n",
    "    \n",
    "    def search(self, query: str) -> List[Document]:\n",
    "        \"\"\"Search for documents containing a specific word\"\"\"\n",
    "        query = query.lower()\n",
    "        return [doc for doc in self.documents if query in doc.text.lower()]\n",
    "    \n",
    "    def tf_idf(self, word: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate TF-IDF (Term Frequency-Inverse Document Frequency) for a word.\n",
    "        Returns TF-IDF scores for each document containing the word.\n",
    "        \"\"\"\n",
    "        word = word.lower()\n",
    "        n_documents = len(self.documents)\n",
    "        \n",
    "        # Document frequency (how many documents contain the word)\n",
    "        df = sum(1 for doc in self.documents if doc.contains_word(word))\n",
    "        \n",
    "        if df == 0:\n",
    "            return {}\n",
    "        \n",
    "        # Inverse document frequency\n",
    "        idf = math.log(n_documents / df)\n",
    "        \n",
    "        # Calculate TF-IDF for each document\n",
    "        results = {}\n",
    "        for doc in self.documents:\n",
    "            if doc.contains_word(word):\n",
    "                # Term frequency in this document\n",
    "                tf = doc.get_words().count(word) / doc.word_count()\n",
    "                tfidf = tf * idf\n",
    "                results[doc.title] = round(tfidf, 4)\n",
    "        \n",
    "        return dict(sorted(results.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    def most_distinctive_words_per_author(self, top_n: int = 5) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Find words that are distinctive to each author using TF-IDF concept.\n",
    "        Words that appear frequently in one author but rarely in others.\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        \n",
    "        for author_name, author in self.authors.items():\n",
    "            # Get author's favorite words\n",
    "            author_words = [w for w, _ in author.favorite_words(30)]\n",
    "            \n",
    "            # Calculate how distinctive each word is\n",
    "            distinctiveness = {}\n",
    "            for word in author_words:\n",
    "                # How often does this author use it vs others?\n",
    "                author_freq = sum(1 for doc in author.documents if doc.contains_word(word))\n",
    "                other_freq = sum(1 for doc in self.documents \n",
    "                               if doc.author != author_name and doc.contains_word(word))\n",
    "                \n",
    "                # Simple distinctiveness score\n",
    "                if author_freq > 0:\n",
    "                    score = author_freq / (other_freq + 1)  # +1 to avoid division by zero\n",
    "                    distinctiveness[word] = score\n",
    "            \n",
    "            # Get top N most distinctive\n",
    "            top_words = sorted(distinctiveness.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "            result[author_name] = [word for word, _ in top_words]\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def author_similarity(self, author1: str, author2: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate similarity between two authors based on shared vocabulary.\n",
    "        Returns a score between 0 and 1 (1 = identical vocabulary overlap).\n",
    "        \"\"\"\n",
    "        if author1 not in self.authors or author2 not in self.authors:\n",
    "            return 0.0\n",
    "        \n",
    "        # Get signature words for each author\n",
    "        words1 = set(self.authors[author1].signature_words(min_frequency=2))\n",
    "        words2 = set(self.authors[author2].signature_words(min_frequency=2))\n",
    "        \n",
    "        if not words1 or not words2:\n",
    "            return 0.0\n",
    "        \n",
    "        # Jaccard similarity\n",
    "        intersection = len(words1 & words2)\n",
    "        union = len(words1 | words2)\n",
    "        \n",
    "        return round(intersection / union, 3) if union > 0 else 0.0\n",
    "    \n",
    "    def chronological_documents(self) -> List[Document]:\n",
    "        \"\"\"Return documents sorted by year\"\"\"\n",
    "        return sorted([doc for doc in self.documents if doc.year], \n",
    "                     key=lambda x: x.year)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Corpus('{self.name}', {len(self.authors)} authors, {len(self.documents)} documents)\"\n",
    "    \n",
    "    def summary(self) -> str:\n",
    "        \"\"\"Overview of the entire corpus\"\"\"\n",
    "        return f\"\"\"\n",
    "üìö {self.name}\n",
    "{'=' * (len(self.name) + 3)}\n",
    "\n",
    "üìä Corpus Statistics:\n",
    "   ‚Ä¢ Authors: {len(self.authors)}\n",
    "   ‚Ä¢ Documents: {len(self.documents)}\n",
    "   ‚Ä¢ Total words: {self.total_words():,}\n",
    "   ‚Ä¢ Themes: {len(self.themes_in_corpus())}\n",
    "\n",
    "‚úçÔ∏è  Authors: {', '.join(self.author_names())}\n",
    "\n",
    "üè∑Ô∏è  Themes: {', '.join(self.themes_in_corpus())}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4cc9ff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üî¨ Analysis Examples\n",
    "\n",
    "Let's put our classes to work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9afc0c",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf4443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a corpus\n",
    "corpus = Corpus(\"Great 20th Century Writers\")\n",
    "\n",
    "# Load all three authors\n",
    "corpus.load_from_json('data/dostoevsky.json')\n",
    "corpus.load_from_json('data/camus.json')\n",
    "corpus.load_from_json('data/remarque.json')\n",
    "\n",
    "print(corpus.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a951da",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Analyze a Single Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf7b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first Dostoevsky document\n",
    "doc = corpus.documents[0]\n",
    "\n",
    "print(doc.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95b1b08",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Compare Writing Styles of Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbdd914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary for each author\n",
    "for author_name in corpus.author_names():\n",
    "    author = corpus.get_author(author_name)\n",
    "    print(author.summary())\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08119eb2",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Compare Specific Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28f0518",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä AVERAGE WORD LENGTH (who uses longer words?)\")\n",
    "for author, length in corpus.compare_authors('avg_word_length').items():\n",
    "    print(f\"   {author:30} {length:.2f} characters\")\n",
    "\n",
    "print(\"\\nüìè SENTENCE LENGTH (who writes longer sentences?)\")\n",
    "for author, length in corpus.compare_authors('avg_sentence_length').items():\n",
    "    print(f\"   {author:30} {length:.1f} words\")\n",
    "\n",
    "print(\"\\nüìñ VOCABULARY RICHNESS (who uses more varied vocabulary?)\")\n",
    "for author, richness in corpus.compare_authors('vocabulary_richness').items():\n",
    "    print(f\"   {author:30} {richness:.4f}\")\n",
    "\n",
    "print(\"\\nüìù READABILITY (Flesch score: higher = easier to read)\")\n",
    "for author, score in corpus.compare_authors('readability').items():\n",
    "    print(f\"   {author:30} {score:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757fe2fe",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Find Distinctive Words for Each Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e84addd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ DISTINCTIVE WORDS (words characteristic of each author):\\n\")\n",
    "\n",
    "distinctive = corpus.most_distinctive_words_per_author(top_n=8)\n",
    "for author, words in distinctive.items():\n",
    "    print(f\"{author}:\")\n",
    "    print(f\"   {', '.join(words)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0955fd",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ TF-IDF Analysis\n",
    "\n",
    "Find which documents use a specific word most meaningfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b7d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the word \"death\"\n",
    "print(\"üíÄ TF-IDF for 'death' (which documents focus most on death?):\\n\")\n",
    "death_tfidf = corpus.tf_idf('death')\n",
    "for doc_title, score in list(death_tfidf.items())[:5]:\n",
    "    print(f\"   {score:.4f} - {doc_title}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Analyze the word \"freedom\"\n",
    "print(\"\\nüïäÔ∏è TF-IDF for 'freedom' (which documents focus most on freedom?):\\n\")\n",
    "freedom_tfidf = corpus.tf_idf('freedom')\n",
    "for doc_title, score in list(freedom_tfidf.items())[:5]:\n",
    "    print(f\"   {score:.4f} - {doc_title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cea695",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Author Similarity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423d0eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§ù AUTHOR SIMILARITY (based on vocabulary overlap):\\n\")\n",
    "\n",
    "authors = corpus.author_names()\n",
    "for i, author1 in enumerate(authors):\n",
    "    for author2 in authors[i+1:]:\n",
    "        similarity = corpus.author_similarity(author1, author2)\n",
    "        print(f\"   {author1} ‚ÜîÔ∏è {author2}: {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fad327",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Search for Themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b97d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üè∑Ô∏è  ALL THEMES IN CORPUS:\")\n",
    "for theme in corpus.themes_in_corpus():\n",
    "    docs = corpus.find_documents_by_theme(theme)\n",
    "    print(f\"\\n   {theme} ({len(docs)} documents):\")\n",
    "    for doc in docs:\n",
    "        print(f\"      ‚Ä¢ {doc.title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58bcf37",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb8fc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üòäüò¢ SENTIMENT PROFILES BY AUTHOR:\\n\")\n",
    "\n",
    "for author_name in corpus.author_names():\n",
    "    author = corpus.get_author(author_name)\n",
    "    sentiment = author.sentiment_profile()\n",
    "    \n",
    "    print(f\"{author_name}:\")\n",
    "    print(f\"   Positive: {sentiment['positive']:.2f}%\")\n",
    "    print(f\"   Negative: {sentiment['negative']:.2f}%\")\n",
    "    print(f\"   Death-related: {sentiment['death_related']:.2f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b6d5a7",
   "metadata": {},
   "source": [
    "## üîü Custom Analysis: Find Long Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50178d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Who uses the longest, most complex words?\n",
    "print(\"üìè LONGEST WORDS (9+ characters) BY AUTHOR:\\n\")\n",
    "\n",
    "for author_name in corpus.author_names():\n",
    "    author = corpus.get_author(author_name)\n",
    "    all_long_words = set()\n",
    "    \n",
    "    for doc in author.documents:\n",
    "        all_long_words.update(doc.long_words(min_length=9))\n",
    "    \n",
    "    print(f\"{author_name} ({len(all_long_words)} unique long words):\")\n",
    "    print(f\"   {', '.join(sorted(list(all_long_words))[:10])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fe598c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ Exercises for Students\n",
    "\n",
    "1. **Add a new author**: Create a JSON file for another author (e.g., Tolstoy, Kafka, Hemingway) and load it into the corpus\n",
    "\n",
    "2. **Extend Document class**: Add a method to find the most common word pairs (bigrams)\n",
    "\n",
    "3. **Create a ThemeAnalyzer class**: A class that focuses specifically on theme-based analysis\n",
    "\n",
    "4. **Visualization**: Use matplotlib to create bar charts comparing authors on different metrics\n",
    "\n",
    "5. **Advanced sentiment**: Expand the sentiment analysis with more emotion categories\n",
    "\n",
    "6. **Word cloud**: Generate word clouds for each author showing their most frequent words\n",
    "\n",
    "7. **Comparison method**: Add a method to `Author` class that compares this author with another author\n",
    "\n",
    "8. **Time analysis**: Analyze how writing style changed over time (using the year field)\n",
    "\n",
    "9. **Export functionality**: Add methods to export analysis results to CSV or JSON\n",
    "\n",
    "10. **Inheritance practice**: Create specialized subclasses like `NovelExcerpt`, `PhilosophicalText`, etc. that inherit from Document"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
